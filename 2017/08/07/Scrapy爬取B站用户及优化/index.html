<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.rexking6.top","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#37c6c0","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"YS7HT61SEB","apiKey":"0fd1eba022e7883c76ff4a71aee2acdc","indexName":"blog_NAME","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"找不到关于 ${query} 的文章","hits_stats":"共找到 ${hits} 篇文章，花了 ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="...">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy爬取B站用户及优化">
<meta property="og:url" content="https://blog.rexking6.top/2017/08/07/Scrapy%E7%88%AC%E5%8F%96B%E7%AB%99%E7%94%A8%E6%88%B7%E5%8F%8A%E4%BC%98%E5%8C%96/">
<meta property="og:site_name" content="RexKing6&#39;s Note">
<meta property="og:description" content="...">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://image.rexking6.top/img/1.png">
<meta property="og:image" content="http://image.rexking6.top/img/2.png">
<meta property="og:image" content="http://image.rexking6.top/img/4.png">
<meta property="og:image" content="http://image.rexking6.top/img/5.png">
<meta property="og:image" content="http://image.rexking6.top/img/3.png">
<meta property="og:image" content="http://image.rexking6.top/img/6.png">
<meta property="og:image" content="http://image.rexking6.top/img/7.png">
<meta property="og:image" content="http://image.rexking6.top/img/8.png">
<meta property="article:published_time" content="2017-08-06T23:40:58.000Z">
<meta property="article:modified_time" content="2021-07-10T11:37:01.661Z">
<meta property="article:author" content="Run-Qing Chen">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://image.rexking6.top/img/1.png">

<link rel="canonical" href="https://blog.rexking6.top/2017/08/07/Scrapy%E7%88%AC%E5%8F%96B%E7%AB%99%E7%94%A8%E6%88%B7%E5%8F%8A%E4%BC%98%E5%8C%96/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Scrapy爬取B站用户及优化 | RexKing6's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="RexKing6's Note" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RexKing6's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/rexking6" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.rexking6.top/2017/08/07/Scrapy%E7%88%AC%E5%8F%96B%E7%AB%99%E7%94%A8%E6%88%B7%E5%8F%8A%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Run-Qing Chen">
      <meta itemprop="description" content="覆苍天以为衾，卧大地以为庐。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RexKing6's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Scrapy爬取B站用户及优化
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-08-07 07:40:58" itemprop="dateCreated datePublished" datetime="2017-08-07T07:40:58+08:00">2017-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-10 19:37:01" itemprop="dateModified" datetime="2021-07-10T19:37:01+08:00">2021-07-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
            <span id="/2017/08/07/Scrapy%E7%88%AC%E5%8F%96B%E7%AB%99%E7%94%A8%E6%88%B7%E5%8F%8A%E4%BC%98%E5%8C%96/" class="post-meta-item leancloud_visitors" data-flag-title="Scrapy爬取B站用户及优化" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>20 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>因为实习在公司做的都是爬虫的二次开发（单纯开发spider），很想尝试一下自己开发一个之后优化。另外一个，秋招来临，感觉项目有点不对口，算是补充一下。</p>
<p>主要的过程，先把基本的爬虫写出来，写好MySQL存储。之后添加IPProxy代理池，之后加入Scrapy-Redis，将阿里云作为master，再之后就是添加MySQL连接池。</p>
<h1 id="Scrapy基本模块"><a href="#Scrapy基本模块" class="headerlink" title="Scrapy基本模块"></a>Scrapy基本模块</h1><h2 id="B站用户网站分析"><a href="#B站用户网站分析" class="headerlink" title="B站用户网站分析"></a>B站用户网站分析</h2><h3 id="选择要爬取的信息"><a href="#选择要爬取的信息" class="headerlink" title="选择要爬取的信息"></a>选择要爬取的信息</h3><p>随机打开B站的一个up主（可能也不是随机的）</p>
<p><img src="http://image.rexking6.top/img/1.png" alt=""></p>
<p>我们可以看到名字，性别，会员等级，关注数，粉丝数，播放数。</p>
<p><img src="http://image.rexking6.top/img/2.png" alt=""></p>
<p>另外一些是用户的个人信息，生日，地点，注册时间，uid。<br>点开了另外几个up主的主页，可以看到uid的长度是有区别的。另外数据库也很有可能随着uid递增，所以测试一下：<br>uid为1的：</p>
<p><img src="http://image.rexking6.top/img/4.png" alt=""></p>
<p>uid为2的：</p>
<p><img src="http://image.rexking6.top/img/5.png" alt=""></p>
<p>貌似发现了什么不得了的东西……<br>另外，可以看到注册时间也是符合猜测的。</p>
<p>因为B站的用户基本信息之前已经有人爬过，所以我想多爬点的来作数据分析：</p>
<p><img src="http://image.rexking6.top/img/3.png" alt=""></p>
<p>就是up主做的视频分类及数量。这样可以分析出那个分类的up主比较多。</p>
<h3 id="分析信息入口"><a href="#分析信息入口" class="headerlink" title="分析信息入口"></a>分析信息入口</h3><p>打开postman，分析<code>https://space.bilibili.com/546195</code>，可以看到没有啥信息。</p>
<p><img src="http://image.rexking6.top/img/6.png" alt=""></p>
<p>打开Chrome的开发者工具F12，选择Network-&gt;XHR：</p>
<p><img src="http://image.rexking6.top/img/7.png" alt=""></p>
<p>所以只要对<code>https://space.bilibili.com/ajax/member/GetInfo</code>POST用户的uid就好了。</p>
<p>另外一个是视频的分类：</p>
<p><img src="http://image.rexking6.top/img/8.png" alt=""></p>
<p><code>https://space.bilibili.com/ajax/member/getSubmitVideos</code>，试了一下，其他的参数可以不加，只需要uid。<br>所以信息的入口就是这样，没有用到xpath解析之类的东西。</p>
<h2 id="编写items-py"><a href="#编写items-py" class="headerlink" title="编写items.py"></a>编写<code>items.py</code></h2><p>信息分析完了之后就可以写<code>items.py</code>，其他挺好写的，就是视频分类这里，因为考虑到B站所有用户里面有做视频的可能还是占少数的，而且每个up主的视频分类也不会太广泛，所以每个分类建一个字段我觉得有些浪费和麻烦。所以最后还是存成<code>dict</code>后转为json在数据库里存储为<code>Blob</code>。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class BilibiliItem(scrapy.Item):</span><br><span class="line">    uid = scrapy.Field()</span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    space =scrapy.Field()</span><br><span class="line">    sex = scrapy.Field()</span><br><span class="line">    birthday = scrapy.Field()</span><br><span class="line">    address = scrapy.Field()</span><br><span class="line">    level = scrapy.Field()</span><br><span class="line">    regtime = scrapy.Field()</span><br><span class="line">    fans = scrapy.Field()</span><br><span class="line">    follows = scrapy.Field()</span><br><span class="line">    playnum = scrapy.Field()</span><br><span class="line">    videonum = scrapy.Field()</span><br><span class="line">    videocate = scrapy.Field()</span><br></pre></td></tr></table></figure></p>
<h2 id="编写biliup-py-spider"><a href="#编写biliup-py-spider" class="headerlink" title="编写biliup.py(spider)"></a>编写<code>biliup.py</code>(spider)</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import copy</span><br><span class="line">import json</span><br><span class="line">import time</span><br><span class="line">import scrapy</span><br><span class="line">import logging</span><br><span class="line">from scrapy import Request</span><br><span class="line">from scrapy import FormRequest</span><br><span class="line">from bilibili.items import BilibiliItem</span><br><span class="line">import sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(&#x27;utf8&#x27;)</span><br><span class="line"></span><br><span class="line">class BiliupSpider(scrapy.Spider):</span><br><span class="line">	name = &#x27;biliup&#x27;</span><br><span class="line"></span><br><span class="line">	def start_requests(self):</span><br><span class="line">		headers = &#123;&#x27;Referer&#x27;: &#x27;http://space.bilibili.com/1&#x27;&#125;</span><br><span class="line">		url = &#x27;https://space.bilibili.com/ajax/member/GetInfo&#x27;</span><br><span class="line">		for mid in range(1, 170000000):</span><br><span class="line">			data = &#123;&#x27;mid&#x27;: str(mid)&#125;</span><br><span class="line">			yield FormRequest(url, callback=self.parse_up, headers=headers, formdata=data)</span><br><span class="line"></span><br><span class="line">	def parse_up(self, response):</span><br><span class="line">		user = json.loads(response.text)</span><br><span class="line">		if not user[&#x27;status&#x27;]:</span><br><span class="line">			return</span><br><span class="line">		user = user[&#x27;data&#x27;]</span><br><span class="line">		item = BilibiliItem()</span><br><span class="line">		item[&#x27;uid&#x27;] = user[&#x27;mid&#x27;]</span><br><span class="line">		item[&#x27;name&#x27;] = user[&#x27;name&#x27;]</span><br><span class="line">		item[&#x27;space&#x27;] = &#x27;https://space.bilibili.com/&#x27; + item[&#x27;uid&#x27;]</span><br><span class="line">		item[&#x27;sex&#x27;] = user[&#x27;sex&#x27;]</span><br><span class="line">		try:</span><br><span class="line">			item[&#x27;birthday&#x27;] = user[&#x27;birthday&#x27;][-5:]</span><br><span class="line">		except KeyError:</span><br><span class="line">			item[&#x27;birthday&#x27;] = &#x27;&#x27;</span><br><span class="line">		try:</span><br><span class="line">			item[&#x27;address&#x27;] = user[&#x27;place&#x27;]</span><br><span class="line">		except KeyError:</span><br><span class="line">			item[&#x27;address&#x27;] = &#x27;&#x27;</span><br><span class="line">		item[&#x27;level&#x27;] = user[&#x27;level_info&#x27;][&#x27;current_level&#x27;]</span><br><span class="line">		try:</span><br><span class="line">			t = time.localtime(user[&#x27;regtime&#x27;])</span><br><span class="line">			item[&#x27;regtime&#x27;] = time.strftime(&#x27;%Y-%m-%d&#x27;,t)</span><br><span class="line">		except KeyError:</span><br><span class="line">			item[&#x27;regtime&#x27;] = &#x27;&#x27;</span><br><span class="line">		item[&#x27;fans&#x27;] = user[&#x27;fans&#x27;]</span><br><span class="line">		item[&#x27;follows&#x27;] = user[&#x27;attention&#x27;]</span><br><span class="line">		item[&#x27;playnum&#x27;] = user[&#x27;playNum&#x27;]</span><br><span class="line"></span><br><span class="line">		url = &#x27;https://space.bilibili.com/ajax/member/getSubmitVideos?mid=&#x27; + item[&#x27;uid&#x27;]</span><br><span class="line">		yield Request(url, callback=self.parse_video, meta=&#123;&#x27;userdata&#x27;: item&#125;)</span><br><span class="line"></span><br><span class="line">	def parse_video(self, response):</span><br><span class="line">		item = response.meta[&#x27;userdata&#x27;]</span><br><span class="line">		data = json.loads(response.text)</span><br><span class="line">		if not data[&#x27;status&#x27;]:</span><br><span class="line">			return</span><br><span class="line">		data = data[&#x27;data&#x27;]</span><br><span class="line">		item[&#x27;videonum&#x27;] = data[&#x27;count&#x27;]</span><br><span class="line">		if data[&#x27;tlist&#x27;]:</span><br><span class="line">			videocate = &#123;&#125;</span><br><span class="line">			for i in data[&#x27;tlist&#x27;]:</span><br><span class="line">				videocate[data[&#x27;tlist&#x27;][i][&#x27;name&#x27;]] = data[&#x27;tlist&#x27;][i][&#x27;count&#x27;]</span><br><span class="line">			item[&#x27;videocate&#x27;] = videocate</span><br><span class="line">		else:</span><br><span class="line">			item[&#x27;videocate&#x27;] = &#x27;&#x27;</span><br><span class="line">		yield item</span><br></pre></td></tr></table></figure>
<p>从头说起，重写<code>start_requests()</code>，当时测试了一下，有170000000多用户，这会儿也很有可能到两亿了。这个到后面存储得估计得分表，或者换个别的。</p>
<p>之后加个<code>Headers</code>post用户的uid，回调到<code>parse_up()</code>，json读出数据之后再GET<code>https://space.bilibili.com/ajax/member/getSubmitVideos</code>，带上解析的数据<code>meta</code>，回调到<code>parse_video()</code>解析出视频的数据，返回<code>item</code>。</p>
<h2 id="编写pipelines-py"><a href="#编写pipelines-py" class="headerlink" title="编写pipelines.py"></a>编写<code>pipelines.py</code></h2><p>存储可以JSON，也有MySQL存储：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">import codecs</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line">class BilibiliJsonPipeline(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.file = codecs.open(&#x27;biliup.json&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;)</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        line = json.dumps(dict(item), ensure_ascii=False) + &quot;\n&quot;</span><br><span class="line">        self.file.write(line)</span><br><span class="line">        return item</span><br><span class="line">    def spider_closed(self, spider):</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import MySQLdb</span><br><span class="line"></span><br><span class="line">class BilibiliMysqlPipeline(object):</span><br><span class="line">	def __init__(self):</span><br><span class="line">		self.conn = MySQLdb.connect(host=&#x27;localhost&#x27;, port=3306, user=&#x27;root&#x27;, passwd=&#x27;123456&#x27;, db=&#x27;biliup&#x27;)</span><br><span class="line">		self.conn.set_character_set(&#x27;utf8&#x27;)</span><br><span class="line">		self.cursor = self.conn.cursor()</span><br><span class="line">		self.cursor.execute(&#x27;SET NAMES utf8;&#x27;)</span><br><span class="line">		self.cursor.execute(&#x27;SET CHARACTER SET utf8;&#x27;)</span><br><span class="line">		self.cursor.execute(&#x27;SET character_set_connection=utf8;&#x27;)</span><br><span class="line">	</span><br><span class="line">	def process_item(self, item, spider):</span><br><span class="line">		uid = item[&#x27;uid&#x27;]</span><br><span class="line">		self.cursor.execute(&#x27;select * from biliup where uid=%s&#x27; % (uid))</span><br><span class="line">		if self.cursor.fetchone():</span><br><span class="line">			sql = &quot;update biliup set name=&#x27;%s&#x27;, space=&#x27;%s&#x27;, sex=&#x27;%s&#x27;, birthday=&#x27;%s&#x27;, address=&#x27;%s&#x27;, level=&#x27;%s&#x27;, regtime=&#x27;%s&#x27;, fans=%s, follows=%s, playnum=%s, videonum=%s, videocate=&#x27;&#123;json&#125;&#x27; where uid=%s&quot; % (item[&#x27;name&#x27;],item[&#x27;space&#x27;],item[&#x27;sex&#x27;],item[&#x27;birthday&#x27;],item[&#x27;address&#x27;],item[&#x27;level&#x27;],item[&#x27;regtime&#x27;],item[&#x27;fans&#x27;],item[&#x27;follows&#x27;],item[&#x27;playnum&#x27;],item[&#x27;videonum&#x27;],item[&#x27;uid&#x27;])</span><br><span class="line">		else:</span><br><span class="line">			if item[&#x27;regtime&#x27;]:</span><br><span class="line">				sql = &quot;insert into biliup values(%s,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,%s,%s,%s,%s,&#x27;&#123;json&#125;&#x27;)&quot; % (item[&#x27;uid&#x27;],item[&#x27;name&#x27;],item[&#x27;space&#x27;],item[&#x27;sex&#x27;],item[&#x27;birthday&#x27;],item[&#x27;address&#x27;],item[&#x27;level&#x27;],item[&#x27;regtime&#x27;],item[&#x27;fans&#x27;],item[&#x27;follows&#x27;],item[&#x27;playnum&#x27;],item[&#x27;videonum&#x27;])</span><br><span class="line">			else:</span><br><span class="line">				sql = &quot;insert into biliup(uid,name,space,sex,birthday,address,level,fans,follows,playnum,videonum,videocate) values(%s,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,%s,%s,%s,%s,&#x27;&#123;json&#125;&#x27;)&quot; % (item[&#x27;uid&#x27;],item[&#x27;name&#x27;],item[&#x27;space&#x27;],item[&#x27;sex&#x27;],item[&#x27;birthday&#x27;],item[&#x27;address&#x27;],item[&#x27;level&#x27;],item[&#x27;fans&#x27;],item[&#x27;follows&#x27;],item[&#x27;playnum&#x27;],item[&#x27;videonum&#x27;])</span><br><span class="line">		s = json.dumps(item[&#x27;videocate&#x27;])</span><br><span class="line">		sql = sql.format(json=MySQLdb.escape_string(s))</span><br><span class="line">		self.cursor.execute(sql)</span><br><span class="line">		self.conn.commit()</span><br><span class="line">		return item</span><br></pre></td></tr></table></figure><br>建库建表：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE biliup character set utf8;</span><br><span class="line"></span><br><span class="line">CREATE TABLE biliup(</span><br><span class="line">uid int(11) PRIMARY KEY,</span><br><span class="line">name char(40) NOT NULL,</span><br><span class="line">space char(80) NOT NULL,</span><br><span class="line">sex char(8),</span><br><span class="line">birthday char(5),</span><br><span class="line">address char(20),</span><br><span class="line">level int,</span><br><span class="line">regtime date,</span><br><span class="line">fans int,</span><br><span class="line">follows int,</span><br><span class="line">playnum int,</span><br><span class="line">videonum int,</span><br><span class="line">videocate blob(1024)</span><br><span class="line">)DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><br>其中在JSON字符串存入MySQL的时候，字典的引号会发生错乱。在网上查找资料之后大约是以下步骤：</p>
<ol>
<li>数据库字段设为<code>Blob</code></li>
<li>用<code>json.dumps</code>将dict转换成str</li>
<li><code>MySQLdb.escape_string()</code>转换后用<code>.format</code>填充</li>
</ol>
<p>Mysql存储的时候判断原表中是否存在已有的记录，如果有<code>update</code>，没有就<code>insert</code>。基本的模块就到这边，之后是改进和优化。</p>
<h1 id="添加代理IP池"><a href="#添加代理IP池" class="headerlink" title="添加代理IP池"></a>添加代理IP池</h1><p>代理IP池是拿七夜的<a target="_blank" rel="noopener" href="https://github.com/qiyeboy/IPProxyPool">IPProxy</a>，之前在公司用过。<br>在服务器上部署之后，数据库里就有代理IP可以用了。<br>在<code>settings.py</code>里添加中间件设置：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    &#x27;bilibili.middlewares.ProxyMiddleware&#x27;: 1,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>接下来是编写代理中间件<code>middlewares.py</code>：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import random </span><br><span class="line">import logging</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">from datetime import datetime, timedelta</span><br><span class="line">from twisted.internet.error import TimeoutError, ConnectionRefusedError, ConnectError</span><br><span class="line">from twisted.web._newclient import ResponseNeverReceived</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">class ProxyMiddleware(object):</span><br><span class="line">    # overwrite process request</span><br><span class="line">    DONT_RETRY_ERRORS = (TimeoutError, ConnectionRefusedError, ResponseNeverReceived, ConnectError, ValueError)</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        #获取IP池的接口</span><br><span class="line">        self.ip_pool_href = &quot;http://127.0.0.1:8000/&quot;</span><br><span class="line">        # 保存上次不用代理直接连接的时间点</span><br><span class="line">        self.last_no_proxy_time = datetime.now()</span><br><span class="line">        # 一定分钟数后切换回不用代理, 因为用代理影响到速度</span><br><span class="line">        self.recover_interval = 20</span><br><span class="line">        # 一个proxy如果没用到这个数字就被发现老是超时, 则永久移除该proxy. 设为0则不会修改代理文件.</span><br><span class="line">        self.dump_count_threshold = 20</span><br><span class="line">        # 是否在超时的情况下禁用代理</span><br><span class="line">        self.invalid_proxy_flag = True</span><br><span class="line">        # 当有效代理小于这个数时(包括直连), 刷新新的代理</span><br><span class="line">        self.extend_proxy_threshold = 5</span><br><span class="line">        # 初始化代理列表</span><br><span class="line">        self.proxyes = [&#123;&quot;proxy&quot;: None, &quot;valid&quot;: True, &quot;count&quot;: 0&#125;]</span><br><span class="line">        # 初始时使用0号代理(即无代理)</span><br><span class="line">        self.proxy_index = 0</span><br><span class="line">        # 表示可信代理的数量(如自己搭建的HTTP代理)+1(不用代理直接连接)</span><br><span class="line">        self.fixed_proxy = len(self.proxyes)</span><br><span class="line">        # 上一次抓新代理的时间</span><br><span class="line">        self.last_fetch_proxy_time = datetime.now()</span><br><span class="line">        # 每隔固定时间强制抓取新代理(min)</span><br><span class="line">        self.fetch_proxy_interval = 120</span><br><span class="line">        # 一个将被设为invalid的代理如果已经成功爬取大于这个参数的页面， 将不会被invalid</span><br><span class="line">        self.invalid_proxy_threshold = 200</span><br><span class="line">        #是否是https 0代表http,1代表https</span><br><span class="line">        self.protocol=0</span><br><span class="line">        </span><br><span class="line">    def url_in_proxyes(self, url):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        返回一个代理url是否在代理列表中</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">       </span><br><span class="line">        for p in self.proxyes:</span><br><span class="line">            if url == p[&quot;proxy&quot;]:</span><br><span class="line">                return True</span><br><span class="line">        return False</span><br><span class="line">    def reset_proxyes(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        将所有count&gt;=指定阈值的代理重置为valid,</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        </span><br><span class="line">        for p in self.proxyes:</span><br><span class="line">            if p[&quot;count&quot;] &gt;= self.dump_count_threshold:</span><br><span class="line">                p[&quot;valid&quot;] = True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def byteify(self,input):</span><br><span class="line">        if isinstance(input, dict):</span><br><span class="line">            return &#123;self.byteify(key): self.byteify(value) for key, value in input.iteritems()&#125;</span><br><span class="line">        elif isinstance(input, list):</span><br><span class="line">            return [self.byteify(element) for element in input]</span><br><span class="line">        elif isinstance(input, unicode):</span><br><span class="line">            return input.encode(&#x27;utf-8&#x27;)</span><br><span class="line">        else:</span><br><span class="line">            return input</span><br><span class="line"></span><br><span class="line">    def remove_invaild_proxy(self,ip):</span><br><span class="line">        ip = ip.split(&quot;//&quot;)[1]</span><br><span class="line">        ip = ip.split(&quot;:&quot;)[0]</span><br><span class="line">        r= requests.get(self.ip_pool_href+&quot;delete?ip=&quot;+ip)</span><br><span class="line">        logging.info(&#x27;remove invaild ip &#x27;+ip+&quot;  ret:&quot;+r.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def fetch_new_proxyes(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        从网上抓取新的代理添加到代理列表中</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">       </span><br><span class="line">        # new_proxyes = fetch_free_proxyes.fetch_all()</span><br><span class="line">        # logger.info(&quot;new proxyes: %s&quot; % new_proxyes)</span><br><span class="line">        self.last_fetch_proxy_time = datetime.now()</span><br><span class="line">     </span><br><span class="line">        r= requests.get(self.ip_pool_href+ (&quot;?count=5&amp;protocol=%d&quot;) % (self.protocol))</span><br><span class="line">        new_proxyes = self.byteify(json.loads(r.text))</span><br><span class="line"></span><br><span class="line">        for np in new_proxyes:</span><br><span class="line">            if self.url_in_proxyes(&quot;http://&quot; + np[0]+&quot;:&quot;+str(np[1])):</span><br><span class="line">                continue</span><br><span class="line">            else:</span><br><span class="line">                self.proxyes.append(&#123;&quot;proxy&quot;: &quot;http://&quot;  + np[0]+&quot;:&quot;+str(np[1]),</span><br><span class="line">                                     &quot;valid&quot;: True,</span><br><span class="line">                                     &quot;count&quot;: 0&#125;)</span><br><span class="line"></span><br><span class="line">        #http 与 https</span><br><span class="line">        r_2= requests.get(self.ip_pool_href+ &quot;?count=5&amp;protocol=2&quot;)</span><br><span class="line">        new_proxyes_2 = self.byteify(json.loads(r_2.text))</span><br><span class="line">        for np in new_proxyes_2:</span><br><span class="line">            if self.url_in_proxyes(&quot;http://&quot; + np[0]+&quot;:&quot;+str(np[1])):</span><br><span class="line">                continue</span><br><span class="line">            else:</span><br><span class="line">                self.proxyes.append(&#123;&quot;proxy&quot;: &quot;http://&quot;  + np[0]+&quot;:&quot;+str(np[1]),</span><br><span class="line">                                     &quot;valid&quot;: True,</span><br><span class="line">                                     &quot;count&quot;: 0&#125;)</span><br><span class="line">        </span><br><span class="line">        if self.len_valid_proxy() &lt; self.extend_proxy_threshold: # 如果发现抓不到什么新的代理了, 缩小threshold以避免白费功夫</span><br><span class="line">            self.extend_proxy_threshold -= 1</span><br><span class="line">    def len_valid_proxy(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        返回proxy列表中有效的代理数量</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        count = 0</span><br><span class="line">        for p in self.proxyes:</span><br><span class="line">            if p[&quot;valid&quot;]:</span><br><span class="line">                count += 1</span><br><span class="line">        return count</span><br><span class="line">    def inc_proxy_index(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        将代理列表的索引移到下一个有效代理的位置</span><br><span class="line">        如果发现代理列表只有fixed_proxy项有效, 重置代理列表</span><br><span class="line">        如果还发现已经距离上次抓代理过了指定时间, 则抓取新的代理</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        </span><br><span class="line">        assert self.proxyes[0][&quot;valid&quot;]</span><br><span class="line">        while True:</span><br><span class="line">            self.proxy_index = (self.proxy_index + 1) % len(self.proxyes)</span><br><span class="line">            if self.proxyes[self.proxy_index][&quot;valid&quot;]:</span><br><span class="line">                break</span><br><span class="line">        </span><br><span class="line">        # 两轮proxy_index==0的时间间隔过短，扩展代理列表</span><br><span class="line">        if self.proxy_index == 0 and datetime.now() &lt; self.last_no_proxy_time + timedelta(minutes=2):</span><br><span class="line">            logging.info(&quot;captcha thrashing&quot;)</span><br><span class="line">            self.fetch_new_proxyes()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        if self.len_valid_proxy() &lt;= self.fixed_proxy or self.len_valid_proxy() &lt; self.extend_proxy_threshold: # 如果代理列表中有效的代理不足的话重置为valid</span><br><span class="line">            self.reset_proxyes()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        if self.len_valid_proxy() &lt; self.extend_proxy_threshold: # 代理数量仍然不足, 抓取新的代理</span><br><span class="line">            logging.info(&quot;valid proxy &lt; threshold: %d/%d&quot; % (self.len_valid_proxy(), self.extend_proxy_threshold))</span><br><span class="line">            self.fetch_new_proxyes()</span><br><span class="line"></span><br><span class="line">        logging.info(&quot;now using new proxy: %s&quot; % self.proxyes[self.proxy_index][&quot;proxy&quot;])</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    def set_proxy(self, request):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        将request设置使用为当前的或下一个有效代理</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # self.inc_proxy_index()</span><br><span class="line">        proxy = self.proxyes[self.proxy_index]</span><br><span class="line">        </span><br><span class="line">        if not proxy[&quot;valid&quot;]:</span><br><span class="line"></span><br><span class="line">            self.inc_proxy_index()</span><br><span class="line">            proxy = self.proxyes[self.proxy_index]</span><br><span class="line"></span><br><span class="line">        if self.proxy_index == 0: # 每次不用代理直接下载时更新self.last_no_proxy_time</span><br><span class="line">            self.last_no_proxy_time = datetime.now()</span><br><span class="line"></span><br><span class="line">        if proxy[&quot;proxy&quot;]:</span><br><span class="line">            request.meta[&quot;proxy&quot;] = proxy[&quot;proxy&quot;]</span><br><span class="line">        elif &quot;proxy&quot; in request.meta.keys():</span><br><span class="line">            del request.meta[&quot;proxy&quot;]</span><br><span class="line">        request.meta[&quot;proxy_index&quot;] = self.proxy_index</span><br><span class="line">        proxy[&quot;count&quot;] += 1</span><br><span class="line">		</span><br><span class="line">    def invalid_proxy(self, index):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        将index指向的proxy设置为invalid,</span><br><span class="line">        并调整当前proxy_index到下一个有效代理的位置</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if index &lt; self.fixed_proxy: # 可信代理永远不会设为invalid</span><br><span class="line">            self.inc_proxy_index()</span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">        self.remove_invaild_proxy(self.proxyes[index][&#x27;proxy&#x27;])</span><br><span class="line"></span><br><span class="line">        if self.proxyes[index][&quot;valid&quot;]:</span><br><span class="line">            logging.info(&quot;invalidate %s&quot; % self.proxyes[index])</span><br><span class="line">            self.proxyes[index][&quot;valid&quot;] = False</span><br><span class="line">            if index == self.proxy_index:</span><br><span class="line">                self.inc_proxy_index()</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line">    def process_request(self, request, spider):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        将request设置为使用代理</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if request.url.split(&#x27;:&#x27;)[0] == &#x27;http&#x27;:</span><br><span class="line">                self.protocol = 0</span><br><span class="line">        else:</span><br><span class="line">                self.protocol = 1</span><br><span class="line"></span><br><span class="line">        if self.proxy_index &gt; 0  and datetime.now() &gt; (self.last_no_proxy_time + timedelta(minutes=self.recover_interval)):</span><br><span class="line">            logging.info(&quot;After %d minutes later, recover from using proxy&quot; % self.recover_interval)</span><br><span class="line">            self.last_no_proxy_time = datetime.now()</span><br><span class="line">            self.proxy_index = 0</span><br><span class="line">        # request.meta[&quot;dont_redirect&quot;] = True  # 有些代理会把请求重定向到一个莫名其妙的地址</span><br><span class="line"></span><br><span class="line">        # spider发现parse error, 要求更换代理</span><br><span class="line">        if &quot;change_proxy&quot; in request.meta.keys() and request.meta[&quot;change_proxy&quot;]:</span><br><span class="line">            logging.info(&quot;change proxy request get by spider: %s&quot;  % request)</span><br><span class="line">            self.invalid_proxy(request.meta[&quot;proxy_index&quot;])</span><br><span class="line">            request.meta[&quot;change_proxy&quot;] = False</span><br><span class="line">        self.set_proxy(request)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    def process_response(self, request, response, spider):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        检查response.status, 根据status是否在允许的状态码中决定是否切换到下一个proxy, 或者禁用proxy</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if &quot;proxy&quot; in request.meta.keys():</span><br><span class="line">            logging.info(&quot;%s %s %s&quot; % (request.meta[&quot;proxy&quot;], response.status, request.url))</span><br><span class="line">        else:</span><br><span class="line">            logging.info(&quot;None %s %s&quot; % (response.status, request.url))</span><br><span class="line"></span><br><span class="line">        # status不是正常的200而且不在spider声明的正常爬取过程中可能出现的</span><br><span class="line">        # status列表中, 则认为代理无效, 切换代理</span><br><span class="line">        </span><br><span class="line">        if response.status in (402,403,429,502,503):</span><br><span class="line">            logging.info(&quot;response status not in spider.website_possible_httpstatus_list&quot;)</span><br><span class="line">            self.invalid_proxy(request.meta[&quot;proxy_index&quot;])</span><br><span class="line">            new_request = request.copy()</span><br><span class="line">            new_request.dont_filter = True</span><br><span class="line">            return new_request</span><br><span class="line">        else:</span><br><span class="line">            return response</span><br><span class="line"></span><br><span class="line">    def process_exception(self, request, exception, spider):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        处理由于使用代理导致的连接异常</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        logging.error(&quot;%s exception: %s&quot; % (self.proxyes[request.meta[&quot;proxy_index&quot;]][&quot;proxy&quot;], exception))</span><br><span class="line">        # logging.error(&quot;%s exception: %s&quot; % (&quot;a&quot;, exception))</span><br><span class="line">        </span><br><span class="line">        request_proxy_index = request.meta[&quot;proxy_index&quot;]</span><br><span class="line"></span><br><span class="line">        # 只有当proxy_index&gt;fixed_proxy-1时才进行比较, 这样能保证至少本地直连是存在的.</span><br><span class="line">        if isinstance(exception, self.DONT_RETRY_ERRORS):</span><br><span class="line">            if request_proxy_index &gt; self.fixed_proxy - 1 and self.invalid_proxy_flag: # WARNING 直连时超时的话换个代理还是重试? 这是策略问题</span><br><span class="line">                if self.proxyes[request_proxy_index][&quot;count&quot;] &lt; self.invalid_proxy_threshold:</span><br><span class="line">                    self.invalid_proxy(request_proxy_index)</span><br><span class="line">                elif request_proxy_index == self.proxy_index:  # 虽然超时，但是如果之前一直很好用，也不设为invalid</span><br><span class="line">                    self.inc_proxy_index()</span><br><span class="line">            else:               # 简单的切换而不禁用</span><br><span class="line">                if request.meta[&quot;proxy_index&quot;] == self.proxy_index:</span><br><span class="line">                    self.inc_proxy_index()</span><br><span class="line">            new_request = request.copy()</span><br><span class="line">            new_request.dont_filter = True</span><br><span class="line">            return new_request</span><br></pre></td></tr></table></figure><br>中间件关键的是重写<code>process_request()</code>和<code>process_response()</code>。<br><code>process_request()</code>主要是添加<code>request.meta[&quot;proxy&quot;]</code>；而<code>process_response()</code>主要是判断响应的状态，确定是否需要代理或者更换代理。</p>
<p>之后放到服务器去跑，结果开始时就被判为<code>memory error</code>，估计是一亿多个请求扛不住。想了想还是得用分布式，下面添加scrapy-redis。</p>
<h1 id="scrapy-redis"><a href="#scrapy-redis" class="headerlink" title="scrapy-redis"></a>scrapy-redis</h1><p>看<a target="_blank" rel="noopener" href="http://www.zhihu.com/question/20899988">知乎上的回答</a></p>
<blockquote>
<p>我们把这100台中的99台运算能力较小的机器叫作slave，另外一台较大的机器叫作master，那么回顾上面代码中的url_queue，如果我们能把这个queue放到这台master机器上，所有的slave都可以通过网络跟master联通，每当一个slave完成下载一个网页，就向master请求一个新的网页来抓取。而每次slave新抓到一个网页，就把这个网页上所有的链接送到master的queue里去。同样，bloom filter也放到master上，但是现在master只发送确定没有被访问过的url给slave。</p>
</blockquote>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><code>pip install scrapy-redis</code></p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>安装完之后，可以去<a target="_blank" rel="noopener" href="https://github.com/rmax/scrapy-redis">scrapy-redis的GitHub</a>下载example示例，里面有三个爬虫，其中两个<code>myspider_redis</code>和<code>mycrawler_redis</code>分别是以<code>spider</code>和<code>crawlspider</code>为基类扩展的，这里参考的是<code>myspider_redis</code>。</p>
<p>以<code>RedisSpider</code>为基类，不用写<code>start_urls</code>，写<code>redis_key</code>，作为redis数据库查询url的键。之后默认采用的回调函数是<code>parse()</code>。我原项目首先发送的是<code>FormRequest()</code>，这里查询到源码用的是<code>make_requests_from_url()</code>发送请求。所以这里我修改了，再套一个函数。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">import copy</span><br><span class="line">import json</span><br><span class="line">import time</span><br><span class="line">import scrapy</span><br><span class="line">import logging</span><br><span class="line">from scrapy import Request</span><br><span class="line">from scrapy import FormRequest</span><br><span class="line">from bilibili.items import BilibiliItem</span><br><span class="line">from scrapy_redis.spiders import RedisSpider</span><br><span class="line">import sys</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(&#x27;utf8&#x27;)</span><br><span class="line"></span><br><span class="line">__author__ = &#x27;Rexking&#x27;</span><br><span class="line"></span><br><span class="line">class BiliupSpider(RedisSpider):</span><br><span class="line">	name = &#x27;biliup&#x27;</span><br><span class="line">	redis_key = &#x27;biliup:start_urls&#x27;</span><br><span class="line"></span><br><span class="line">	def parse(self, response):</span><br><span class="line">		headers = &#123;&#x27;Referer&#x27;: &#x27;http://space.bilibili.com/1&#x27;&#125;</span><br><span class="line">		url = &#x27;https://space.bilibili.com/ajax/member/GetInfo&#x27;</span><br><span class="line">		mid = response.url.split(&#x27;/&#x27;)[-1]</span><br><span class="line">		data = &#123;&#x27;mid&#x27;: mid&#125;</span><br><span class="line">		yield FormRequest(url, callback=self.parse_up, headers=headers, formdata=data)</span><br><span class="line"></span><br><span class="line">	def parse_up(self, response):</span><br><span class="line">		user = json.loads(response.text)</span><br><span class="line">		if not user[&#x27;status&#x27;]:</span><br><span class="line">			return</span><br><span class="line">		user = user[&#x27;data&#x27;]</span><br><span class="line">		item = BilibiliItem()</span><br><span class="line">		item[&#x27;uid&#x27;] = user[&#x27;mid&#x27;]</span><br><span class="line">		item[&#x27;name&#x27;] = user[&#x27;name&#x27;]</span><br><span class="line">		item[&#x27;space&#x27;] = &#x27;https://space.bilibili.com/&#x27; + item[&#x27;uid&#x27;]</span><br><span class="line">		item[&#x27;sex&#x27;] = user[&#x27;sex&#x27;]</span><br><span class="line">		try:</span><br><span class="line">			item[&#x27;birthday&#x27;] = user[&#x27;birthday&#x27;][-5:]</span><br><span class="line">		except KeyError:</span><br><span class="line">			item[&#x27;birthday&#x27;] = &#x27;&#x27;</span><br><span class="line">		try:</span><br><span class="line">			item[&#x27;address&#x27;] = user[&#x27;place&#x27;]</span><br><span class="line">		except KeyError:</span><br><span class="line">			item[&#x27;address&#x27;] = &#x27;&#x27;</span><br><span class="line">		item[&#x27;level&#x27;] = user[&#x27;level_info&#x27;][&#x27;current_level&#x27;]</span><br><span class="line">		try:</span><br><span class="line">			t = time.localtime(user[&#x27;regtime&#x27;])</span><br><span class="line">			item[&#x27;regtime&#x27;] = time.strftime(&#x27;%Y-%m-%d&#x27;,t)</span><br><span class="line">		except KeyError:</span><br><span class="line">			item[&#x27;regtime&#x27;] = &#x27;&#x27;</span><br><span class="line">		item[&#x27;fans&#x27;] = user[&#x27;fans&#x27;]</span><br><span class="line">		item[&#x27;follows&#x27;] = user[&#x27;attention&#x27;]</span><br><span class="line">		item[&#x27;playnum&#x27;] = user[&#x27;playNum&#x27;]</span><br><span class="line"></span><br><span class="line">		url = &#x27;https://space.bilibili.com/ajax/member/getSubmitVideos?mid=&#x27; + item[&#x27;uid&#x27;]</span><br><span class="line">		yield Request(url, callback=self.parse_video, meta=&#123;&#x27;userdata&#x27;: item&#125;)</span><br><span class="line"></span><br><span class="line">	def parse_video(self, response):</span><br><span class="line">		item = response.meta[&#x27;userdata&#x27;]</span><br><span class="line">		data = json.loads(response.text)</span><br><span class="line">		if not data[&#x27;status&#x27;]:</span><br><span class="line">			return</span><br><span class="line">		data = data[&#x27;data&#x27;]</span><br><span class="line">		item[&#x27;videonum&#x27;] = data[&#x27;count&#x27;]</span><br><span class="line">		if data[&#x27;tlist&#x27;]:</span><br><span class="line">			videocate = &#123;&#125;</span><br><span class="line">			for i in data[&#x27;tlist&#x27;]:</span><br><span class="line">				videocate[data[&#x27;tlist&#x27;][i][&#x27;name&#x27;]] = data[&#x27;tlist&#x27;][i][&#x27;count&#x27;]</span><br><span class="line">			item[&#x27;videocate&#x27;] = videocate</span><br><span class="line">		else:</span><br><span class="line">			item[&#x27;videocate&#x27;] = &#x27;&#x27;</span><br><span class="line">		yield item</span><br></pre></td></tr></table></figure></p>
<p>另外的，需要在<code>settings.py</code>中添加以下设置：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</span><br><span class="line">DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span><br><span class="line">REDIS_URL = &#x27;redis://root:cRq138@115.28.168.74:6379&#x27;</span><br></pre></td></tr></table></figure><br>在redis数据库里取url，是靠调度器scheduler来调度的。另外一个对urls的去重，这里采用了<code>bloom filter</code>来判定，具体可以参考：<code>https://llimllib.github.io/bloomfilter-tutorial/</code>。原本的scrapy去重是通过<code>set()</code>的，数量一上去效率就下来了。</p>
<p>另外一个<code>REDIS_URL</code>用来设定远程的redis数据库IP、端口、用户名和密码，如果不设定，则用的是本地的redis数据库。到这里scrapy-redis就添加好了，另外需要添加的是在远程redis数据库打入一亿多个url。目前服务器在跑，不知道会不会出什么差错，再看看。</p>
<h1 id="MySQL连接池"><a href="#MySQL连接池" class="headerlink" title="MySQL连接池"></a>MySQL连接池</h1><p>查了一下资料，有MySQL原生的connector，也有SQLAlchemy的session内置连接池。SQLAlchemy使用得学习一下ORM层的用法。这里就用MySQL原生的。</p>
<p>所以<code>pipelines.py</code>修改为：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">import codecs</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BilibiliJsonPipeline(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.file = codecs.open(&#x27;biliup.json&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;)</span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        line = json.dumps(dict(item), ensure_ascii=False) + &quot;\n&quot;</span><br><span class="line">        self.file.write(line)</span><br><span class="line">        return item</span><br><span class="line">    def spider_closed(self, spider):</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import MySQLdb</span><br><span class="line">import mysql.connector as conner</span><br><span class="line">from bilibili.settings import MYSQL_HOST, MYSQL_USER, MYSQL_PASSWD, MYSQL_DBNAME, MYSQL_POOLNAME, MYSQL_POOLSIZE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BilibiliMysqlPipeline(object):</span><br><span class="line">	def __init__(self):</span><br><span class="line">		self.conn = conner.connect(host=MYSQL_HOST, port=3306, user=MYSQL_USER, passwd=MYSQL_PASSWD, db=MYSQL_DBNAME, pool_name=MYSQL_POOLNAME, pool_size=MYSQL_POOLSIZE)</span><br><span class="line">		self.cursor = self.conn.cursor()</span><br><span class="line">		self.cursor.execute(&#x27;SET character_set_server=utf8;&#x27;)</span><br><span class="line">		self.cursor.execute(&#x27;SET NAMES utf8;&#x27;)</span><br><span class="line">		self.cursor.execute(&#x27;SET CHARACTER SET utf8;&#x27;)</span><br><span class="line">		self.cursor.execute(&#x27;SET character_set_connection=utf8;&#x27;)</span><br><span class="line">	</span><br><span class="line">	def process_item(self, item, spider):</span><br><span class="line">		uid = item[&#x27;uid&#x27;]</span><br><span class="line">		self.cursor.execute(&#x27;select * from biliup where uid=%s&#x27; % (uid))</span><br><span class="line">		if self.cursor.fetchone():</span><br><span class="line">			sql = &quot;update biliup set name=&#x27;%s&#x27;, space=&#x27;%s&#x27;, sex=&#x27;%s&#x27;, birthday=&#x27;%s&#x27;, address=&#x27;%s&#x27;, level=&#x27;%s&#x27;, regtime=&#x27;%s&#x27;, fans=%s, follows=%s, playnum=%s, videonum=%s, videocate=&#x27;&#123;json&#125;&#x27; where uid=%s&quot; % (item[&#x27;name&#x27;],item[&#x27;space&#x27;],item[&#x27;sex&#x27;],item[&#x27;birthday&#x27;],item[&#x27;address&#x27;],item[&#x27;level&#x27;],item[&#x27;regtime&#x27;],item[&#x27;fans&#x27;],item[&#x27;follows&#x27;],item[&#x27;playnum&#x27;],item[&#x27;videonum&#x27;],item[&#x27;uid&#x27;])</span><br><span class="line">		else:</span><br><span class="line">			if item[&#x27;regtime&#x27;]:</span><br><span class="line">				sql = &quot;insert into biliup values(%s,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,%s,%s,%s,%s,&#x27;&#123;json&#125;&#x27;)&quot; % (item[&#x27;uid&#x27;],item[&#x27;name&#x27;],item[&#x27;space&#x27;],item[&#x27;sex&#x27;],item[&#x27;birthday&#x27;],item[&#x27;address&#x27;],item[&#x27;level&#x27;],item[&#x27;regtime&#x27;],item[&#x27;fans&#x27;],item[&#x27;follows&#x27;],item[&#x27;playnum&#x27;],item[&#x27;videonum&#x27;])</span><br><span class="line">			else:</span><br><span class="line">				sql = &quot;insert into biliup(uid,name,space,sex,birthday,address,level,fans,follows,playnum,videonum,videocate) values(%s,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,&#x27;%s&#x27;,%s,%s,%s,%s,&#x27;&#123;json&#125;&#x27;)&quot; % (item[&#x27;uid&#x27;],item[&#x27;name&#x27;],item[&#x27;space&#x27;],item[&#x27;sex&#x27;],item[&#x27;birthday&#x27;],item[&#x27;address&#x27;],item[&#x27;level&#x27;],item[&#x27;fans&#x27;],item[&#x27;follows&#x27;],item[&#x27;playnum&#x27;],item[&#x27;videonum&#x27;])</span><br><span class="line">		s = json.dumps(item[&#x27;videocate&#x27;])</span><br><span class="line">		sql = sql.format(json=MySQLdb.escape_string(s))</span><br><span class="line">		self.cursor.execute(sql)</span><br><span class="line">		self.conn.commit()</span><br><span class="line">		return item</span><br></pre></td></tr></table></figure></p>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2017\07\20\Scrapy结合IPProxyPool实现代理的一些坑\" rel="bookmark">Scrapy结合IPProxyPool实现代理的一些坑</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2017\07\15\Scrapy-1-0-5官方文档记录\" rel="bookmark">Scrapy 1.0.5官方文档</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2017\08\11\urllib-quote对url的编码\" rel="bookmark">urllib.quote对url的编码</a></div>
    </li>
  </ul>

        <div class="reward-container">
  <div>一分一毛，也是心意。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Run-Qing Chen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Run-Qing Chen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Run-Qing Chen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://blog.rexking6.top/2017/08/07/Scrapy%E7%88%AC%E5%8F%96B%E7%AB%99%E7%94%A8%E6%88%B7%E5%8F%8A%E4%BC%98%E5%8C%96/" title="Scrapy爬取B站用户及优化">https://blog.rexking6.top/2017/08/07/Scrapy爬取B站用户及优化/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"># 爬虫</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2017/08/07/KMP%E7%AE%97%E6%B3%95/" rel="prev" title="KMP算法">
      <i class="fa fa-chevron-left"></i> KMP算法
    </a></div>
      <div class="post-nav-item">
    <a href="/2017/08/11/urllib-quote%E5%AF%B9url%E7%9A%84%E7%BC%96%E7%A0%81/" rel="next" title="urllib.quote对url的编码">
      urllib.quote对url的编码 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

    <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9D%97"><span class="nav-number">2.</span> <span class="nav-text">Scrapy基本模块</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#B%E7%AB%99%E7%94%A8%E6%88%B7%E7%BD%91%E7%AB%99%E5%88%86%E6%9E%90"><span class="nav-number">2.1.</span> <span class="nav-text">B站用户网站分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E8%A6%81%E7%88%AC%E5%8F%96%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="nav-number">2.1.1.</span> <span class="nav-text">选择要爬取的信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E6%9E%90%E4%BF%A1%E6%81%AF%E5%85%A5%E5%8F%A3"><span class="nav-number">2.1.2.</span> <span class="nav-text">分析信息入口</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E5%86%99items-py"><span class="nav-number">2.2.</span> <span class="nav-text">编写items.py</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E5%86%99biliup-py-spider"><span class="nav-number">2.3.</span> <span class="nav-text">编写biliup.py(spider)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E5%86%99pipelines-py"><span class="nav-number">2.4.</span> <span class="nav-text">编写pipelines.py</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E4%BB%A3%E7%90%86IP%E6%B1%A0"><span class="nav-number">3.</span> <span class="nav-text">添加代理IP池</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#scrapy-redis"><span class="nav-number">4.</span> <span class="nav-text">scrapy-redis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85"><span class="nav-number">4.1.</span> <span class="nav-text">安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8"><span class="nav-number">4.2.</span> <span class="nav-text">使用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MySQL%E8%BF%9E%E6%8E%A5%E6%B1%A0"><span class="nav-number">5.</span> <span class="nav-text">MySQL连接池</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Run-Qing Chen"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Run-Qing Chen</p>
  <div class="site-description" itemprop="description">覆苍天以为衾，卧大地以为庐。</div>
</div>


   <div class="feed-link motion-element">
     <a href="/atom.xml" rel="alternate">
       <i class="fa fa-rss"></i>
       RSS
     </a>
   </div>
 
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">183</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/RexKing6" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;RexKing6" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1010026261@qq.com" title="E-Mail → mailto:1010026261@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zxpblog.cn/" title="https:&#x2F;&#x2F;www.zxpblog.cn&#x2F;" rel="noopener" target="_blank">赵小平</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://whitepuffer.github.io/" title="https:&#x2F;&#x2F;whitepuffer.github.io&#x2F;" rel="noopener" target="_blank">江斓</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://kexue.fm/" title="https:&#x2F;&#x2F;kexue.fm&#x2F;" rel="noopener" target="_blank">科学空间</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yongyuan.name/" title="https:&#x2F;&#x2F;yongyuan.name&#x2F;" rel="noopener" target="_blank">袁勇</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/abcjennifer" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;abcjennifer" rel="noopener" target="_blank">Rachel Zhang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dmkf.xyz/" title="http:&#x2F;&#x2F;dmkf.xyz&#x2F;" rel="noopener" target="_blank">代码咖啡</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://wuxiaolong.me/" title="http:&#x2F;&#x2F;wuxiaolong.me&#x2F;" rel="noopener" target="_blank">吴小龙同学</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.tennfy.com/" title="http:&#x2F;&#x2F;www.tennfy.com&#x2F;" rel="noopener" target="_blank">TENNFY WU</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fab fa-accessible-icon"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Run-Qing Chen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">3.2m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">48:01</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"6XDsO3aHIjDk3nV6eLJCufbl-MdYXbMMI","app_key":"YK4qOc0TpkazN6exhuqsnwmB","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  

  

</body>
</html>

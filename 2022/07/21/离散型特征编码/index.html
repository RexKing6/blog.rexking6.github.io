<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.rexking6.top","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#37c6c0","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"YS7HT61SEB","apiKey":"0fd1eba022e7883c76ff4a71aee2acdc","indexName":"blog_NAME","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"找不到关于 ${query} 的文章","hits_stats":"共找到 ${hits} 篇文章，花了 ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="...">
<meta property="og:type" content="article">
<meta property="og:title" content="离散型特征编码">
<meta property="og:url" content="https://blog.rexking6.top/2022/07/21/%E7%A6%BB%E6%95%A3%E5%9E%8B%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81/">
<meta property="og:site_name" content="RexKing6&#39;s Note">
<meta property="og:description" content="...">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://image.rexking6.top/img/1251096-20171106143504638-2121612119.png">
<meta property="og:image" content="https://image.rexking6.top/img/1251096-20171106151030778-484474659.png">
<meta property="og:image" content="https://image.rexking6.top/img/1251096-20171106165501794-386583892.png">
<meta property="og:image" content="https://image.rexking6.top/img/v2-b5483978bf87d1b399518d4ff8853f72_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-95a192623070671a96b22650b18f74fe_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-aad48636aa1ca8b969f8a0a2a3fdd150_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-0eb2ec5ff0ee1448cd586f477f9a486b_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-14e4a3bc4224ddfb7ff356d6a36d8536_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-6b0a6935db6a4331b71137a36429166a_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-44453b29fa2d9b6960ca8383e74e62ba_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-dde85d36b7f8772cd17ddb3f9a15b8bf_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-da4efaea588ce485f0d6e59f01b0dc9e_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-d06af20eedce764d404eefd60488826d_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-91195ada03259565052446ca0bb1bc3b_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-1b9a9cfbdfff4cc0af9907039d47f493_720w.webp">
<meta property="og:image" content="https://image.rexking6.top/img/v2-21ac50ef9105de8ffd6aeb6f1a79d010_720w.webp">
<meta property="article:published_time" content="2022-07-20T16:04:11.000Z">
<meta property="article:modified_time" content="2023-01-02T16:03:57.540Z">
<meta property="article:author" content="Run-Qing Chen">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://image.rexking6.top/img/1251096-20171106143504638-2121612119.png">

<link rel="canonical" href="https://blog.rexking6.top/2022/07/21/%E7%A6%BB%E6%95%A3%E5%9E%8B%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>离散型特征编码 | RexKing6's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="RexKing6's Note" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RexKing6's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/rexking6" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.rexking6.top/2022/07/21/%E7%A6%BB%E6%95%A3%E5%9E%8B%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Run-Qing Chen">
      <meta itemprop="description" content="覆苍天以为衾，卧大地以为庐。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RexKing6's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          离散型特征编码
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-07-21 00:04:11" itemprop="dateCreated datePublished" datetime="2022-07-21T00:04:11+08:00">2022-07-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-03 00:03:57" itemprop="dateModified" datetime="2023-01-03T00:03:57+08:00">2023-01-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/2022/07/21/%E7%A6%BB%E6%95%A3%E5%9E%8B%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81/" class="post-meta-item leancloud_visitors" data-flag-title="离散型特征编码" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>22k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>20 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>综合转载于：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://contrib.scikit-learn.org/category_encoders/">Category Encoders</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/87203369">11种离散型变量编码方式及效果对比</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/lianyingteng/p/7792693.html">离散型特征编码方式：one-hot与哑变量</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/67475635">特征编码方法总结—part1</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/68259539">特征编码方法总结—part2</a></li>
</ul>
<h1 id="Category-Encoders"><a href="#Category-Encoders" class="headerlink" title="Category Encoders"></a>Category Encoders</h1><p>关于离散型编码的Python库，里面封装了十几种（包括文中的所有方法）对于离散型特征的编码方法，接口接近于Sklearn通用接口，非常实用，<a target="_blank" rel="noopener" href="https://contrib.scikit-learn.org/category_encoders/">这个库的链接</a>。</p>
<h1 id="OneHot-Encoder-Dummy-Encoder-OHE"><a href="#OneHot-Encoder-Dummy-Encoder-OHE" class="headerlink" title="OneHot Encoder/Dummy Encoder/OHE"></a>OneHot Encoder/Dummy Encoder/OHE</h1><p>大家熟知的OneHot方法就避免了对特征排序的缺点。对于一列有N种取值的特征，Onehot方法会创建出对应的N列特征，其中每列代表该样本是否为该特征的某一种取值。因为生成的每一列有值的都是1，所以这个方法起名为Onehot特征。但是在离散特征的特征值过多的时候不宜使用，因为会导致生成特征的数量太多且过于稀疏。</p>
<p>举个例子，假设我们以学历为例，我们想要研究的类别为小学、中学、大学、硕士、博士五种类别，我们使用one-hot对其编码就会得到：</p>
<p><img src="https://image.rexking6.top/img/1251096-20171106143504638-2121612119.png" alt=""></p>
<h1 id="dummy-encoding"><a href="#dummy-encoding" class="headerlink" title="dummy encoding"></a>dummy encoding</h1><p>哑变量编码直观的解释就是任意的将一个状态位去除。还是拿上面的例子来说，我们用4个状态位就足够反应上述5个类别的信息，也就是我们仅仅使用前四个状态位 [0,0,0,0] 就可以表达博士了。只是因为对于一个我们研究的样本，他已不是小学生、也不是中学生、也不是大学生、又不是研究生，那么我们就可以默认他是博士，是不是。（额，当然他现实生活也可能上幼儿园，但是我们统计的样本中他并不是，^-^）。所以，我们用哑变量编码可以将上述5类表示成：</p>
<p><img src="https://image.rexking6.top/img/1251096-20171106151030778-484474659.png" alt=""></p>
<h2 id="one-hot编码和dummy编码：区别与联系"><a href="#one-hot编码和dummy编码：区别与联系" class="headerlink" title="one-hot编码和dummy编码：区别与联系"></a>one-hot编码和dummy编码：区别与联系</h2><p>通过上面的例子，我们可以看出它们的“思想路线”是相同的，只是哑变量编码觉得one-hot编码太罗嗦了（一些很明显的事实还说的这么清楚），所以它就很那么很明显的东西省去了。这种简化不能说到底好不好，这要看使用的场景。下面我们以一个例子来说明：</p>
<p>假设我们现在获得了一个模型$\mu=\beta_0+\beta_1 X_1+\beta_2 X_2+\beta_3 X_3$，这里自变量满足$X_1+X_2+X_3=1$（因为特征是one-hot获得的，所有只有一个状态位为1，其他都为了0，所以它们加和总是等于1），故我们可以用表$X_3=1-X_1-X_2$示第三个特征，将其带入模型中，得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mu & =a_0+a_1 X_1+a_2 X_2+a_3 X_3 \\
& =a_0+a_1 X_1+a_2 X_2+a_3\left(1-X_1-X_2\right) \\
& =\left(a_0+a_3\right)+\left(a_1-a_3\right) X_1+\left(a_2-a_3\right) X_2
\end{aligned}</script><p>这时，我们就惊奇的发现$\left(a_0, a_1, a_2, a_3\right)$和$\left(a_0+a_3, a_1-a_3, a_2-a_3, 0\right)$这两个参数是等价的！那么我们模型的稳定性就成了一个待解决的问题。这个问题这么解决呢？有三种方法：</p>
<ol>
<li><p>使用L2正则化手段，将参数的选择上加一个限制，就是选择参数元素值小的那个作为最终参数，这样我们得到的参数就唯一了，模型也就稳定了。</p>
</li>
<li><p>把偏置项去掉，这时我们发现也可以解决同一个模型参数等价的问题。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mu & =a_1 X_1+a_2 X_2+a_3 X_3 \\
& =X_1+a_2 X_2+a_3\left(1-X_1-X_2\right) \\
& =a_3+\left(a_1-a_3\right) X_1+\left(a_2-a_3\right) X_2+0 \cdot X_3
\end{aligned}</script></li>
<li></li>
</ol>
<p>​        因为有了bias项，所以和我们去掉bias项的模型是完全不同的模型，不存在参数等价的问题。</p>
<ol>
<li>再加上bias项的前提下，使用哑变量编码代替one-hot编码，这时去除了$X_3$，也就不存在之前一种特征可以用其他特征表示的问题了。</li>
</ol>
<p><strong>总结：我们使用one-hot编码时，通常我们的模型不加bias项 或者 加上bias项然后使用L2正则化手段去约束参数；当我们使用哑变量编码时，通常我们的模型都会加bias项，因为不加bias项会导致固有属性的丢失。</strong></p>
<p>选择建议：我感觉最好是选择<strong>正则化 + one-hot编码</strong>；哑变量编码也可以使用，不过最好选择前者。虽然哑变量可以去除one-hot编码的冗余信息，但是因为每个离散型特征各个取值的地位都是对等的，随意取舍未免来的太随意。</p>
<p><strong>连续值的离散化为什么会提升模型的非线性能力？</strong></p>
<p>简单的说，使用连续变量的LR模型，模型表示为公式（1），而使用了one-hot或哑变量编码后的模型表示为公式（2）</p>
<p><img src="https://image.rexking6.top/img/1251096-20171106165501794-386583892.png" alt=""></p>
<p>式中$x_1$表示连续型特征，$\theta_1, \theta_2, \theta_3$分别是离散化后在使用one-hot或哑变量编码后的若干个特征表示。这时我们发现使用连续值的LR模型用一个权值去管理该特征，而one-hot后有三个权值管理了这个特征，这样使得参数管理的更加精细，所以这样拓展了LR模型的非线性能力。</p>
<p>这样做除了增强了模型的<strong>非线性能力</strong>外，还有什么好处呢？这样做了我们至少不用再去对变量进行归一化，也可以<strong>加速</strong>参数的更新速度；再者使得一个很大权值管理一个特征，拆分成了许多小的权值管理这个特征多个表示，这样做降低了特征值扰动对模型为<strong>稳定性</strong>影响，也降低了异常数据对模型的影响，进而使得模型具有更好的<strong>鲁棒性</strong>。</p>
<h1 id="Label-Encoder-Ordered-Encoder"><a href="#Label-Encoder-Ordered-Encoder" class="headerlink" title="Label Encoder/Ordered Encoder"></a>Label Encoder/Ordered Encoder</h1><p>这个编码方式非常容易理解，就是把所有的相同类别的特征编码成同一个值，例如女=0，男=1，狗狗=2，所以最后编码的特征值是在[0, n-1]之间的整数。</p>
<p>这个编码的缺点在于它随机的给特征排序了，会给这个特征增加不存在的顺序关系，也就是增加了噪声。假设预测的目标是购买力，那么真实Label的排序显然是 女 &gt; 狗狗 &gt; 男，与我们编码后特征的顺序不存在相关性。</p>
<h1 id="label-binarize-二值化编码"><a href="#label-binarize-二值化编码" class="headerlink" title="label_binarize 二值化编码"></a>label_binarize 二值化编码</h1><p>举个例子就知道是干嘛用的了，比如特征为【晴天，雨天，阴天，雷暴】则特征转化为【是否晴天，是否雨天，是否阴天，是否雷暴】，用数字来表示【雷暴】就是[0,0,0,1]，和onthot看起来很类似，很多时候不那么严格界定，其实等同于onehot，一般来说独热编码的结果是多个0和1个1组成的比如类别特征的处理，但是也存在处理之后出现多个1和多个0的情况，比如文本问题，whatever，不做严格区分，因为很多文章都不划分那么细，反正自己心里有数就行了，实现使用sklearn的label_binarize或者自己用字典来实现。</p>
<h1 id="Frequency-Encoder-Count-Encoder"><a href="#Frequency-Encoder-Count-Encoder" class="headerlink" title="Frequency Encoder/Count Encoder"></a>Frequency Encoder/Count Encoder</h1><p>这个方法统计训练集中每个特征出现的频率，在某些场景下非常有用（例如推荐系统中商品被购买的次数，直接反映了商品的流行程度），也不容易出现过拟合，但是缺点是在每个特征的取值数分布比较均匀时会遗漏大量的信息。</p>
<h1 id="直方图编码"><a href="#直方图编码" class="headerlink" title="直方图编码"></a>直方图编码</h1><p>直方图编码，主要针对类别型特征与类别型标签的一种编码方式，还是举个例子来说明什么是直方图编码吧，最好理解了：</p>
<p>假设类别特征f1=【A，A，B，B，B，C，C】，对应的二分类标签为【0，1，0，1，1，0，0】，则我们是这样来计算类别特征f1中对应的类别的编码值的：</p>
<p>以A为例，类别特征f1的值为A的样本有两个，这两个样本的标签分别为【0，1】，则A被直方图编码为【1/2,1/2】=【0.5，0.5】（A的样本一共有2个所以分母为2，其中一个样本标签为1，一个样本标签为0），实际上就是计算取值为A的样本中，不同类别样本的比例，然后用这个比例来替换原始的类别标签，这里需要强调的是，无论是直方图编码还是我们后面要介绍的target encoding，本质上都是用类别特征的统计量来代替原来的类别值的，没什么神秘的地方，很好理解。</p>
<p>如法炮制，我们来对B进行类别编码，f1值为B的一共3个样本，其中一个样本标签为0，两个样本标签为1，所以B被编码为【1/3,2/3】，很好理解了。同样对于C，一共两个样本，并且两个样本标签均为0，则编码为【2/2,0】。</p>
<p>直方图编码实际上存在着比较多的问题，我们目前针对高基类特征的常用的目标编码或者均值编码实际上可以看作是在直方图编码之上的问题改进。</p>
<p>直方图编码存在以下问题：</p>
<ol>
<li>没有考虑到类别特征中不同类别的数量的影响，举个例子，假设样本的某个类别特征为【A,A,A,A,A,A,B】，对应的标签为【0，0，0，1，1，1，0】，则根据直方图编码的公式得到的结果为A：【1/2,1/2】，B：【1,0】，然而这实际上对于A来说是很不公平的，因为B的样本数量太少，计算出来的结果根本不能算是明显的统计特征，而很可能是一种噪音，这实际上是一种非常“过拟合”的计算方式，因为一旦测试集中的样本有多个B之后，B的直方图编码的结果很可能发生非常大的变化；</li>
<li>假设没有1中出现的情况，所有的类别A，B的数量都比较均匀，直方图编码还是存在着一个潜在的隐患，直方图编码的计算非常依赖于训练集中的样本标签的分布情况，以f1特征的那个例子为例，实际上直方图这么计算的隐含的假设是潜在的所有的数据的在类别f1上的每一个类别计算出来的结果可以用训练集的结果来近似代替，简单说比如我在训练集中算出来A的直方图编码为【1/2,1/2】，即类别为A的样本中有一半标签0的样本，一半标签1的样本，那么一旦测试集的分布情况发生改变，或者是训练集本身的采样过程就是有偏的，则直方图编码的结果就是完全错误的，（比如全样本中，类别为A的样本其实只有10%是标签为0的，90%标签为1的，则这个时候A的直方图编码为【1/10,9/10】，训练集的产生可能是有偏的）；</li>
</ol>
<h1 id="二进制编码与N进制编码"><a href="#二进制编码与N进制编码" class="headerlink" title="二进制编码与N进制编码"></a>二进制编码与N进制编码</h1><p>N进制编码是二进制编码的扩展，实际上原理也不难，就是把类别特征的编码结果换了一种进制表示而已：</p>
<p>例如我们的类别有[A,B,C,D,E]，正常用labelencoder是使用十进制变成[0,1,2,3,4]（注意，这里的编码结果是完全没有任何大小关系，就是字符串转数字而已。）而二进制编码就是用二进制的方式来表示比如上面的A,B,C,D,E一共有5个数字，所以可以用二进制编码为:</p>
<p>[0,0,0],[0,0,1],[0,1,0],[0,1,1],[1,0,0] 就是这么简单。</p>
<p>三进制编码：</p>
<p>[0,0],[0,1],[0,2],[1,0],[1,1]</p>
<p>依次类推……N进制编码。</p>
<p>那么问题来了，这到底有什么卵用。</p>
<p>category_encoders中也实现了，N进制方法叫base_N。二进制方法叫Binary。</p>
<h1 id="Sum-Encoder-Deviation-Encoder-Effect-Encoder"><a href="#Sum-Encoder-Deviation-Encoder-Effect-Encoder" class="headerlink" title="Sum Encoder/Deviation Encoder/Effect Encoder"></a>Sum Encoder/Deviation Encoder/Effect Encoder</h1><p>求和编码通过比较某一特征取值下对应标签（或其他相关变量）的均值与标签的均值之间的差别来对特征进行编码。但是据我所知 ，如果做不好细节，这个方法非常容易出现过拟合，所以需要配合留一法或者五折交叉验证进行特征的编码。还有根据方差加入惩罚项防止过拟合的方法。</p>
<h1 id="Leave-one-out-Encoder-LOO-or-LOOE"><a href="#Leave-one-out-Encoder-LOO-or-LOOE" class="headerlink" title="Leave-one-out Encoder (LOO or LOOE)"></a>Leave-one-out Encoder (LOO or LOOE)</h1><p>这个方法类似于SUM的方法，只是在计算训练集每个样本的特征值转换时都要把该样本排除(消除特征某取值下样本太少导致的严重过拟合)，在计算测试集每个样本特征值转换时与SUM相同。可见以下公式：</p>
<script type="math/tex; mode=display">
\hat{x}_i^k=\frac{\sum_{j \neq i}\left(y_j *\left(x_j==k\right)\right)-y_i}{\sum_{j \neq i} x_j==k}</script><h1 id="Helmet-Encoder"><a href="#Helmet-Encoder" class="headerlink" title="Helmet Encoder"></a>Helmet Encoder</h1><p>Helmet编码是仅次于OHE和SumEncoder使用最广泛的编码方法，与SumEncoder不同的是，它比较的是某一特征取值下对应标签（或其他相关变量）的均值与他之前特征的均值之间的差异，而不是和所有特征的均值比较。这个特征同样容易出现过拟合的情况。不知道Helmet这个词是指的什么方面……使用标签时容易出现过拟合。</p>
<h1 id="Target-Encoder"><a href="#Target-Encoder" class="headerlink" title="Target Encoder"></a>Target Encoder</h1><p><strong>先调一波包</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from category_encoders import *</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.datasets import load_boston</span><br><span class="line">bunch = load_boston()</span><br><span class="line">y = bunch.target</span><br><span class="line">X = pd.DataFrame(bunch.data, columns=bunch.feature_names)</span><br><span class="line">enc = TargetEncoder(cols=[&#x27;CHAS&#x27;, &#x27;RAD&#x27;]).fit(X, y)</span><br><span class="line">numeric_dataset = enc.transform(X)</span><br><span class="line">print(numeric_dataset.info())</span><br></pre></td></tr></table></figure>
<p>原理也不难；</p>
<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>对于C分类问题,目标编码（target encode）后只需要增加C−1个属性列,如果C远远小于N,则相对one-hot-encoding可以节省很多内存. 其出发点是用概率P(y=yi|x=xi)代替属性值x, 其中x表示属性值,y表示类别值. 但实际问题中,经常会遇到x=xi对应的样本数目比较少,导致对P(y=yi|x=xi)的计算不准确. 所以后来的改进结果是引入先验概率P(y=yi),公式转换成 ：</p>
<script type="math/tex; mode=display">
f\left(y_j, x_i\right)=\lambda\left(n_i\right) P\left(y=y_j \mid x=x_i\right)+\left(1-\lambda\left(n_i\right)\right) P\left(y=y_j\right)</script><p>细心一点就可以发现，如果上述不引入先验概率P以及lambda项，其实就是我们前面提到的直方图编码。直方图编码是target encode和mean encode的前辈了。</p>
<p>其中$j∈[0,C)$,$n_i$是训练集中xi的样本个数,$λ(n_i)∈[0,1]$负责计算两个概率值的可靠性,针对应用有不同的定义方法,如下是一个例子 ：</p>
<script type="math/tex; mode=display">
\lambda(n)=\frac{1}{1+e^{-(n-k) / f}}</script><p>（我们的category_encoders库使用的就是上面这个例子的计算方式，其中参数$k$和$f$分别是我们的<strong>min_sample_leaf和smoothing参数）</strong>。二者都是一个可调参数，当$x$在训练集中出现次数$n=k$时，$λ(n)=0.5$，两个概率的可靠性相等,随着$n$的增大，先验概率$P(y=y_i)$的可靠性逐渐降低。</p>
<p>我第一次接触这里的$λ(n)$还是比较奇怪的，长得很奇怪，不过其实带几个数进去算一算也能理解这个项的意义了，公式转换成这样主要是考虑到有的类别xi的数量太少从而编码结果不精确（原因在直方图编码那边已经描述过了），对于数量很大的xi来说，入(n)的引入几乎没有影响，比如n=100000，此时$λ(n)$的计算结果趋近于1，先验项的系数趋近于0，则target_encode计算的结果和直方图编码的计算结果是基本近似的。如果n很小，比如n=2，则入(n)=0.731，此时根据先验项的系数为0.269，即最终编码结果部分受到先验项的影响，从而通过这种方式降低由于n数量太小而导致的编码不精确的问题。类似于用先验的统计值对原来的编码结果进行一个调和加权平均），所以显然，这里的k越大，则意味着先验的影响越大。</p>
<h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><p>回归问题同样可以使用均值编码,只需要把概率换成均值：</p>
<script type="math/tex; mode=display">
f(y,x_i)=\lambda(n_i)\frac{\sum_{x=x_i}y}{n_i}+(1-\lambda(n_i))\frac{\sum_y}{N}</script><p>其中：</p>
<script type="math/tex; mode=display">
\frac{\sum_{x=x_i} y}{n_i}</script><p>表示$x=x_i$对应的$y$均值，</p>
<script type="math/tex; mode=display">
\frac{\sum y}{N}</script><p>是整个训练集上y的均值。</p>
<p>我们来看一看源代码，下面是核心实现代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def fit_target_encoding(self, X, y):</span><br><span class="line">    mapping = &#123;&#125;</span><br><span class="line">    for switch in self.ordinal_encoder.category_mapping:</span><br><span class="line">        col = switch.get(&#x27;col&#x27;)</span><br><span class="line">        values = switch.get(&#x27;mapping&#x27;)</span><br><span class="line">        prior = self._mean = y.mean()</span><br><span class="line">        stats = y.groupby(X[col]).agg([&#x27;count&#x27;, &#x27;mean&#x27;])</span><br><span class="line">        smoove = 1 / (1 + np.exp(-(stats[&#x27;count&#x27;] - self.min_samples_leaf) / self.smoothing))</span><br><span class="line">        smoothing = prior * (1 - smoove) + stats[&#x27;mean&#x27;] * smoove</span><br><span class="line">        smoothing[stats[&#x27;count&#x27;] == 1] = prior</span><br><span class="line">        if self.handle_unknown == &#x27;return_nan&#x27;:</span><br><span class="line">        smoothing.loc[-1] = np.nan</span><br><span class="line">        elif self.handle_unknown == &#x27;value&#x27;:</span><br><span class="line">        smoothing.loc[-1] = prior</span><br><span class="line">        if self.handle_missing == &#x27;return_nan&#x27;:</span><br><span class="line">        smoothing.loc[values.loc[np.nan]] = np.nan</span><br><span class="line">        elif self.handle_missing == &#x27;value&#x27;:</span><br><span class="line">        smoothing.loc[-2] = prior</span><br><span class="line">        mapping[col] = smoothing</span><br><span class="line">	return mapping</span><br></pre></td></tr></table></figure>
<p>核心中的核心代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prior = self._mean = y.mean() ##计算连续值标签的均值</span><br><span class="line">stats = y.groupby(X[col]).agg([&#x27;count&#x27;, &#x27;mean&#x27;]) #根据类别特征中的不同的类分别进行groupby，聚合函数为计数group和求平均值mean</span><br><span class="line">smoove = 1 / (1 + np.exp(-(stats[&#x27;count&#x27;] - self.min_samples_leaf) / self.smoothing)) #计算smoove值，</span><br><span class="line">smoothing = prior * (1 - smoove) + stats[&#x27;mean&#x27;] * smoove #计算最终的编码结果smoothing值</span><br><span class="line">smoothing[stats[&#x27;count&#x27;] == 1] = prior#出现次数为1的类别直接用先验值prior代替</span><br></pre></td></tr></table></figure>
<p>为了便于理解还是举个例子吧。</p>
<p>假设特征f1为【A，A，A，B，B，C，C，C，C，D】，对应的连续值标签为【1，2，3，4，5，6，7，8，9，10】则根据上面的源代码我们计算结果如下：</p>
<p>对于A，对应的子数据集为【A，A，A】。【1，2，3】，则y.mean()=(1+2+3+4+……+10)/10=5.5，groupby之后的结果为count=3，mean=（1+2+3）/3=2，因为<strong>min_samples_leaf和smoothing</strong>默认值为1，则smoove=1/(1+np.exp(-3-<strong>min_samples_leaf</strong>))=0.982</p>
<p>5.5x(1-0.892)+2x0.892=2.378，调包试了一下，结果差不多，思路没什么问题。</p>
<p>target encode是针对高基数类别特征进行处理手段的最好的选择之一。但它也有缺点，就是容易过拟合，因为所有的统计计算都是基于训练集来的，所以一旦新数据集的分布发生变化，就会产生类似于过拟合所产生的不良的训练效果，所以接下来我们要介绍target encode 的升级版，也是目前最常用的特征编码方法之一，mean encoding。</p>
<h1 id="mean-encoding"><a href="#mean-encoding" class="headerlink" title="mean encoding"></a>mean encoding</h1><p>网上有实现的源码，就不费心思去看论文了，直接根据代码来解释均值编码的原理吧，均值编码的原理和target encoding非常非常类似，只不过为了避免过拟合加入了一些特别的手段而已。首先来看一下初始化的部分：</p>
<p><img src="https://image.rexking6.top/img/v2-b5483978bf87d1b399518d4ff8853f72_720w.webp" alt=""></p>
<ul>
<li>self.categorical_features 用来指定特征变量中的类别变量的变量名</li>
<li>self.n_splits 用与指定后面交叉验证的折数（后文详述）</li>
<li>self.learned_stats 用于统计量的存放</li>
</ul>
<p>然后做了一个分类和回归的判断，分类和回归下的均值编码略有不同</p>
<p>判断是否存在先验权重计算函数，没有的话则默认使用下面的公式并且k和f是根据用户给定的字典类型的参数取值的，有的话则使用用户给定的先验权重计算函数来计算先验权重：</p>
<script type="math/tex; mode=display">
\lambda(n)=\frac{1}{1+e^{-(n-k) / f}}</script><p>如果用户没有提供先验权重计算函数也没有提供k和f的参数值则k，f则使用默认值分别为$k=2$，$f=1$。</p>
<p>然后我们看一下“fit_transform”部分：</p>
<p><img src="https://image.rexking6.top/img/v2-95a192623070671a96b22650b18f74fe_720w.webp" alt=""></p>
<p>首先是copy一个新的特征矩阵Xnew，然后根据分类还是回归问题选择不同的抽样方式（分层抽样or普通抽样），然后我们生成一个字典learned_stas用于存放编码之后的结果。接下来是核心实现的部分：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for variable, target in product(self.categorical_features, self.target_values):</span><br><span class="line">    nf_name = &#x27;&#123;&#125;_pred_&#123;&#125;&#x27;.format(variable, target)</span><br><span class="line">    X_new.loc[:, nf_name] = np.nan</span><br><span class="line">    for large_ind, small_ind in skf.split(y, y):</span><br><span class="line">        nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(</span><br><span class="line">            X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)</span><br><span class="line">        X_new.iloc[small_ind, -1] = nf_small</span><br><span class="line">        self.learned_stats[nf_name].append((prior, col_avg_y))</span><br></pre></td></tr></table></figure>
<p>X_new中通过loc函数先占个坑，然后进入交叉验证：（补充：建议原始的输入变量X和y先shuffle一下再进入计算）</p>
<p>然后我们就进入了核心的实现 MeanEncoder.mean_encode_subroutine（静态函数）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">@staticmethod</span><br><span class="line">def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):</span><br><span class="line">    X_train = X_train[[variable]].copy()</span><br><span class="line">    X_test = X_test[[variable]].copy()</span><br><span class="line"> </span><br><span class="line">    if target is not None:</span><br><span class="line">        nf_name = &#x27;&#123;&#125;_pred_&#123;&#125;&#x27;.format(variable, target)</span><br><span class="line">        X_train[&#x27;pred_temp&#x27;] = (y_train == target).astype(int)  # classification</span><br><span class="line">    else:</span><br><span class="line">        nf_name = &#x27;&#123;&#125;_pred&#x27;.format(variable)</span><br><span class="line">        X_train[&#x27;pred_temp&#x27;] = y_train  # regression</span><br><span class="line">    prior = X_train[&#x27;pred_temp&#x27;].mean()</span><br><span class="line"> </span><br><span class="line">    col_avg_y = X_train.groupby(by=variable, axis=0)[&#x27;pred_temp&#x27;].agg(&#123;&#x27;mean&#x27;: &#x27;mean&#x27;, &#x27;beta&#x27;: &#x27;size&#x27;&#125;)</span><br><span class="line">    col_avg_y[&#x27;beta&#x27;] = prior_weight_func(col_avg_y[&#x27;beta&#x27;])</span><br><span class="line">    col_avg_y[nf_name] = col_avg_y[&#x27;beta&#x27;] * prior + (1 - col_avg_y[&#x27;beta&#x27;]) * col_avg_y[&#x27;mean&#x27;]</span><br><span class="line">    col_avg_y.drop([&#x27;beta&#x27;, &#x27;mean&#x27;], axis=1, inplace=True)</span><br><span class="line"> </span><br><span class="line">    nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values</span><br><span class="line">    nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values</span><br><span class="line"> </span><br><span class="line">    return nf_train, nf_test, prior, col_avg_y</span><br></pre></td></tr></table></figure>
<p>还是举个例子好理解吧，假设这里categorical_features=[‘f1’,’f2’,’f3’],target=[0,1,2]，那么这里我们以variable=‘f1’，target=0为例来计算，首先是去原始数据中标签为0的样本的f1特征：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train[&#x27;pred_temp&#x27;] = (y_train == target).astype(int)</span><br></pre></td></tr></table></figure>
<p>然后是根据X_train[‘pred_temp’]的来计算target为0的样本的占比情况以作为prior先验概率的值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prior = X_train[&#x27;pred_temp&#x27;].mean()</span><br></pre></td></tr></table></figure>
<p>然后接下来的计算方式和target encoding是一致的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">col_avg_y = X_train.groupby(by=variable, axis=0)[&#x27;pred_temp&#x27;].agg(&#123;&#x27;mean&#x27;: &#x27;mean&#x27;, &#x27;beta&#x27;: &#x27;size&#x27;&#125;)</span><br><span class="line">col_avg_y[&#x27;beta&#x27;] = prior_weight_func(col_avg_y[&#x27;beta&#x27;])</span><br><span class="line">col_avg_y[nf_name] = col_avg_y[&#x27;beta&#x27;] * prior + (1 - col_avg_y[&#x27;beta&#x27;]) * col_avg_y[&#x27;mean&#x27;]</span><br><span class="line">col_avg_y.drop([&#x27;beta&#x27;, &#x27;mean&#x27;], axis=1, inplace=True)</span><br></pre></td></tr></table></figure>
<p>唯一不同的方式是，mean encoding这里用到了交叉计算的方式，以5折交叉为例，在80%的数据上计算编码结果得到转换的规则，然后将剩下20%的数据按照转换规则进行转换，最后将结果返回：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values</span><br><span class="line">nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values</span><br><span class="line"> </span><br><span class="line">return nf_train, nf_test, prior, col_avg_y</span><br></pre></td></tr></table></figure>
<p>最后：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_new.iloc[small_ind, -1] = nf_small</span><br></pre></td></tr></table></figure>
<p>把测试集的转换结果赋给原始数据的copy。</p>
<p><strong>综上所属，mean encoding的原理和target encoding基本是一样的，只不过比target encoding多了一个交叉计算的步骤，假设有10000条数据，target encoding是直接在这10000条数据上进行编码结果的计算的，而mean encoding则是每次用类似与模型训练的方法，比如五折交叉计算，用80%的数据计算编码结果然后赋给剩下的20%，重复5次则所有特征都编码完毕，这样的好处就是一定程度上降低过拟合的影响。</strong></p>
<p>实际上均值编码的思路挺好，但凡是涉及到标签的编码方式多少都会有信息泄漏的问题，并且如果训练集和测试集的标签分布很不均衡，有监督编码很容易称为罪魁祸首。</p>
<h1 id="M-Estimate-Encoder"><a href="#M-Estimate-Encoder" class="headerlink" title="M-Estimate Encoder"></a>M-Estimate Encoder</h1><p>相当于 一个简化版的Target Encoder</p>
<script type="math/tex; mode=display">
\hat{x}^{k}=\frac{n^{+}+\text {prior } \times m}{y^{+}+m}</script><p>其中 $y^+$ 代表所有正Label的个数，$m$ 是一个调参的参数，$m$ 越大过拟合的程度就会越小，同样的在处理连续值时$n^+$ 可以换成label的求和，$y^+$ 换成所有正label的求和。</p>
<h1 id="James-Stein-Encoder"><a href="#James-Stein-Encoder" class="headerlink" title="James-Stein Encoder"></a>James-Stein Encoder</h1><p>James-Stein Encoder 同样是基于target的一种算法。算法的思想很简单，对于特征的每个取值 $k$ 可以根据下面的公式获得：</p>
<script type="math/tex; mode=display">
\hat{x}^{k}=(1-B) \times \frac{n^{+}}{n}+B \times \frac{y^{+}}{y}</script><p>其中 $B$ 由以下公式估计：</p>
<script type="math/tex; mode=display">
B=\frac{\operatorname{Var}\left[y^{k}\right]}{\operatorname{Var}\left[y^{k}\right]+\operatorname{Var}[y]}</script><p>但是它有一个要求是target必须符合正态分布，这对于分类问题是不可能的，因此可以把 $y$ 先转化成概率的形式。或者在实际操作中，使用grid search的方法选择一个比较好的B值。</p>
<h1 id="Weight-of-Evidence-Encoder"><a href="#Weight-of-Evidence-Encoder" class="headerlink" title="Weight of Evidence Encoder"></a>Weight of Evidence Encoder</h1><p><strong>Weight Of Evidence</strong> 同样是基于target的方法。</p>
<script type="math/tex; mode=display">
W O E_i=\ln \left(\frac{p y_i}{p n_i}\right)=\ln \left(\frac{\sharp y_i / \sharp y_T}{\sharp n_i / \sharp}\right)=\ln \left(\frac{\sharp y_i / \sharp n_i}{\sharp y_T / \sharp n}\right)</script><p>woe编码仅仅针对于二分类问题，实际上woe编码的方法很容易就可以扩展到多类，后面会写。单纯从woe的公式就可以看出woe编码存在的问题：</p>
<ol>
<li><strong>分母可能为0的问题；</strong></li>
<li><strong>类似于直方图编码，没有考虑到不同类别数量的大小，例如类别特征为【A，A，A，A，A，A，B】而标签为【0，0，0，1，1，1，1】这样的情况计算出来的woe明显对A这个类别不公平</strong></li>
<li><strong>应用局限性太大了，只能针对二分类问题，并且特征也必须为离散特征。</strong></li>
<li><strong>训练集计算的woe编码结果可能和测试集计算的woe编码结果存在较大差异（所有基于统计特征的编码方式的通病）</strong></li>
</ol>
<p>首先我们调个包，使用到的是注明scikit-learn contrib分支中的category_encoders：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from category_encoders import *</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.datasets import load_boston</span><br><span class="line">bunch = load_boston()</span><br><span class="line">y = bunch.target &gt; 22.5</span><br><span class="line">X = pd.DataFrame(bunch.data, columns=bunch.feature_names)</span><br><span class="line">enc = WOEEncoder(cols=[&#x27;CHAS&#x27;, &#x27;RAD&#x27;]).fit(X, y)</span><br><span class="line">numeric_dataset = enc.transform(X)</span><br></pre></td></tr></table></figure>
<p>通过查看内部核心实现代码，对比原始公式：</p>
<script type="math/tex; mode=display">
W O E_i=\ln \left(\frac{p y_i}{p n_i}\right)=\ln \left(\frac{\sharp y_i / \sharp y_T}{\sharp n_i / \sharp}\right)=\ln \left(\frac{\sharp y_i / \sharp n_i}{\sharp y_T / \sharp n}\right)</script><p>源代码中大致实现了上图的计算逻辑，为了避免除0的问题，引入了“regulation”这个参数（用户自定义，默认为1）来进行拉普拉斯平滑。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nominator = (stats[&#x27;sum&#x27;] + self.regularization) / (self._sum + 2*self.regularization)</span><br><span class="line">denominator = ((stats[&#x27;count&#x27;] - stats[&#x27;sum&#x27;]) + self.regularization) / (self._count - self._sum + 2*self.regularization)</span><br></pre></td></tr></table></figure>
<p>所以对于很小样本的数据进行woe编码计算的结果会和实际计算上有出入，不过说实话如果样本数量很小个人认为没有太多编码的必要吧，统计学意义非常不明显。</p>
<p>这里我们只解决了上面除0的问题，对于问题二，我们可以使用IV值的思路，针对类别特征中不同类别的数量给woe的公式施加一个惩罚项：</p>
<p><img src="https://image.rexking6.top/img/v2-aad48636aa1ca8b969f8a0a2a3fdd150_720w.webp" alt=""></p>
<p>这样就把样本数量的问题也考虑进去了。实现也很简单，计算出woe编码结果之后再计算惩罚项然后相乘即可，不赘述了。</p>
<p>针对问题3，如果要拓展多多分类，我想到的思路是使用直方图编码的思路：修改的思路是，分子为类别特征中第i个类别中的 y_i/y_sum，分母为所有训练样本中的yi/y_sum，举个例子把，例如类别特征为【A，A，A，B，B】，标签为【0，0，1，2，1】，则对于A，类别0的编码的计算过程为ln(2/3 / 2/5)依次类推，不过就是不知道这种编码结果效果好不好。</p>
<p>针对问题4，没想出来什么好的办法。</p>
<h1 id="Catboost-Encoder"><a href="#Catboost-Encoder" class="headerlink" title="Catboost Encoder"></a>Catboost Encoder</h1><p>根据官方描述，是把catboost对类别特征的编码方案直接提取出来写成一个独立的模块了，catboost还没怎么仔细研究过，先熟悉一下编码方案也是好的，直接看源代码吧：</p>
<p><img src="https://image.rexking6.top/img/v2-0eb2ec5ff0ee1448cd586f477f9a486b_720w.webp" alt=""></p>
<p>核心代码，进入看看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def fit_leave_one_out(self, X_in, y, cols=None):</span><br><span class="line">    X = X_in.copy(deep=True)</span><br><span class="line">    if cols is None:</span><br><span class="line">        cols = X.columns.values</span><br><span class="line">    self._mean = y.mean() # 计算标签值的均值</span><br><span class="line">    return &#123;col: self.fit_column_map(X[col], y) for col in cols&#125;</span><br><span class="line"></span><br><span class="line">def fit_column_map(self, series, y):</span><br><span class="line">    category = pd.Categorical(series)</span><br><span class="line">    categories = category.categories</span><br><span class="line">    codes = category.codes.copy()</span><br><span class="line">    codes[codes == -1] = len(categories)</span><br><span class="line">    categories = np.append(categories, np.nan)</span><br><span class="line">    return_map = pd.Series(dict([(code, category) for code, category in enumerate(categories)]))</span><br><span class="line">    result = y.groupby(codes).agg([&#x27;sum&#x27;, &#x27;count&#x27;])</span><br><span class="line">    return result.rename(return_map)</span><br></pre></td></tr></table></figure>
<p>为了方便叙述这里拿一些示例数据进去试试看：</p>
<p>这是要转化的类别特征</p>
<p><img src="https://image.rexking6.top/img/v2-14e4a3bc4224ddfb7ff356d6a36d8536_720w.webp" alt=""></p>
<p>这是标签数据，二分类标签：</p>
<p><img src="https://image.rexking6.top/img/v2-6b0a6935db6a4331b71137a36429166a_720w.webp" alt=""></p>
<p>最终输出的result为：</p>
<p><img src="https://image.rexking6.top/img/v2-44453b29fa2d9b6960ca8383e74e62ba_720w.webp" alt=""></p>
<p>从这里可以看出，一开始是计算了所有标签值的均值（无论是分类还是回归问题都是），接下来的处理比较kaggle，catboost的分类编码方案把所有的np.nan，也就是缺失值都当作类别变量加入到最终的编码方案中，在kaggle上也很常见经常会把缺失值转化为一个‘missing’的类别，把缺失也当作一种信息。 另外gbdt系列的算法在缺失值特别多的情况下非常容易过拟合，比如10000个样本中某个类别特征A存在80%的缺失率，那么分裂的时候，lightgbm（xgb无法处理类别特征这里不讨论）会直接在剩下的20%的样本中进行分裂与决策，然后剩下的80%的样本归入增益大的分支，显然这样非常容易过拟合。另外可以看到category这个数据类型将缺失值的标签默认设置为-1。</p>
<p>中间一大堆检查调用之类的无关的步骤省略直接进入核心代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def transform_leave_one_out(self, X_in, y, mapping=result):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Leave one out encoding uses a single column of floats to represent the means of the target variables.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        X = X_in.copy(deep=True)</span><br><span class="line">        random_state_ = check_random_state(self.random_state)</span><br><span class="line"></span><br><span class="line">        # Prepare the data</span><br><span class="line">        if y is not None: #如果存在y的话，把X中的所有缺失值用-999来代替</span><br><span class="line">            # Convert bools to numbers (the target must be summable)</span><br><span class="line">            y = y.astype(&#x27;double&#x27;)</span><br><span class="line"></span><br><span class="line">            # Cumsum and cumcount do not work nicely with None.</span><br><span class="line">            # This is a terrible workaround that will fail, when the</span><br><span class="line">            # categorical input contains -999.9</span><br><span class="line">            for cat_col in X.select_dtypes(&#x27;category&#x27;).columns.values:</span><br><span class="line">                X[cat_col] = X[cat_col].cat.add_categories(-999.9)</span><br><span class="line">            X = X.fillna(-999.9)</span><br><span class="line"></span><br><span class="line">        for col, colmap in mapping.items():</span><br><span class="line">            level_notunique = colmap[&#x27;count&#x27;] &gt; 1 #数量等于1的类别全部忽略剩下的留下来</span><br><span class="line"></span><br><span class="line">            unique_train = colmap.index #根据训练集得到的类别的标识</span><br><span class="line">            unseen_values = pd.Series([x for x in X_in[col].unique() if x not in unique_train])#如果新的数据集存在</span><br><span class="line">#训练集中没有的类别则存放到unseen_values中</span><br><span class="line"></span><br><span class="line">            is_nan = X_in[col].isnull()#返回数据集中缺失值的位置</span><br><span class="line">            is_unknown_value = X_in[col].isin(unseen_values.dropna()) #获取测试集中带有训练集未出现标签的样本</span><br><span class="line">#但是排除缺失值</span><br><span class="line"></span><br><span class="line">            if self.handle_unknown == &#x27;error&#x27; and is_unknown_value.any(): #建议还是报错比较好，要不然自己都不知道</span><br><span class="line">#底层怎么处理新数据的</span><br><span class="line">                raise ValueError(&#x27;Columns to be encoded can not contain new values&#x27;)</span><br><span class="line"></span><br><span class="line">            if y is None:    # Replace level with its mean target; if level occurs only once, use global mean</span><br><span class="line"></span><br><span class="line">                level_means = ((colmap[&#x27;sum&#x27;] + self._mean) / (colmap[&#x27;count&#x27;] + 1)).where(level_notunique, self._mean)</span><br><span class="line"></span><br><span class="line">                X[col] = X[col].map(level_means)</span><br><span class="line"></span><br><span class="line">            else:</span><br><span class="line">                # Simulation of CatBoost implementation, which calculates leave-one-out on the fly.</span><br><span class="line">                # The nice thing about this is that it helps to prevent overfitting. The bad thing</span><br><span class="line">                # is that CatBoost uses many iterations over the data. But we run just one iteration.</span><br><span class="line">                # Still, it works better than leave-one-out without any noise.</span><br><span class="line">                # See:</span><br><span class="line">                #   https://tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages_cat-to-numberic-docpage/</span><br><span class="line">                temp = y.groupby(X[col]).agg([&#x27;cumsum&#x27;, &#x27;cumcount&#x27;])</span><br><span class="line">                X[col] = (temp[&#x27;cumsum&#x27;] - y + self._mean) / (temp[&#x27;cumcount&#x27;] + 1)</span><br><span class="line"></span><br><span class="line">            if self.handle_unknown == &#x27;value&#x27;:</span><br><span class="line">                X.loc[is_unknown_value, col] = self._mean</span><br><span class="line">            elif self.handle_unknown == &#x27;return_nan&#x27;:</span><br><span class="line">                X.loc[is_unknown_value, col] = np.nan</span><br><span class="line"></span><br><span class="line">            if self.handle_missing == &#x27;value&#x27;:</span><br><span class="line">                X.loc[is_nan &amp; unseen_values.isnull().any(), col] = self._mean</span><br><span class="line">            elif self.handle_missing == &#x27;return_nan&#x27;:</span><br><span class="line">                X.loc[is_nan, col] = np.nan</span><br><span class="line"></span><br><span class="line">            if self.sigma is not None and y is not None:</span><br><span class="line">                X[col] = X[col] * random_state_.normal(1., self.sigma, X[col].shape[0])</span><br><span class="line"></span><br><span class="line">        return X</span><br></pre></td></tr></table></figure>
<p>首先来看一下y=None的情况，(我没弄明白都fit完了为什么还要根据y来transform)我们来一步一步测试一下看看每步的输出：</p>
<p>首先是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">            level_notunique = colmap[&#x27;count&#x27;] &gt; 1 #数量等于1的类别全部忽略剩下的留下来</span><br><span class="line"></span><br><span class="line">            unique_train = colmap.index #根据训练集得到的类别的标识</span><br><span class="line">            unseen_values = pd.Series([x for x in X_in[col].unique() if x not in unique_train])#如果新的数据集存在</span><br><span class="line">#训练集中没有的类别则存放到unseen_values中</span><br><span class="line"></span><br><span class="line">            is_nan = X_in[col].isnull()#返回数据集中缺失值的位置</span><br><span class="line">            is_unknown_value = X_in[col].isin(unseen_values.dropna()) #获取测试集中带有训练集未出现标签的样本</span><br><span class="line">#但是排除缺失值</span><br><span class="line"></span><br><span class="line">            if self.handle_unknown == &#x27;error&#x27; and is_unknown_value.any(): #建议还是报错比较好，要不然自己都不知道</span><br><span class="line">#底层怎么处理新数据的</span><br><span class="line">                raise ValueError(&#x27;Columns to be encoded can not contain new values&#x27;)</span><br></pre></td></tr></table></figure>
<p><strong>把类别数量为1的类别忽略，得到类别数量大于1的</strong>：</p>
<p><img src="https://image.rexking6.top/img/v2-dde85d36b7f8772cd17ddb3f9a15b8bf_720w.webp" alt=""></p>
<p>得到所有类别的标识（包括类别为1的）unique_train = colmap.index：</p>
<p><img src="https://image.rexking6.top/img/v2-da4efaea588ce485f0d6e59f01b0dc9e_720w.webp" alt=""></p>
<p>获取transform数据集中没见过的类别，因为这里我们直接用训练集的数据来transform，所以这一项为空：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unseen_values = pd.Series([x for x in X.unique() if x not in unique_train])</span><br></pre></td></tr></table></figure>
<p><code>is_nan = X.isnull()</code>#返回数据集中缺失值的位置</p>
<p><img src="https://image.rexking6.top/img/v2-d06af20eedce764d404eefd60488826d_720w.webp" alt=""></p>
<p><code>is_unknown_value = X.isin(unseen_values.dropna())</code></p>
<p>获取原始类别数据中在transform数据集中未出现，这里用的就是原始数据所以当然没有unknown的数据了：</p>
<p><img src="https://image.rexking6.top/img/v2-91195ada03259565052446ca0bb1bc3b_720w.webp" alt=""></p>
<p>然后进入核心部分：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">    if y is None:    # Replace level with its mean target; if level occurs only once, use global mean</span><br><span class="line"></span><br><span class="line">        level_means = ((colmap[&#x27;sum&#x27;] + self._mean) / (colmap[&#x27;count&#x27;] + 1)).where(level_notunique, self._mean)</span><br><span class="line"></span><br><span class="line">        X[col] = X[col].map(level_means)</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line">        # Simulation of CatBoost implementation, which calculates leave-one-out on the fly.</span><br><span class="line">        # The nice thing about this is that it helps to prevent overfitting. The bad thing</span><br><span class="line">        # is that CatBoost uses many iterations over the data. But we run just one iteration.</span><br><span class="line">        # Still, it works better than leave-one-out without any noise.</span><br><span class="line">        # See:</span><br><span class="line">        #   https://tech.yandex.com/catboost/doc/dg/concepts/algorithm-main-stages_cat-to-numberic-docpage/</span><br><span class="line">        temp = y.groupby(X[col]).agg([&#x27;cumsum&#x27;, &#x27;cumcount&#x27;])</span><br><span class="line">        X[col] = (temp[&#x27;cumsum&#x27;] - y + self._mean) / (temp[&#x27;cumcount&#x27;] + 1)</span><br><span class="line"></span><br><span class="line">    if self.handle_unknown == &#x27;value&#x27;:</span><br><span class="line">        X.loc[is_unknown_value, col] = self._mean</span><br><span class="line">    elif self.handle_unknown == &#x27;return_nan&#x27;:</span><br><span class="line">        X.loc[is_unknown_value, col] = np.nan</span><br><span class="line"></span><br><span class="line">    if self.handle_missing == &#x27;value&#x27;:</span><br><span class="line">        X.loc[is_nan &amp; unseen_values.isnull().any(), col] = self._mean</span><br><span class="line">    elif self.handle_missing == &#x27;return_nan&#x27;:</span><br><span class="line">        X.loc[is_nan, col] = np.nan</span><br><span class="line"></span><br><span class="line">    if self.sigma is not None and y is not None:</span><br><span class="line">        X[col] = X[col] * random_state_.normal(1., self.sigma, X[col].shape[0])</span><br><span class="line"></span><br><span class="line">return X</span><br></pre></td></tr></table></figure>
<p>$Replace level with its mean target; if level occurs only once, use global mean</p>
<h2 id="不提供标签y的情况下"><a href="#不提供标签y的情况下" class="headerlink" title="不提供标签y的情况下"></a>不提供标签y的情况下</h2><p>把类别特征用它对应的数量来代替，如果类别只出现一次则用全部类别的数量之和除以类别数来代替。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">level_means =((colmap[&#x27;sum&#x27;]+ self._mean)/(colmap[&#x27;count&#x27;]+1)).where(level_notunique, self._mean)#</span><br></pre></td></tr></table></figure>
<p>这里的操作比较独特，对每一个类别对应的标签为1的样本的数量加上所有标签的均值得到分子，分母就是每个类别对应的总样本的数量，加一的操作是为了避免出现除0的操作，后面的where操作是这样的，对于每一个数量仅仅为1的类别，我们直接将其编码为标签值的均值 mean，否则就按照这里说的处理方式处理，这里where起到一个非常方便的if else的判断作用。</p>
<p><img src="https://image.rexking6.top/img/v2-1b9a9cfbdfff4cc0af9907039d47f493_720w.webp" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">if self.handle_unknown == &#x27;value&#x27;:</span><br><span class="line">    X.loc[is_unknown_value, col] = self._mean</span><br><span class="line">elif self.handle_unknown == &#x27;return_nan&#x27;:</span><br><span class="line">    X.loc[is_unknown_value, col] = np.nan</span><br><span class="line"></span><br><span class="line">if self.handle_missing == &#x27;value&#x27;:</span><br><span class="line">    X.loc[is_nan &amp; unseen_values.isnull().any(), col] = self._mean</span><br><span class="line">elif self.handle_missing == &#x27;return_nan&#x27;:</span><br><span class="line">    X.loc[is_nan, col] = np.nan</span><br></pre></td></tr></table></figure>
<p>这里是判断transform数据的缺失值和未见标签的编码方案，用标签均值代替或者直接用np.nan代替两种选择。</p>
<p>总结一下：</p>
<p>catboost编码的大体思路是：把类别特征中的每一个类别进行，假设有一个类别特征中有一个类别A，类别A中对应的好样本有1000个，坏样本有100个，总的好样本有10000个，坏样本有3000个则：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">level_means =((colmap[&#x27;sum&#x27;]+ self._mean)/(colmap[&#x27;count&#x27;]+1)).where(level_notunique, self._mean)</span><br></pre></td></tr></table></figure>
<p>的转换，根据这个转换公式，A的编码结果为： (100+0.3)/(1100+1)=0.09109900090826521</p>
<p>实际上0.3和1对于结果影响基本没有，去掉之后A的编码结果就是它对应的bad rate，mean 和 1 只是为了起到平滑的作用而已，而对于出现次数仅1次的类别来说直接用全部数据的bad rate来编码。原来catboost的编码这么简单吗。</p>
<h1 id="效果分析与讨论"><a href="#效果分析与讨论" class="headerlink" title="效果分析与讨论"></a>效果分析与讨论</h1><p>数据集使用了八个存在离散型变量的数据集，最后的结果加权如下：</p>
<ul>
<li><p>不使用交叉验证的情况：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">HelmertEncoder	        0.9517</span><br><span class="line">SumEncoder	        0.9434</span><br><span class="line">FrequencyEncoder	0.9176</span><br><span class="line">CatBoostEncoder	        0.5728</span><br><span class="line">TargetEncoder	        0.5174</span><br><span class="line">JamesSteinEncoder	0.5162</span><br><span class="line">OrdinalEncoder	        0.4964</span><br><span class="line">WOEEncoder	        0.4905</span><br><span class="line">MEstimateEncoder	0.4501</span><br><span class="line">BackwardDifferenceEncode0.4128</span><br><span class="line">LeaveOneOutEncoder	0.0697</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用交叉验证的情况：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CatBoostEncoder		0.9726</span><br><span class="line">OrdinalEncoder		0.9694</span><br><span class="line">HelmertEncoder		0.9558</span><br><span class="line">SumEncoder		0.9434</span><br><span class="line">WOEEncoder		0.9326</span><br><span class="line">FrequencyEncoder	0.9315</span><br><span class="line">BackwardDifferenceEncode0.9108</span><br><span class="line">TargetEncoder		0.8915</span><br><span class="line">JamesSteinEncoder	0.8555</span><br><span class="line">MEstimateEncoder	0.8189</span><br><span class="line">LeaveOneOutEncoder	0.0729</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>下面是Kaggle上大佬们给出的一些建议，具体原因尚未分析。</p>
<ul>
<li><p>对于无序的离散特征，实战中使用 OneHot, Hashing, LeaveOneOut, and Target encoding 方法效果较好，但是使用OneHot时要避免高基类别的特征以及基于决策树的模型，理由如下图所示。</p>
<p><img src="https://image.rexking6.top/img/v2-21ac50ef9105de8ffd6aeb6f1a79d010_720w.webp" alt=""></p>
<p>但是在实战中，我发现使用Xgboost处理高维稀疏的问题效果并不会很差。例如在IJCAI-18商铺中用户定位比赛中，一个很好的baseline就是把高维稀疏的wifi信号向量直接当做特征放到Xgboost里面，也可以获得很好的预测结果。不知道是不是因为Xgboost对于稀疏特征的优化导致。</p>
</li>
<li><p>对于有序离散特征，尝试 Ordinal (Integer), Binary, OneHot, LeaveOneOut, and Target. Helmert, Sum, BackwardDifference and Polynomial 基本没啥用，但是当你有确切的原因或者对于业务的理解的话，可以进行尝试。</p>
</li>
<li><p>对于回归问题而言，Target 与 LeaveOneOut 方法可能不会有比较好的效果。</p>
</li>
<li><p>LeaveOneOut、 WeightOfEvidence、 James-Stein、M-estimator 适合用来处理高基数特征。Helmert、 Sum、 Backward Difference、 Polynomial 在机器学习问题里的效果往往不是很好（过拟合的原因）</p>
</li>
</ul>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\08\30\A-B测试\" rel="bookmark">A/B测试</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2023\03\31\D-Separation和PC算法\" rel="bookmark">D-Separation和PC算法</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2019\03\21\Fisher线性判别\" rel="bookmark">Fisher线性判别</a></div>
    </li>
  </ul>

        <div class="reward-container">
  <div>一分一毛，也是心意。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Run-Qing Chen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Run-Qing Chen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Run-Qing Chen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://blog.rexking6.top/2022/07/21/%E7%A6%BB%E6%95%A3%E5%9E%8B%E7%89%B9%E5%BE%81%E7%BC%96%E7%A0%81/" title="离散型特征编码">https://blog.rexking6.top/2022/07/21/离散型特征编码/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/07/12/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E5%85%A5%E9%97%A8/" rel="prev" title="语音识别入门">
      <i class="fa fa-chevron-left"></i> 语音识别入门
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/07/28/Python%E7%9A%84%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%B9%B6%E8%A1%8C/" rel="next" title="Python的并发与并行">
      Python的并发与并行 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

    <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Category-Encoders"><span class="nav-number">2.</span> <span class="nav-text">Category Encoders</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#OneHot-Encoder-Dummy-Encoder-OHE"><span class="nav-number">3.</span> <span class="nav-text">OneHot Encoder&#x2F;Dummy Encoder&#x2F;OHE</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dummy-encoding"><span class="nav-number">4.</span> <span class="nav-text">dummy encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#one-hot%E7%BC%96%E7%A0%81%E5%92%8Cdummy%E7%BC%96%E7%A0%81%EF%BC%9A%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB"><span class="nav-number">4.1.</span> <span class="nav-text">one-hot编码和dummy编码：区别与联系</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Label-Encoder-Ordered-Encoder"><span class="nav-number">5.</span> <span class="nav-text">Label Encoder&#x2F;Ordered Encoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#label-binarize-%E4%BA%8C%E5%80%BC%E5%8C%96%E7%BC%96%E7%A0%81"><span class="nav-number">6.</span> <span class="nav-text">label_binarize 二值化编码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Frequency-Encoder-Count-Encoder"><span class="nav-number">7.</span> <span class="nav-text">Frequency Encoder&#x2F;Count Encoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%BC%96%E7%A0%81"><span class="nav-number">8.</span> <span class="nav-text">直方图编码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%E8%BF%9B%E5%88%B6%E7%BC%96%E7%A0%81%E4%B8%8EN%E8%BF%9B%E5%88%B6%E7%BC%96%E7%A0%81"><span class="nav-number">9.</span> <span class="nav-text">二进制编码与N进制编码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Sum-Encoder-Deviation-Encoder-Effect-Encoder"><span class="nav-number">10.</span> <span class="nav-text">Sum Encoder&#x2F;Deviation Encoder&#x2F;Effect Encoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Leave-one-out-Encoder-LOO-or-LOOE"><span class="nav-number">11.</span> <span class="nav-text">Leave-one-out Encoder (LOO or LOOE)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Helmet-Encoder"><span class="nav-number">12.</span> <span class="nav-text">Helmet Encoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Target-Encoder"><span class="nav-number">13.</span> <span class="nav-text">Target Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-number">13.1.</span> <span class="nav-text">分类问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="nav-number">13.2.</span> <span class="nav-text">回归问题</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mean-encoding"><span class="nav-number">14.</span> <span class="nav-text">mean encoding</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#M-Estimate-Encoder"><span class="nav-number">15.</span> <span class="nav-text">M-Estimate Encoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#James-Stein-Encoder"><span class="nav-number">16.</span> <span class="nav-text">James-Stein Encoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Weight-of-Evidence-Encoder"><span class="nav-number">17.</span> <span class="nav-text">Weight of Evidence Encoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Catboost-Encoder"><span class="nav-number">18.</span> <span class="nav-text">Catboost Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E6%8F%90%E4%BE%9B%E6%A0%87%E7%AD%BEy%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B"><span class="nav-number">18.1.</span> <span class="nav-text">不提供标签y的情况下</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%95%88%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E8%AE%A8%E8%AE%BA"><span class="nav-number">19.</span> <span class="nav-text">效果分析与讨论</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Run-Qing Chen"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Run-Qing Chen</p>
  <div class="site-description" itemprop="description">覆苍天以为衾，卧大地以为庐。</div>
</div>


   <div class="feed-link motion-element">
     <a href="/atom.xml" rel="alternate">
       <i class="fa fa-rss"></i>
       RSS
     </a>
   </div>
 
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">210</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/RexKing6" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;RexKing6" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1010026261@qq.com" title="E-Mail → mailto:1010026261@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zxpblog.cn/" title="https:&#x2F;&#x2F;www.zxpblog.cn&#x2F;" rel="noopener" target="_blank">赵小平</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://whitepuffer.github.io/" title="https:&#x2F;&#x2F;whitepuffer.github.io&#x2F;" rel="noopener" target="_blank">江斓</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://kexue.fm/" title="https:&#x2F;&#x2F;kexue.fm&#x2F;" rel="noopener" target="_blank">科学空间</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yongyuan.name/" title="https:&#x2F;&#x2F;yongyuan.name&#x2F;" rel="noopener" target="_blank">袁勇</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/abcjennifer" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;abcjennifer" rel="noopener" target="_blank">Rachel Zhang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dmkf.xyz/" title="http:&#x2F;&#x2F;dmkf.xyz&#x2F;" rel="noopener" target="_blank">代码咖啡</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://wuxiaolong.me/" title="http:&#x2F;&#x2F;wuxiaolong.me&#x2F;" rel="noopener" target="_blank">吴小龙同学</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.tennfy.com/" title="http:&#x2F;&#x2F;www.tennfy.com&#x2F;" rel="noopener" target="_blank">TENNFY WU</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fab fa-accessible-icon"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Run-Qing Chen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">3.7m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">56:12</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"6XDsO3aHIjDk3nV6eLJCufbl-MdYXbMMI","app_key":"YK4qOc0TpkazN6exhuqsnwmB","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>

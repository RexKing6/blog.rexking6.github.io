<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.rexking6.top","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#37c6c0","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"YS7HT61SEB","apiKey":"0fd1eba022e7883c76ff4a71aee2acdc","indexName":"blog_NAME","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"找不到关于 ${query} 的文章","hits_stats":"共找到 ${hits} 篇文章，花了 ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="...">
<meta property="og:type" content="article">
<meta property="og:title" content="序列模型中的注意力机制">
<meta property="og:url" content="https://blog.rexking6.top/2019/09/20/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">
<meta property="og:site_name" content="RexKing6&#39;s Note">
<meta property="og:description" content="...">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://image.rexking6.top/img/clip1568978764.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569053921.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569054540.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569055408.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569055499.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569055576.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569055900.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569056073.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569056190.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569056891.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569056924.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569067029.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569065791.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569118390.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569118927.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569119387.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569119404.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569120673.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569120778.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569121072.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569121239.png">
<meta property="og:image" content="https://pic2.zhimg.com/v2-e5ff03d223bf846c7442b2ac4a876cc1_r.jpg">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569066684.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569066702.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1569066709.png">
<meta property="article:published_time" content="2019-09-20T08:56:52.000Z">
<meta property="article:modified_time" content="2021-07-10T11:28:07.560Z">
<meta property="article:author" content="Run-Qing Chen">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://image.rexking6.top/img/clip1568978764.png">

<link rel="canonical" href="https://blog.rexking6.top/2019/09/20/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>序列模型中的注意力机制 | RexKing6's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="RexKing6's Note" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RexKing6's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/rexking6" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.rexking6.top/2019/09/20/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Run-Qing Chen">
      <meta itemprop="description" content="覆苍天以为衾，卧大地以为庐。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RexKing6's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          序列模型中的注意力机制
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-20 16:56:52" itemprop="dateCreated datePublished" datetime="2019-09-20T16:56:52+08:00">2019-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-10 19:28:07" itemprop="dateModified" datetime="2021-07-10T19:28:07+08:00">2021-07-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          
            <span id="/2019/09/20/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="post-meta-item leancloud_visitors" data-flag-title="序列模型中的注意力机制" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>8.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>综合转载以下文章：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51383402">完全解析RNN, Seq2Seq, Attention注意力机制</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40920384">真正的完全图解Seq2Seq Attention模型</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53682800">nlp中的Attention注意力机制+Transformer详解</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35739040">Attention用于NLP的一些小结</a></li>
</ul>
<h1 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h1><h2 id="经典RNN结构"><a href="#经典RNN结构" class="headerlink" title="经典RNN结构"></a>经典RNN结构</h2><p><img src="http://image.rexking6.top/img/clip1568978764.png" alt=""></p>
<p>抛开起始符和终止符不说，输入和输出序列必须要有相同的时间长度。这个对很多序列任务来说是个问题，例如文本翻译、阅读理解等，所以就有了Seq2Seq结构。</p>
<p><img src="http://image.rexking6.top/img/clip1569053921.png" alt=""></p>
<p>编码器Encoder把所有的输入序列都编码成一个统一的语义向量context（其实就是最后一个时刻的隐藏状态），然后再由解码器Decoder解码。在解码器Decoder解码的过程中，第一个时刻输入起始符，之后不断地将前一个时刻 $t-1$ 的输出作为后一个时刻 $t$ 的输入，循环解码，直到输出终止符符为止。Seq2Seq结构不再要求输入和输出序列有相同的时间长度，所以它被应用于很多问题中。</p>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>在将字词和标识符作为输入时，需要把它们转化为向量，称为“嵌入”。</p>
<ol>
<li><p>最简单是one-hot编码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&lt;start&gt;&#x27; : 0  &lt;-----&gt; [1, 0, 0, 0, ..., 0].T</span><br><span class="line">&#x27;&lt;stop&gt;&#x27; :  1  &lt;-----&gt; [0, 1, 0, 0, ..., 0].T</span><br><span class="line">&#x27;good&#x27; :    2  &lt;-----&gt; [0, 0, 1, 0, ..., 0].T</span><br><span class="line">&#x27;morning&#x27; : 3  &lt;-----&gt; [0, 0, 0, 1, ..., 0].T</span><br></pre></td></tr></table></figure>
<p>one-hot编码过于稀疏。</p>
</li>
<li><p>随机矩阵</p>
<p>首先生成一个embedding随机矩阵，之后每一行随机向量代表一个字词/标识符。在Tensorflow中上述过程通过以下函数实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.embedding_lookup</span><br></pre></td></tr></table></figure>
<p>而在pytorch中通过以下接口实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Embedding</span><br></pre></td></tr></table></figure>
<p>需要注意的是：train和test阶段必须使用一样的embedding矩阵。</p>
</li>
</ol>
<p>另外，还有word2vec/elmo/bert等方法，而且有的方法也可以在训练过程中迭代更新embedding。</p>
<h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><p>在seq2seq结构中将 $y_t$ 作为下一时刻输入 $x_{t+1}\Leftarrow y_t$ 进网络，那么某一时刻输出 $y_t$ 错误就会导致后面全错。在训练时由于网络尚未收敛，这种效应格外明显。为了解决这个问题，Google提出了Scheduled Sampling，即在训练中 $x_t$ 按照一定概率选择输入 $y_{t-1}$ 或 $y$ 时刻对应的真实标签，如图。</p>
<p><img src="http://image.rexking6.top/img/clip1569054540.png" alt=""></p>
<h1 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h1><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>当用神经网络来处理大量的输入信息时，可以借鉴人脑的注意力机制，只选择一些关键的信息输入进行处理，来提高效率。目前总体上分为两类：</p>
<ul>
<li><p>聚焦式（focus）注意力：自上而下的有意识的注意力，主动注意—是指有预定目的、依赖任务的、主动有意识地聚焦于某一对象的注意力；</p>
</li>
<li><p>显著性（saliency-based）注意力：自下而上的有意识的注意力，被动注意—基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关；可以将最大池化和门控机制来近似地看作是自下而上的基于显著性的注意力机制。</p>
<p><em>这么描述也是不太准确的，显著性注意力主要应用于弱监督的目标检测和语义分割，它们所指的显著性并不是被动的，而是先有了分类这个任务的需求，然后才注意到某些高显著性的区域。当然，这边指的任务无关，可能指的是当前任务。</em></p>
</li>
</ul>
<p>目前，注意力机制一般就是特指聚焦式注意力。</p>
<p>seq2seq的Encoder把所有的输入序列都编码成一个统一的语义向量context，然后再由Decoder解码。由于context包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。如机器翻译问题，当要翻译的句子较长时，一个Context可能存不下那么多信息，就会造成精度的下降。除此之外，如果按照上述方式实现，只用到了编码器的最后一个隐藏层状态，信息利用率低下。所以如果要改进Seq2Seq结构，就是利用Encoder所有隐藏层状态 $h_t$ 解决context长度限制问题。</p>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><p>主要结构图：</p>
<p><img src="http://image.rexking6.top/img/clip1569055408.png" alt=""></p>
<p><em>左侧为Encoder+输入，右侧为Decoder+输出。中间为Attention。</em></p>
<ol>
<li><p>$h_t=\text{RNN}_{\text{enc}}(x_t,h_{t-1})$，Encoder方面接受的是每一个单词的word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。</p>
<p><img src="http://image.rexking6.top/img/clip1569055499.png" alt=""></p>
</li>
<li><p>$s_t=\text{RNN}_{\text{dec}}(\hat y_{t-1},s_{t-1})$，Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。</p>
<p><img src="http://image.rexking6.top/img/clip1569055576.png" alt=""></p>
</li>
<li><p>$e_{ij}=\text{score}(s_i,h_j)$，通过Decoder的hidden states与Encoder的hidden states来计算一个分数，用于计算权重。</p>
</li>
<li><p>$\alpha_{ij}=\frac{\exp(e_{ij})}{\sum_{k=1}^{T_x}\exp (e_{ik})}$，每一个Encoder的hidden states在当前时间步对应的权重。</p>
</li>
<li><p>$c_i=\sum_{j=1}^{T_x}\alpha_{ij}h_j$，context向量是一个对于Encoder输出的hidden states的一个加权平均。</p>
<p><img src="http://image.rexking6.top/img/clip1569055900.png" alt=""></p>
</li>
<li><p>将context向量作为下一个时刻的输入和目标的单词串起来作为LSTM的输入，之后又得到一个hiddn state，以此循环。</p>
<p><img src="http://image.rexking6.top/img/clip1569056073.png" alt=""></p>
</li>
<li><p>$\hat s_t=\tanh(W_c[c_t;s_t])$，将context向量和Decoder的hidden states拼接。</p>
</li>
<li><p>$p(y_t|y_{&lt;t},x)=\text{softmax}(W_s\hat s_t)$，计算最后的输出概率。</p>
<p><img src="http://image.rexking6.top/img/clip1569056190.png" alt=""></p>
</li>
</ol>
<h2 id="score-function"><a href="#score-function" class="headerlink" title="score function"></a>score function</h2><h3 id="Luong"><a href="#Luong" class="headerlink" title="Luong"></a>Luong</h3><script type="math/tex; mode=display">
\text{score}(h_t,\bar h_s)=\left\{\begin{aligned}&h_t^\top\bar h_s &&\text{dot}\\&h_t^\top W_a\bar h_s &&\text{general}\\&v_a^\top \tanh(W_a[h_t;\bar h_s])&&\text{concat}\end{aligned}\right.</script><h4 id="dot"><a href="#dot" class="headerlink" title="dot"></a>dot</h4><p><img src="http://image.rexking6.top/img/clip1569056891.png" alt=""></p>
<p>输入是Encoder的所有hidden states $H_t$：大小为 <code>(h_dim, sequence_length)</code>。Decoder在一个时刻上的hidden states $\bar h_s$：大小为 <code>(h_dim, 1)</code>。</p>
<ol>
<li>旋转 $H_t$ 与 $\bar h_s$ 做点乘得到一个 大小为 <code>(sequence_length, 1)</code> 的分数；</li>
<li>对分数做softmax，得到一个合为1的权重；</li>
<li>将 $H_t$ 与第2步得到的权重做点乘得到一个大小为 <code>(h_dim, 1)</code> 的context向量。</li>
</ol>
<h4 id="general"><a href="#general" class="headerlink" title="general"></a>general</h4><p><img src="http://image.rexking6.top/img/clip1569056924.png" alt=""></p>
<p>输入是Encoder的所有hidden states $H_t$：大小为 <code>(h_dim1, sequence_length)</code>。Decoder在一个时刻上的hidden states $\bar h_s$：大小为 <code>(h_dim2, 1)</code>。此处两个hidden state的维度并不一样。</p>
<ol>
<li>旋转 $H_t$ 与 $W_a$ <code>(h_dim1, h_dim2)</code> 做点乘， 再和 $\bar h_s$ 做点乘得到一个 大小为 <code>(sequence_length, 1)</code> 的分数；</li>
<li>对分数做softmax得到一个合为1的权重；</li>
<li>将 $H_t$ 与第2步得到的权重做点乘得到一个大小为 <code>(h_dim1, 1)</code> 的context向量。</li>
</ol>
<h3 id="scaled-dot"><a href="#scaled-dot" class="headerlink" title="scaled dot"></a>scaled dot</h3><script type="math/tex; mode=display">
\text{score}(h_t,\bar h_s)=\frac{h_t^\top\bar h_s}{\sqrt{d}}</script><h3 id="cosine"><a href="#cosine" class="headerlink" title="cosine"></a>cosine</h3><script type="math/tex; mode=display">
\text{score}(h_t,\bar h_s)=\cos(h_t^\top,\bar h_s)</script><h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><script type="math/tex; mode=display">
\text{score}(h_t,\bar h_s)=\sigma(w_2^\top\tanh(W_1[h_t,\bar h_s]+b_1)+b_2)</script><h3 id="location-based-function"><a href="#location-based-function" class="headerlink" title="location-based function"></a>location-based function</h3><script type="math/tex; mode=display">
\alpha_{i,j}=\text{softmax}(W_a\bar h_s)</script><p>这种方法的对齐分数仅从目标隐藏状态学习得到。</p>
<h2 id="另外一个角度：Key、Value和Query"><a href="#另外一个角度：Key、Value和Query" class="headerlink" title="另外一个角度：Key、Value和Query"></a>另外一个角度：Key、Value和Query</h2><p><img src="http://image.rexking6.top/img/clip1569067029.png" alt=""></p>
<p>Attention机制的实质其实就是一个寻址（addressing）的过程，如上图所示：给定一个和任务相关的查询Query向量，通过计算与Key的相似性（注意力分布）并附加在Value上，从而计算Attention Value，这个过程实际上是Attention机制缓解神经网络模型复杂度的体现：不需要将所有的N个输入信息都输入到神经网络进行计算，只需要从X中选择一些和任务相关的信息输入给神经网络。</p>
<p>为什么要引入Key、Value和Query这个角度呢？因为很多Attention的模型的变种和任务的变种，都是围绕这三者的变化来做。</p>
<h2 id="观察"><a href="#观察" class="headerlink" title="观察"></a>观察</h2><p>在实际应用中当输入一组 $x$ ，除了可以获得输出  $y$，还能提取出 $x_t$ 与 $y_t$ 的权重数值 $\alpha_{i,j}$ 并画出来，这样就可以直观的看到时刻 $t$ 注意力机制到底“注意”了什么。</p>
<p><img src="http://image.rexking6.top/img/clip1569065791.png" alt=""></p>
<h1 id="Multi-dimensional-Attention"><a href="#Multi-dimensional-Attention" class="headerlink" title="Multi-dimensional Attention"></a>Multi-dimensional Attention</h1><p>Multi-dimensional Attention是一种较为容易理解的Attention变种。原始的Attention中，每个Query对应一组Attention分数。然而，这样的表示在很多时候会认为是信息不足的，当Key本身包括多维度信息时，显然我们需要多组Attention分数共同表示其结果。于是，Multi-dimensional Attention使用2维Attention而不是1维Attention，如下表：</p>
<p><img src="http://image.rexking6.top/img/clip1569118390.png" alt=""></p>
<p>最终，对于每个Query我们可以计算出多个加权求和的结果，因此如果想用向量表示结果，就需要将多个Attention结果拼接起来。</p>
<script type="math/tex; mode=display">
e_i=\tanh(u^\top \pmb W v_i)=\text{concat}(\tanh(u^\top W_kv_i))</script><p>此外，我们可能需要加入一些强约束来保证各组Attention能学到更加充分的信息，如果各组Attention的结果相同，那么将失去其意义。尽管随机初始化也可能隐式地让各组Attention能够到达自己的局部最优点，但显式的强约束将让该条件变得更加有力：</p>
<script type="math/tex; mode=display">
||AA^\top-I||_F^2</script><p>该式子中 $A$ 表示权重矩阵，$I$ 表示单位矩阵。利用上述约束，让权重矩阵A倾向于是正交的，进而保证各个权重维度都能尽量垂直。</p>
<p><em>这个 $A$ 权重矩阵是指注意力Attention还是指score计算时的 $\pmb W$ 矩阵？如果是注意力Attention的话，应该是 [$k$, $d$] 维的，那就是在 $k$ 个信道上要正交。</em></p>
<h1 id="Soft-Attention-和-Hard-Attention"><a href="#Soft-Attention-和-Hard-Attention" class="headerlink" title="Soft Attention 和 Hard Attention"></a>Soft Attention 和 Hard Attention</h1><ol>
<li><p>Soft Attention，这是比较常见的Attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）。这种方式比较理性，参考了所有key的内容，再进行加权。但是计算量可能会比较大一些。，每个权重取值在[0, 1]。其选择的信息是所有输入信息在注意力 分布下的期望。</p>
</li>
<li><p>Hard Attention，直接精准定位到某个key，这个key的概率是1，其余key的概率全部是0。因此这种对齐方式要求很高。有两种实现方式：1. 一种是选取最高概率的输入信息；2. 另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。如下图所示，Hard Attention在Soft Attention的第二步和第三步中间加入了采样层，这里使用的是蒙特卡洛采样，可以保证整个模型的端到端特性。在采样层中，以标准化后的权重为概率值，随机抽样出有限个权重值，抽样中的结果权重设置为1，其余设置为0。</p>
<blockquote>
<p>硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。硬性注意力需要通过强化学习（或者使用gumbel softmax之类的）来进行训练。——<a target="_blank" rel="noopener" href="https://nndl.github.io/">《神经网络与深度学习》</a></p>
</blockquote>
</li>
</ol>
<p><img src="http://image.rexking6.top/img/clip1569118927.png" alt=""></p>
<h1 id="Global-Attention-和-Local-Attention"><a href="#Global-Attention-和-Local-Attention" class="headerlink" title="Global Attention 和 Local Attention"></a>Global Attention 和 Local Attention</h1><p>原始的Attention即为Global Attention，具体地说，在计算语义向量时，会考虑编码器所有的隐藏状态。如下图：</p>
<p><img src="http://image.rexking6.top/img/clip1569119387.png" alt=""></p>
<p>由于Global Attention必须计算源句子序列所有隐藏状态，当句子长度过长会使得计算代价昂贵并使得注意力不集中，比如在翻译段落和文档的时候。因此可以通过限制注意力机制的范围，令注意力机制更加有效。</p>
<p>Local Attention中，计算语义向量时只关注每个目标词的一部分编码器隐藏状态。每个解码器的 $h_t$ 对应一个编码器位置 $p_t$，根据经验选定区间大小 $D$ ，进而只在编码器的 $[p_t-D, p_t+D]$ 位置使用注意力机制。根据不同的 $p_t$ 选择方式，存在两种常见的Local Attention分类：Local-m和Local-p。</p>
<ul>
<li>Local-m：简单设置 $p_t$ 为 $h_t$ 对应位置。</li>
</ul>
<script type="math/tex; mode=display">
p_t=t</script><ul>
<li>Local-p：利用 $h_t$ 预测 $p_t$，进而使用高斯分布令Local Attention的权重以 $p_t$ 呈现出峰值形状。</li>
</ul>
<script type="math/tex; mode=display">
p_t=S·\text{sigmoid}(v_p^\top\tanh(W_ph_t))\\
a_t(s)=\text{align}(h_t,\bar h_s)\exp(-\frac{(s-p_t)^2}{2\sigma^2})</script><p>​        在高斯分布中，我们设置：</p>
<script type="math/tex; mode=display">
\sigma=\frac{D}{2}</script><p><img src="http://image.rexking6.top/img/clip1569119404.png" alt=""></p>
<h1 id="Hierarchical-Attention"><a href="#Hierarchical-Attention" class="headerlink" title="Hierarchical Attention"></a>Hierarchical Attention</h1><p>Hierarchical Attention也可以用来解决长文本注意力不集中的问题，与Local Attention不同的是，Local Attention强行限制了注意力机制的范围，忽略剩余位置。而Hierarchical Attention则使用分层的思想，在所有的状态上都利用了注意力机制，如下图：</p>
<p><img src="http://image.rexking6.top/img/clip1569120673.png" alt=""></p>
<p>在篇章级文本分类中，文本由多个句子组成，句子由多个词语组成。在这样的思路中，首先分别在各个句子中使用注意力机制，提取出每个句子的关键信息，进而对每个句子的关键信息使用注意力机制，提取出文本的关键信息，最终利用文本的关键信息进行篇章及文本分类。这种结构能够反映文档的层次结构。模型在单词和句子级别分别设计了两个不同级别的注意力机制，这样做能够在构建文档表示时区别地对待这些内容。Hierarchical attention可以相应地构建分层注意力，自下而上（即词级到句子级）或自上而下（词级到字符级），即下图：</p>
<p><img src="http://image.rexking6.top/img/clip1569120778.png" alt=""></p>
<p>和机器翻译类似，作者依旧采用Encoder-Decoder架构，然后用Word-Level Attention对全局语法和流畅性纠错，设计Character-Level Attention对本地拼写错误纠正。</p>
<h1 id="Attention-Over-Attention"><a href="#Attention-Over-Attention" class="headerlink" title="Attention Over Attention"></a>Attention Over Attention</h1><p>Attention Over Attention在阅读理解的完形填空任务中提出，基本思想是对Attention之后的结果再进行Attention，但具体步骤区别于Hierarchical Attention。</p>
<p>在Attention Over Attention中，第一次Attention的对象是Query中的词向量和阅读中整个文本构成的词向量。在第一次Attention中，我们可以获取到一个权重矩阵，两个维度分别是Query长度和文本长度，横轴和纵轴分别代表一方对另一方的注意力分布。在文本长度上进行取均值并归一化，则能得出和Query长度相同的注意力平均分布向量，可以用来表示引入了注意力机制的Query。在Query长度上归一化，则可以表示文本中各个词关于Query的注意力分布矩阵。</p>
<p>在第二次Attention中，我们用第一次Attention的两个结果再次求Attention权重，可以得到一个请求关于阅读文本的注意力分布向量，进而可以用来进行完形填空，例如求出 $P(\text{“Mary”}|D, Q)$。模型架构图如下：</p>
<p><img src="http://image.rexking6.top/img/clip1569121072.png" alt=""></p>
<h1 id="Memory-based-Attention"><a href="#Memory-based-Attention" class="headerlink" title="Memory-based Attention"></a>Memory-based Attention</h1><p>Memory Network也可以视为是一种Attention的变种，这里列出End-to-End Memory Network来解释Memory-based Attention，如下图(a)：</p>
<p><img src="http://image.rexking6.top/img/clip1569121239.png" alt=""></p>
<p>图中是阅读理解任务，阅读理解任务中，给定一个阅读，阅读由多个语句构成，然后给定一个问题，返回一个答案。图中对阅读中的多个句子进行两套Embedding，蓝色部分表示抽离出的问题Embedding，对应于注意力机制中的Key，黄色部分表示抽离出的答案Embedding，对应于注意力机制中的Value。对于整个过程中，输入问题对应于注意力中的Query，与问题Embedding进行对齐匹配，得到权重后，与答案Embedding进行加权求和。整个过程中都可以和注意力机制中的步骤对应，如下：</p>
<p><img src="https://pic2.zhimg.com/v2-e5ff03d223bf846c7442b2ac4a876cc1_r.jpg" alt=""></p>
<p>通过第5步迭代内存更新（也称为多跳）来模拟时间推理过程，以逐步引导注意到答案的正确位置。在每次迭代中，使用新内容更新查询，并且使用更新的查询来检索相关内容。一种简单的更新方法为相加 $q_{t+1}=q_t+c_t$。</p>
<p><em>没看懂为什么要更新。</em></p>
<h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h1><p>见另外一篇博客：<a href="http://blog.rexking6.top/2019/09/22/Transformer/">Transformer</a>。</p>
<h1 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h1><p>Attention的评价方式主要分为两类：定量指标和定性指标。</p>
<h2 id="定量指标"><a href="#定量指标" class="headerlink" title="定量指标"></a>定量指标</h2><p>一般而言，Attention机制是模型的一部分，而非独立为一个模型。因此，想要定量评价Attention的效果一般都需要下游任务指标的支撑。例如，我们会使用BLEU值去评价机器翻译任务，那么可以在机器翻译模型中加入Attention机制，根据最终机器翻译的BLEU值的升降来评价Attention机制的好坏。</p>
<p>除此之外，也存在直接评价Attention机制的方法，但存在一定的困难。例如，某些数据存在实现标注好的对齐值，可以当做Attention的目标值，那么可以计算出实际值和目标值的差异来计算出对齐错误率（AER）。但这样的方式非常受限，标注对齐值的成本过大。</p>
<h2 id="定性指标"><a href="#定性指标" class="headerlink" title="定性指标"></a>定性指标</h2><p>定性指标也是Attention机制常用的一种评价标准，我们可以通过可视化的方式来定性评价Attention机制的好坏。</p>
<p><img src="http://image.rexking6.top/img/clip1569066684.png" alt=""></p>
<p>例如，在某个Seq2Seq任务中，我们可以用颜色表示Attention的权重值，深红色表示权重值接近于1，白色表示接近于0，那么我们可以通过颜色深浅的直观可视化表示来判断Attention机制的好坏。</p>
<p>此外，也存在其他类似的可视化表示：</p>
<p><img src="http://image.rexking6.top/img/clip1569066702.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1569066709.png" alt=""></p>
<h1 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h1><p>Attention机制比较适合有以下特点的任务：</p>
<ol>
<li>长文本任务，document级别，因为长文本本身所携带的信息量比较大，可能会带来信息过载问题，很多任务可能只需要用到其中一些关键信息（比如文本分类），所以Attention机制用在这里正适合捕获这些关键信息。</li>
<li>涉及到两段的相关文本，可能会需要对两段内容进行对齐，找到这两段文本之间的一些相关关系。比如机器翻译，将英文翻译成中文，英文和中文明显是有对齐关系的，Attention机制可以找出，在翻译到某个中文字的时候，需要对齐到哪个英文单词。又比如阅读理解，给出问题和文章，其实问题中也可以对齐到文章相关的描述，比如“什么时候”可以对齐到文章中相关的时间部分。</li>
<li>任务很大部分取决于某些特征。举个例子，比如在AI+法律领域，根据初步判决文书来预测所触犯的法律条款，在文书中可能会有一些罪名判定，而这种特征对任务是非常重要的，所以用Attention来捕获到这种特征就比较有用。</li>
</ol>
<p>下面的任务，其中机器翻译、摘要生成、图文互搜属于seq2seq任务，需要对两段内容进行对齐，文本蕴含用到前提和假设两段文本，阅读理解也用到了文章和问题两段文本，文本分类、序列标注和关系抽取属于单文本Attention的做法。</p>
<ol>
<li>机器翻译：Encoder用于对原文建模，Decoder用于生成译文，Attention用于连接原文和译文，在每一步翻译的时候关注不同的原文信息。</li>
<li>摘要生成：Encoder用于对原文建模，Decoder用于生成新文本，从形式上和机器翻译都是seq2seq任务，但是从任务特点上看，机器翻译可以具体对齐到某几个词，但这里是由长文本生成短文本，Decoder可能需要捕获到Encoder更多的内容，进行总结。</li>
<li>图文互搜：Encoder对图片建模，Decoder生成相关文本，在Decoder生成每个词的时候，用Attention机制来关注图片的不同部分。</li>
<li>文本蕴含：判断前提和假设是否相关，Attention机制用来对前提和假设进行对齐。</li>
<li>阅读理解：可以对文本进行Self-Attention，也可以对文章和问题进行对齐。</li>
<li>文本分类：一般是对一段句子进行Attention，得到一个句向量去做分类。</li>
<li>序列标注：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.01586">Deep Semantic Role Labeling with Self-Attention</a>，这篇论文在softmax前用到了Self-Attention，学习句子结构信息，和利用到标签依赖关系的CRF进行比较。</li>
<li>关系抽取：也可以用到self attention</li>
</ol>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2018\03\28\AMPC-1080Ti机器-深度环境搭建\" rel="bookmark">AMPC-1080Ti机器-深度环境搭建</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\07\20\Transformer\" rel="bookmark">Transformer</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2018\04\07\triplet-loss原理推导及变体\" rel="bookmark">triplet loss原理推导及变体</a></div>
    </li>
  </ul>

        <div class="reward-container">
  <div>一分一毛，也是心意。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Run-Qing Chen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Run-Qing Chen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Run-Qing Chen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://blog.rexking6.top/2019/09/20/%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="序列模型中的注意力机制">https://blog.rexking6.top/2019/09/20/序列模型中的注意力机制/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/08/28/%E4%BD%BF%E7%94%A8BibTeX%E7%94%9F%E6%88%90%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E5%88%97%E8%A1%A8/" rel="prev" title="使用BibTeX生成参考文献列表">
      <i class="fa fa-chevron-left"></i> 使用BibTeX生成参考文献列表
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/11/20/Python-collections%E5%9B%9B%E7%A7%8D%E9%AB%98%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" rel="next" title="Python collections四种高性能数据类型">
      Python collections四种高性能数据类型 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

    <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Seq2Seq"><span class="nav-number">2.</span> <span class="nav-text">Seq2Seq</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%8F%E5%85%B8RNN%E7%BB%93%E6%9E%84"><span class="nav-number">2.1.</span> <span class="nav-text">经典RNN结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Embedding"><span class="nav-number">2.2.</span> <span class="nav-text">Embedding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train"><span class="nav-number">2.3.</span> <span class="nav-text">Train</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Seq2Seq-with-Attention"><span class="nav-number">3.</span> <span class="nav-text">Seq2Seq with Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention"><span class="nav-number">3.1.</span> <span class="nav-text">Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4"><span class="nav-number">3.2.</span> <span class="nav-text">步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#score-function"><span class="nav-number">3.3.</span> <span class="nav-text">score function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Luong"><span class="nav-number">3.3.1.</span> <span class="nav-text">Luong</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#dot"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">dot</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#general"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">general</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scaled-dot"><span class="nav-number">3.3.2.</span> <span class="nav-text">scaled dot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cosine"><span class="nav-number">3.3.3.</span> <span class="nav-text">cosine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MLP"><span class="nav-number">3.3.4.</span> <span class="nav-text">MLP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#location-based-function"><span class="nav-number">3.3.5.</span> <span class="nav-text">location-based function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%A6%E5%A4%96%E4%B8%80%E4%B8%AA%E8%A7%92%E5%BA%A6%EF%BC%9AKey%E3%80%81Value%E5%92%8CQuery"><span class="nav-number">3.4.</span> <span class="nav-text">另外一个角度：Key、Value和Query</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%82%E5%AF%9F"><span class="nav-number">3.5.</span> <span class="nav-text">观察</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multi-dimensional-Attention"><span class="nav-number">4.</span> <span class="nav-text">Multi-dimensional Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Soft-Attention-%E5%92%8C-Hard-Attention"><span class="nav-number">5.</span> <span class="nav-text">Soft Attention 和 Hard Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Global-Attention-%E5%92%8C-Local-Attention"><span class="nav-number">6.</span> <span class="nav-text">Global Attention 和 Local Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Hierarchical-Attention"><span class="nav-number">7.</span> <span class="nav-text">Hierarchical Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Attention-Over-Attention"><span class="nav-number">8.</span> <span class="nav-text">Attention Over Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Memory-based-Attention"><span class="nav-number">9.</span> <span class="nav-text">Memory-based Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Self-Attention"><span class="nav-number">10.</span> <span class="nav-text">Self-Attention</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">11.</span> <span class="nav-text">评价指标</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E9%87%8F%E6%8C%87%E6%A0%87"><span class="nav-number">11.1.</span> <span class="nav-text">定量指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9A%E6%80%A7%E6%8C%87%E6%A0%87"><span class="nav-number">11.2.</span> <span class="nav-text">定性指标</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1"><span class="nav-number">12.</span> <span class="nav-text">任务</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Run-Qing Chen"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Run-Qing Chen</p>
  <div class="site-description" itemprop="description">覆苍天以为衾，卧大地以为庐。</div>
</div>


   <div class="feed-link motion-element">
     <a href="/atom.xml" rel="alternate">
       <i class="fa fa-rss"></i>
       RSS
     </a>
   </div>
 
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">181</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">32</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/RexKing6" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;RexKing6" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1010026261@qq.com" title="E-Mail → mailto:1010026261@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zxpblog.cn/" title="https:&#x2F;&#x2F;www.zxpblog.cn&#x2F;" rel="noopener" target="_blank">赵小平</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://whitepuffer.github.io/" title="https:&#x2F;&#x2F;whitepuffer.github.io&#x2F;" rel="noopener" target="_blank">江斓</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://kexue.fm/" title="https:&#x2F;&#x2F;kexue.fm&#x2F;" rel="noopener" target="_blank">科学空间</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yongyuan.name/" title="https:&#x2F;&#x2F;yongyuan.name&#x2F;" rel="noopener" target="_blank">袁勇</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/abcjennifer" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;abcjennifer" rel="noopener" target="_blank">Rachel Zhang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dmkf.xyz/" title="http:&#x2F;&#x2F;dmkf.xyz&#x2F;" rel="noopener" target="_blank">代码咖啡</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://wuxiaolong.me/" title="http:&#x2F;&#x2F;wuxiaolong.me&#x2F;" rel="noopener" target="_blank">吴小龙同学</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.tennfy.com/" title="http:&#x2F;&#x2F;www.tennfy.com&#x2F;" rel="noopener" target="_blank">TENNFY WU</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fab fa-accessible-icon"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Run-Qing Chen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">2.9m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">43:13</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"6XDsO3aHIjDk3nV6eLJCufbl-MdYXbMMI","app_key":"YK4qOc0TpkazN6exhuqsnwmB","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.rexking6.top","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#37c6c0","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"YS7HT61SEB","apiKey":"0fd1eba022e7883c76ff4a71aee2acdc","indexName":"blog_NAME","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"找不到关于 ${query} 的文章","hits_stats":"共找到 ${hits} 篇文章，花了 ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="...">
<meta property="og:type" content="article">
<meta property="og:title" content="coursera机器学习笔记">
<meta property="og:url" content="https://blog.rexking6.top/2018/02/13/coursera%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">
<meta property="og:site_name" content="RexKing6&#39;s Note">
<meta property="og:description" content="...">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://image.rexking6.top/img/clip1516073378.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1516073330.png">
<meta property="og:image" content="http://image.rexking6.top/img/Q9sX8nnxEeamDApmnD43Fw_1cb67ecfac77b134606532f5caf98ee4_Logistic_regression_cost_function_positive_class.png">
<meta property="og:image" content="http://image.rexking6.top/img/Ut7vvXnxEead-BJkoDOYOw_f719f2858d78dd66d80c5ec0d8e6b3fa_Logistic_regression_cost_function_negative_class.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1517240062.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1517323644.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1517358808.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1517374834.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1517914616.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1517915144.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1517917501.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1517917514.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267017.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267168.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267268.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267309.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267375.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267421.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267487.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267507.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267542.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518267573.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518324529.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518348871.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518348886.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1518348919.png">
<meta property="article:published_time" content="2018-02-13T15:41:48.000Z">
<meta property="article:modified_time" content="2024-07-26T14:03:44.500Z">
<meta property="article:author" content="Run-Qing Chen">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://image.rexking6.top/img/clip1516073378.png">

<link rel="canonical" href="https://blog.rexking6.top/2018/02/13/coursera%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>coursera机器学习笔记 | RexKing6's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="RexKing6's Note" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RexKing6's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/rexking6" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.rexking6.top/2018/02/13/coursera%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Run-Qing Chen">
      <meta itemprop="description" content="覆苍天以为衾，卧大地以为庐。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RexKing6's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          coursera机器学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-02-13 23:41:48" itemprop="dateCreated datePublished" datetime="2018-02-13T23:41:48+08:00">2018-02-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-07-26 22:03:44" itemprop="dateModified" datetime="2024-07-26T22:03:44+08:00">2024-07-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%AE%97%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          
            <span id="/2018/02/13/coursera%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-meta-item leancloud_visitors" data-flag-title="coursera机器学习笔记" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>11 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="LMS梯度下降法-基于最小均方"><a href="#LMS梯度下降法-基于最小均方" class="headerlink" title="LMS梯度下降法(基于最小均方)"></a>LMS梯度下降法(基于最小均方)</h2><p>时间复杂度：$O(kn^2)$<br>梯度下降法指的是参数（权重）同时更新，而不是先更新$\theta_0$再更新$\theta_1$</p>
<p>正确：</p>
<script type="math/tex; mode=display">temp0:=\theta_0-\alpha\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)</script><script type="math/tex; mode=display">temp1:=\theta_1-\alpha\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)</script><script type="math/tex; mode=display">\theta_0:=temp0</script><script type="math/tex; mode=display">\theta_1:=temp1</script><p>错误：</p>
<script type="math/tex; mode=display">temp0:=\theta_0-\alpha\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)</script><script type="math/tex; mode=display">\theta_0:=temp0</script><script type="math/tex; mode=display">temp1:=\theta_1-\alpha\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)</script><script type="math/tex; mode=display">\theta_1:=temp1</script><h3 id="批量梯度下降法-Batch-Gradient-Descent"><a href="#批量梯度下降法-Batch-Gradient-Descent" class="headerlink" title="批量梯度下降法(Batch Gradient Descent)"></a>批量梯度下降法(Batch Gradient Descent)</h3><p>再推导：<br>$\frac{\partial}{\partial \theta_j}J(\theta) = \frac{\partial}{\partial \theta_j}\frac{1}{2}(h_\theta(x)-y)^2$<br>$=2·\frac{1}{2}(h_\theta(x)-y)·\frac{\partial}{\partial \theta_j}(h_\theta(x)-y)$<br>$=(h_\theta(x)-y)·\frac{\partial}{\partial \theta_j}(\sum_{i=0}^n\theta_ix_i-y)$<br>$=(h_\theta(x)-y)x_j$</p>
<h4 id="二元"><a href="#二元" class="headerlink" title="二元"></a>二元</h4><p>$repeat\{$<br>$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})$<br>$\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x^{(i)}$<br>$\}$</p>
<h4 id="多元"><a href="#多元" class="headerlink" title="多元"></a>多元</h4><p>$repeat\{$<br>$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$<br>$\}$</p>
<h3 id="随机梯度下降法-Stochastic-Gradient-Descent"><a href="#随机梯度下降法-Stochastic-Gradient-Descent" class="headerlink" title="随机梯度下降法(Stochastic Gradient Descent)"></a>随机梯度下降法(Stochastic Gradient Descent)</h3><p>效率较低，常用于在线学习<br>$Loop\{$<br>$\space \space for\space i=1\space to\space m,\{$<br>$\space \space \space \space \theta_j := \theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$<br>$\space \space \}$<br>$\}$</p>
<p>计算$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$不太现实，所以一般判断是否收敛有两种方式：</p>
<ol>
<li>在使用$(x^{(i)},y^{(i)})$训练之前，计算$\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2$，训练之后，再重新计算该值 </li>
<li>每1000次迭代或者2000次迭代后，计算总的cost值</li>
</ol>
<p>随机梯度下降法会进入局部最优值，要进入全局最优值，可以缓慢减少学习速率$\alpha$，例如：</p>
<script type="math/tex; mode=display">\alpha = \frac{const1}{iterationNumber+const2}</script><h3 id="小批量梯度下降法-Mini-Batch-Gradient-Descent"><a href="#小批量梯度下降法-Mini-Batch-Gradient-Descent" class="headerlink" title="小批量梯度下降法(Mini-Batch Gradient Descent)"></a>小批量梯度下降法(Mini-Batch Gradient Descent)</h3><p>每次选取b个（少量，比如10个）进行梯度下降法。</p>
<h3 id="特征缩放和均值化"><a href="#特征缩放和均值化" class="headerlink" title="特征缩放和均值化"></a>特征缩放和均值化</h3><p>当特征之间的数值范围比较接近时，梯度下降法能够更有效地工作。<br>特征缩放，将变量值除以（最大值-最小值）。<br>另外一个，均值化</p>
<script type="math/tex; mode=display">x_i=\frac{x_i-u_i}{s_i}</script><p>$s_i$一般是最大值减最小值或者标准差。</p>
<h3 id="选择学习效率"><a href="#选择学习效率" class="headerlink" title="选择学习效率"></a>选择学习效率</h3><p>根据迭代次数对代价函数进行画图<br><img src="http://image.rexking6.top/img/clip1516073378.png" alt=""><br>最好的是第一个，称之为收敛；<br>第二个为学习效率较小的状态，原因是多次迭代仍然下降幅度较大；<br>第三个为学校效率较大的状态，原因是如下图，逐次上升。<br><img src="http://image.rexking6.top/img/clip1516073330.png" alt=""></p>
<h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>例如拟合$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_1^2+\theta_3x_1^3$<br>造出$x_2=x_1^2$和$x_3=x_1^3$<br>但这种情况需要很注意归一化问题。</p>
<h2 id="正规方程Normal-Equation"><a href="#正规方程Normal-Equation" class="headerlink" title="正规方程Normal Equation"></a>正规方程Normal Equation</h2><p>特征维数比较少时使用，不需要归一化，不需要迭代，不需要选择学习效率。</p>
<script type="math/tex; mode=display">\begin{align*}
\nabla_\theta J(\theta)&=\nabla_\theta \frac{1}{2}(X\theta-\vec y)^T(X\theta - \vec y) \\
&=\frac{1}{2}\nabla_\theta(\theta^TX^TX\theta-\theta^TX^T\vec y- \vec y^TX\theta + \vec y^T \vec y) \\
&=\frac{1}{2}\nabla_\theta tr(\theta^TX^TX\theta - \theta^TX^T \vec y- \vec y^TX\theta + \vec y^T \vec y) \\
&=\frac{1}{2}\nabla_\theta(tr\theta^TX^TX\theta-2tr \vec y^T X\theta) \\
&=\frac{1}{2}(X^TX\theta + X^TX\theta - 2X^T \vec y) \\
&=X^TX\theta - X^T \vec y 
\end{align*}</script><script type="math/tex; mode=display">X^TX\theta = X^T \vec y</script><script type="math/tex; mode=display">\theta = (X^TX)^{-1}X^Ty</script><p>时间复杂度：$O(n^3)$</p>
<h3 id="不适用情况"><a href="#不适用情况" class="headerlink" title="不适用情况"></a>不适用情况</h3><p>一般时$X^TX$无法求逆，原因有两个：</p>
<ol>
<li>大量重复多余的特征，例如之间线性相关；</li>
<li>过多的特征$m \leq n$。</li>
</ol>
<h2 id="局部线性回归"><a href="#局部线性回归" class="headerlink" title="局部线性回归"></a>局部线性回归</h2><p>用预测点周围的点计算线性回归，给训练集的点赋予权重进行计算。<br>$J(\theta)=\sum_i w^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$<br>$w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2\tau^2})$<br>一个标准取法是上述。</p>
<h2 id="证明最小二乘法"><a href="#证明最小二乘法" class="headerlink" title="证明最小二乘法"></a>证明最小二乘法</h2><p>$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$<br>$\epsilon是误差，遵循高斯分布$<br>$p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$<br>$p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$<br>$likelihood \space function:$<br>$L(\theta)=L(\theta;X,\vec y)=p(\vec y|X;\theta)$</p>
<script type="math/tex; mode=display">\begin{align*}
L(\theta)&=\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\
&=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
\end{align*}</script><p>$log \space likelihood l(\theta):$</p>
<script type="math/tex; mode=display">\begin{align*}
l(\theta) &= log\space L(\theta) \\
&=log\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\
&=\sum_{i=1}^mlog\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \\
&=m \space log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma^2}·\frac{1}{2}\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2
\end{align*}</script><p>最大化l$(\theta)$，即最小化$\frac{1}{2}\sum_{i=1}^m(y^{(i)}-\theta^Tx^{(i)})^2$</p>
<h1 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h1><h2 id="Sigmoid函数-Logistic函数"><a href="#Sigmoid函数-Logistic函数" class="headerlink" title="Sigmoid函数/Logistic函数"></a>Sigmoid函数/Logistic函数</h2><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><h2 id="Logistic回归-1"><a href="#Logistic回归-1" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><p>Logistic回归，因为$0 \leq h_\theta \leq 1$，使用Logistic函数，所以才叫Logistic回归，但一般用来分类。</p>
<script type="math/tex; mode=display">h_\theta(x)=g(\theta^Tx)</script><p>假设：<br>$P(y=1|x;\theta)=h_\theta(x)$<br>$P(y=0|x;\theta)=1-h_\theta(x)$<br>$p(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}$</p>
<script type="math/tex; mode=display">\begin{align*}
L(\theta)&=p(\vec y|X;\theta) \\
&=\prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\
&=\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}
\end{align*}</script><script type="math/tex; mode=display">\begin{align*}
-l(\theta)&=-log \space L(\theta) \\
&=-\sum_{i=1}^m y^{(i)}log \space h(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)})
\end{align*}</script><p>求导：</p>
<script type="math/tex; mode=display">\begin{align*}
\frac{\partial}{\partial \theta_j}-l(\theta) &= -(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)})\frac{\partial}{\partial \theta_j}g(\theta^Tx) \\
&= -(y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)})g(\theta^Tx)(1-g(\theta^Tx))\frac{\partial}{\partial \theta_j}\theta^Tx \\
&=-(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx))x_j \\
&=-(y-h_\theta(x))x_j
\end{align*}</script><p>所以：<br>$\theta_j:=\theta_j-\alpha[-(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}]$</p>
<h2 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h2><p>分开两类的超平面</p>
<h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><p>分类之后，需要计算cost function，直接用线性回归的cost function，无法计算，因为不是凸函数。采用新的cost function。<br>$\space$<br>$J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)}, y)$<br>$\space$<br>$Cost(h_\theta(x^{(i)}, y) = -log(h_\theta(x))\space if\space y=1$</p>
<p><img src="http://image.rexking6.top/img/Q9sX8nnxEeamDApmnD43Fw_1cb67ecfac77b134606532f5caf98ee4_Logistic_regression_cost_function_positive_class.png" alt=""></p>
<p>$Cost(h_\theta(x^{(i)}, y) = -log(1-h_\theta(x))\space if\space y=0$</p>
<p><img src="http://image.rexking6.top/img/Ut7vvXnxEead-BJkoDOYOw_f719f2858d78dd66d80c5ec0d8e6b3fa_Logistic_regression_cost_function_negative_class.png" alt=""></p>
<p>$Cost(h_\theta(x^{(i)}, y) = -ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$<br>$\space$<br>$J(\theta)=-\frac{1}{m}\sum_{i=1}^m[-y^{(i)}log(h_\theta(x^{(i)}))-(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$<br>$向量形式：$<br>$h=g(X\theta)$<br>$J(\theta)=\frac{1}{m}·(-y^Tlog(h)-(1-y)^Tlog(1-h))$<br>$更新公式：$<br>$\theta_j:=\theta_j-\frac{\alpha}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$</p>
<h2 id="更优的优化算法"><a href="#更优的优化算法" class="headerlink" title="更优的优化算法"></a>更优的优化算法</h2><ul>
<li>共轭梯度法</li>
<li>牛顿法(二阶)</li>
</ul>
<p>$\theta:=\theta-\frac{f(\theta)}{f’(\theta)}$<br>$目的是让l(\theta)最小值，即l’(\theta)=0$<br>$\theta:=\theta-\frac{l’(\theta)}{l’’(\theta)}$<br>$向量形式为：$<br>$\theta:=\theta-H^{-1} \nabla_\theta l(\theta)$<br>$其中H_{ij}=\frac{\partial^2l(\theta)}{\partial \theta_i \partial \theta_j}$</p>
<ul>
<li>拟牛顿法</li>
</ul>
<h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><p>一对多（One-vs-all）</p>
<h1 id="感知器学习算法Perceptron-Learning-Algorithm"><a href="#感知器学习算法Perceptron-Learning-Algorithm" class="headerlink" title="感知器学习算法Perceptron Learning Algorithm"></a>感知器学习算法Perceptron Learning Algorithm</h1><p>$g(z)=1,if \space z \ge 0$<br>$g(z)=0, if \space z &lt; 0$<br>$let \space h_\theta(x) = g(\theta^Tx)$<br>$\theta_j:=\theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$</p>
<h1 id="过拟合-overfit"><a href="#过拟合-overfit" class="headerlink" title="过拟合(overfit)"></a>过拟合(overfit)</h1><p>欠拟合(underfit) 高偏差<br>过拟合(overfit) 高方差</p>
<h2 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h2><ol>
<li><p>减少特征个数<br>减少一些多余的或者贡献较小的特征，可以采用特征选择算法(model selection algorithm)。</p>
</li>
<li><p>正则化<br>给$\theta$前加上系数，限制$\theta$的作用。</p>
</li>
</ol>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>当不确定具体需要限制哪些$\theta$的作用时，采用总体的限制，比如范数</p>
<script type="math/tex; mode=display">min_\theta \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n \theta_j^2</script><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>$更新公式：$<br>$\{$<br>$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}$<br>$\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j] \space j \in \{1,2…n\}$<br>$\}$<br>$上式可写作：$<br>$\theta_j:=\theta_j(1-\alpha \frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \space j \in \{1,2…n\}$<br>可以看作，相比于原本的更新公式，该更新公式每次更新会减小一点$\theta$的值。</p>
<h3 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h3><p>$\theta = (X^TX+ \lambda·L)^{-1}X^Ty$</p>
<p><img src="http://image.rexking6.top/img/clip1517240062.png" alt=""><br>L为$(n+1)*(n+1)维$。</p>
<h3 id="Logistic回归-2"><a href="#Logistic回归-2" class="headerlink" title="Logistic回归"></a>Logistic回归</h3><p>$J(\theta) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$<br>$更新公式：$<br>$\{$<br>$\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}$<br>$\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j] \space j \in \{1,2…n\}$<br>$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$<br>$\}$</p>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p><img src="http://image.rexking6.top/img/clip1517323644.png" alt=""></p>
<script type="math/tex; mode=display">\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}</script><p>$s_j表示第j层拥有的神经元数目，所以\Theta^{(j)}的维数s_{j+1}\times(s_j+1)$</p>
<p>$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$<br>$a^{(j)} = g(z^{(j)})$</p>
<h2 id="逻辑运算的例子"><a href="#逻辑运算的例子" class="headerlink" title="逻辑运算的例子"></a>逻辑运算的例子</h2><p><img src="http://image.rexking6.top/img/clip1517358808.png" alt=""></p>
<h2 id="多分类的例子"><a href="#多分类的例子" class="headerlink" title="多分类的例子"></a>多分类的例子</h2><p><img src="http://image.rexking6.top/img/clip1517374834.png" alt=""></p>
<h2 id="cost-function-1"><a href="#cost-function-1" class="headerlink" title="cost function"></a>cost function</h2><p>$\begin{gather<em>} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather</em>}$<br>上式为多分类时cost function，$L$为神经网络层数，$K$为分类类数，直接导致最终结果的向量维数，等于最后一层的神经元数。即对每一种类别采用logistic回归时的cost function。</p>
<h2 id="误差反向传播"><a href="#误差反向传播" class="headerlink" title="误差反向传播"></a>误差反向传播</h2><p>误差反向传播四个核心公式：</p>
<ol>
<li><p>$\delta_i^{(L)} = -(y_i-a_i^{(L)})f’(z_i^{(L)})$</p>
</li>
<li><p>$\delta_i^{(l)}=(\sum_{j=1}^{n_{l+1}}\delta_j^{(l+1)}w_{ji}^{(l+1)})f’(z_i^{(l)})$</p>
</li>
<li><p>$\frac{\partial E}{\partial w_{ij}^{(l)}}=\delta_i^{(l)}a_j^{(l-1)}$</p>
</li>
<li><p>$\frac{\partial E}{\partial b_i^{(l)}}=\delta_i^{(l)}$</p>
</li>
</ol>
<p>具体参考coursera深度学习里的推导，主要用的是求导的链式法则。</p>
<h2 id="梯度检验Gradient-Checking"><a href="#梯度检验Gradient-Checking" class="headerlink" title="梯度检验Gradient Checking"></a>梯度检验Gradient Checking</h2><p>$\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$<br>$\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$<br>将结果与梯度比较，若正确则将梯度检验关闭，因为梯度检验速度很慢</p>
<h2 id="随机初始化Random-Initialization"><a href="#随机初始化Random-Initialization" class="headerlink" title="随机初始化Random Initialization"></a>随机初始化Random Initialization</h2><p>随机初始化是为了打破对称。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Theta1 = rand(1,1) * (2 * INIT_EPSILON) - INIT_EPSILON;</span><br></pre></td></tr></table></figure></p>
<h2 id="训练步骤"><a href="#训练步骤" class="headerlink" title="训练步骤"></a>训练步骤</h2><ol>
<li>随机初始化权重</li>
<li>向前传播，得到$h_\theta(x^{(i)})$</li>
<li>计算cost function $J(\theta)$</li>
<li>向后传播，计算偏导数</li>
<li>使用梯度检验，之后关闭</li>
<li>使用梯度下降等方法最小化成本函数</li>
</ol>
<h1 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h1><ol>
<li>$训练:测试:验证=6:2:2$</li>
<li>$高方差 or  高偏差$</li>
</ol>
<p><img src="http://image.rexking6.top/img/clip1517914616.png" alt=""></p>
<ol>
<li>正则化的作用</li>
</ol>
<p><img src="http://image.rexking6.top/img/clip1517915144.png" alt=""></p>
<ol>
<li>学习曲线</li>
</ol>
<p><img src="http://image.rexking6.top/img/clip1517917501.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1517917514.png" alt=""></p>
<p>收集更多数据量对高方差是有作用的。</p>
<p>总结：</p>
<ol>
<li>收集更多数据量：高方差</li>
<li>尝试更少的特征数：高方差</li>
<li>收集更多的特征：高偏差</li>
<li>增加多项式次数：高偏差</li>
<li>减小$\lambda$：高偏差</li>
<li>增加$\lambda$：高方差</li>
</ol>
<h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><h2 id="特征量"><a href="#特征量" class="headerlink" title="特征量"></a>特征量</h2><p>如果每个特征需要N个样本，那么对于10个特征将需要N<sup>10</sup>个样本，对于包含1000个特征的词汇表将需要N<sup>1000</sup>个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。如果特征之间相互独立，那么样本数就可以从N<sup>1000</sup>减少到1000×N。</p>
<h2 id="朴素贝叶斯常见模型"><a href="#朴素贝叶斯常见模型" class="headerlink" title="朴素贝叶斯常见模型"></a>朴素贝叶斯常见模型</h2><ol>
<li><p>高斯模型<br>假设这些一个特征的所有属于某个类别的观测值符合高斯分布，目前还没看到例子，不理解</p>
</li>
<li><p>多项式模型<br>按照次数作为特征，常用于文本</p>
</li>
<li><p>伯努利模型<br>出现/不出现，对应01向量 </p>
</li>
</ol>
<h1 id="k-means聚类"><a href="#k-means聚类" class="headerlink" title="k-means聚类"></a>k-means聚类</h1><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol>
<li>随机初始化k个点</li>
<li>将样本归类到最近的点</li>
<li>计算同个簇的点的均值点，作为新的簇中心点</li>
<li>将23重复，直到样本归类不再发生变化</li>
</ol>
<h2 id="k的取值"><a href="#k的取值" class="headerlink" title="k的取值"></a>k的取值</h2><p>肘部法：最近一次明显下降的点。</p>
<h1 id="PCA降维"><a href="#PCA降维" class="headerlink" title="PCA降维"></a>PCA降维</h1><h2 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h2><ol>
<li>特征均值化</li>
<li>计算协方差矩阵$\Sigma = \frac{1}{m} \sum_{i=1}^n (x^{(i)})(x^{(i)})^T$</li>
<li>$[U,S,V]=svd(Sigma);$</li>
<li>$Z = U(1:K)’*X;$</li>
<li>重建：$X=U(1:K)*Z$</li>
</ol>
<h2 id="K的取值"><a href="#K的取值" class="headerlink" title="K的取值"></a>K的取值</h2><p>根据贡献率选取：</p>
<script type="math/tex; mode=display">\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2}\leq 0.01</script><p>贡献率：99%<br>上式同等于：</p>
<script type="math/tex; mode=display">1-\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \leq 0.01</script><script type="math/tex; mode=display">\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \ge 0.99</script><h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><ol>
<li>特征降维，以提高运算速度，减少内存使用；</li>
<li>降至2~3维，以可视化数据；</li>
<li>不可用于改善过拟合。</li>
</ol>
<h1 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h1><h2 id="高斯分布检测系统"><a href="#高斯分布检测系统" class="headerlink" title="高斯分布检测系统"></a>高斯分布检测系统</h2><ol>
<li>选取正常样本</li>
<li>计算参数$\mu,\sigma^2$</li>
</ol>
<script type="math/tex; mode=display">\mu_j=\frac{1}{m}\sum_{i=1}^mx_j^{(i)}</script><script type="math/tex; mode=display">\sigma_j^2 = \frac{1}{m}\sum_{i=1}^m(x_j^{(i)}-\mu_j)^2</script><ol>
<li>来了一个检测样本，计算$p(x)$</li>
</ol>
<script type="math/tex; mode=display">p(x)=\prod_{j=1}^n p(x_j;\mu_j, \sigma_j^2) = \prod_{j=1}^n \frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})</script><p>$判断p(x) &lt; \varepsilon$</p>
<p>仍然要分出训练集、测试集和交叉验证集</p>
<h2 id="异常检测-vs-监督学习"><a href="#异常检测-vs-监督学习" class="headerlink" title="异常检测 vs 监督学习"></a>异常检测 vs 监督学习</h2><h3 id="异常检测-1"><a href="#异常检测-1" class="headerlink" title="异常检测"></a>异常检测</h3><ol>
<li>只有少量的异常样本</li>
<li>许多不同的异常类别</li>
<li>未来可能出现新的异常类别</li>
</ol>
<h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><ol>
<li>大量的正常和异常样本</li>
</ol>
<h2 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h2><p>将样本特征画图，如果不像正态分布，可用多种函数映射成正态分布。</p>
<script type="math/tex; mode=display">log(x)</script><script type="math/tex; mode=display">\sqrt{x}</script><script type="math/tex; mode=display">...</script><h2 id="多元高斯分布检测分布"><a href="#多元高斯分布检测分布" class="headerlink" title="多元高斯分布检测分布"></a>多元高斯分布检测分布</h2><p>动机：当出现以下这种异常点，无法检测。</p>
<p><img src="http://image.rexking6.top/img/clip1518267017.png" alt=""></p>
<h3 id="协方差矩阵-Sigma"><a href="#协方差矩阵-Sigma" class="headerlink" title="协方差矩阵$\Sigma$"></a>协方差矩阵$\Sigma$</h3><p><img src="http://image.rexking6.top/img/clip1518267168.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1518267268.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1518267309.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1518267375.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1518267421.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1518267487.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1518267507.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1518267542.png" alt=""></p>
<p><img src="http://image.rexking6.top/img/clip1518267573.png" alt=""></p>
<h3 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h3><ol>
<li>计算$\mu,\Sigma$</li>
</ol>
<script type="math/tex; mode=display">\mu = \frac{1}{m}\sum_{i=1}^m x^{(i)}</script><script type="math/tex; mode=display">\Sigma = \frac{1}{m}\sum_{i=1}^m (x^{(i)}-\mu)(x^{(i)}-\mu)^T</script><ol>
<li>计算$p(x)$</li>
</ol>
<script type="math/tex; mode=display">p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))</script><p>$判断p(x) &lt; \varepsilon$</p>
<p>当$m &gt;&gt; n$时，才使用多元高斯分布，其他情况使用普通高斯分布。</p>
<h1 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h1><p><img src="http://image.rexking6.top/img/clip1518324529.png" alt=""></p>
<p>$n_u = 用户数量$<br>$n_m = 电影数量$<br>$r(i,j) = 1 如果用户j给电影i评了分$<br>$y^{(i,j)} = 用户j给电影i评的分$</p>
<h2 id="基于内容的推荐系统"><a href="#基于内容的推荐系统" class="headerlink" title="基于内容的推荐系统"></a>基于内容的推荐系统</h2><p>$对于每个用户j，我们要学习出一个\theta^{(j)}；而对于每部电影i，我们要学习出一个x^{(i)}，则预测评分为(\theta^{(j)})^T(x^{(i)})$<br>$m^{(j)} = 用户j评分的电影部数$<br>$优化目标为：$</p>
<script type="math/tex; mode=display">min_{\theta^{(j)}} = \frac{1}{2} \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{k=1}^n(\theta_k^{(j)})^2</script><script type="math/tex; mode=display">min_{\theta^{(1)},...,\theta^{(n_u)}} = \frac{1}{2} \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2</script><p>$\space$<br>$梯度下降法：$</p>
<script type="math/tex; mode=display">\theta_k^{(j)}:=\theta_k^{(j)} - \alpha \sum_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} \space (for \space k =0)</script><script type="math/tex; mode=display">\theta_k^{(j)}:=\theta_k^{(j)} - \alpha (\sum_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} + \lambda \theta_k^{(j)}) \space (for \space k \neq 0)</script><p>$基于内容的推荐系统需要事先对每部电影给出x^{(i)}，操作性不够强$</p>
<h2 id="基于协同过滤的推荐系统"><a href="#基于协同过滤的推荐系统" class="headerlink" title="基于协同过滤的推荐系统"></a>基于协同过滤的推荐系统</h2><p>$假设每个用户的\theta^{(j)}已给出，则要学习出x^{(i)}$<br>$优化目标：$</p>
<script type="math/tex; mode=display">min_{x^{(i)}} \frac{1}{2} \sum_{j:r(i,j)=1} (( \theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{k=1}^n (x_k^{(i)})^2</script><script type="math/tex; mode=display">min_{x^{(1)},...,x^{(n_m)}} \frac{1}{2} \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} (( \theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2 + \frac{\lambda}{2} \sum_{i=1}^{n_m} \sum_{k=1}^n (x_k^{(i)})^2</script><p>$结合基于内容的推荐系统，我们可以先随机取值\theta，然后学习出x，再学习出\theta……$<br>$不用那么麻烦的办法，同时训练学习出\theta和x：$</p>
<script type="math/tex; mode=display">J(x^{(1)},...,x^{(n_m)},\theta^{(1)},...,\theta^{(n_u)})=\frac{1}{2} \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^n (x_k^{(i)})^2 + \frac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n (\theta_k^{(j)})^2</script><h2 id="步骤-3"><a href="#步骤-3" class="headerlink" title="步骤"></a>步骤</h2><ol>
<li>$随机初始化x和\theta为小的随机值$</li>
<li>$使用梯度下降法，最小化J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})$</li>
</ol>
<script type="math/tex; mode=display">x_k^{(i)}:=x_k^{(i)} - \alpha (\sum_{j:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(j)}+\lambda x_k^{(i)})</script><script type="math/tex; mode=display">\theta_k^{(j)}:=\theta_k^{(j)} - \alpha (\sum_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})\theta_k^{(i)}+\lambda \theta_k^{(j)})</script><ol>
<li>$\theta^Tx即为预测评分$</li>
</ol>
<h2 id="均值化"><a href="#均值化" class="headerlink" title="均值化"></a>均值化</h2><p><img src="http://image.rexking6.top/img/clip1518348871.png" alt=""></p>
<ol>
<li>每行求平均值</li>
</ol>
<p><img src="http://image.rexking6.top/img/clip1518348886.png" alt=""></p>
<ol>
<li>矩阵每个值都减去该行的平均值</li>
</ol>
<p><img src="http://image.rexking6.top/img/clip1518348919.png" alt=""></p>
<ol>
<li>最后算出来的评分再加上该平均值$(\theta^{(j)})^T(x^{(i)})+\mu_i$</li>
</ol>
<h1 id="Map-reduce"><a href="#Map-reduce" class="headerlink" title="Map-reduce"></a>Map-reduce</h1><p>将样本分开到每个计算机计算，之后再加和。<br>有将样本加和的步骤都可以采用Map-reduce。</p>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\08\30\A-B测试\" rel="bookmark">A/B测试</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2019\03\21\Fisher线性判别\" rel="bookmark">Fisher线性判别</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2018\02\13\cs229中文笔记-一二\" rel="bookmark">cs229中文笔记(一二)</a></div>
    </li>
  </ul>

        <div class="reward-container">
  <div>一分一毛，也是心意。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Run-Qing Chen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Run-Qing Chen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Run-Qing Chen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://blog.rexking6.top/2018/02/13/coursera%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="coursera机器学习笔记">https://blog.rexking6.top/2018/02/13/coursera机器学习笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%AE%97%E6%B3%95/" rel="tag"># 算法</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/02/13/cs229%E4%B8%AD%E6%96%87%E7%AC%94%E8%AE%B0-%E4%B8%80%E4%BA%8C/" rel="prev" title="cs229中文笔记(一二)">
      <i class="fa fa-chevron-left"></i> cs229中文笔记(一二)
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/02/14/cs229%E4%B8%AD%E6%96%87%E7%AC%94%E8%AE%B0-%E4%B8%89/" rel="next" title="cs229中文笔记(三)">
      cs229中文笔记(三) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

    <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LMS%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E5%9F%BA%E4%BA%8E%E6%9C%80%E5%B0%8F%E5%9D%87%E6%96%B9"><span class="nav-number">1.1.</span> <span class="nav-text">LMS梯度下降法(基于最小均方)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-Batch-Gradient-Descent"><span class="nav-number">1.1.1.</span> <span class="nav-text">批量梯度下降法(Batch Gradient Descent)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8C%E5%85%83"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">二元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E5%85%83"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">多元</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-Stochastic-Gradient-Descent"><span class="nav-number">1.1.2.</span> <span class="nav-text">随机梯度下降法(Stochastic Gradient Descent)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-Mini-Batch-Gradient-Descent"><span class="nav-number">1.1.3.</span> <span class="nav-text">小批量梯度下降法(Mini-Batch Gradient Descent)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E5%92%8C%E5%9D%87%E5%80%BC%E5%8C%96"><span class="nav-number">1.1.4.</span> <span class="nav-text">特征缩放和均值化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E5%AD%A6%E4%B9%A0%E6%95%88%E7%8E%87"><span class="nav-number">1.1.5.</span> <span class="nav-text">选择学习效率</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="nav-number">1.2.</span> <span class="nav-text">多项式回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8BNormal-Equation"><span class="nav-number">1.3.</span> <span class="nav-text">正规方程Normal Equation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E9%80%82%E7%94%A8%E6%83%85%E5%86%B5"><span class="nav-number">1.3.1.</span> <span class="nav-text">不适用情况</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.4.</span> <span class="nav-text">局部线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%81%E6%98%8E%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="nav-number">1.5.</span> <span class="nav-text">证明最小二乘法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logistic%E5%9B%9E%E5%BD%92"><span class="nav-number">2.</span> <span class="nav-text">Logistic回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid%E5%87%BD%E6%95%B0-Logistic%E5%87%BD%E6%95%B0"><span class="nav-number">2.1.</span> <span class="nav-text">Sigmoid函数&#x2F;Logistic函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic%E5%9B%9E%E5%BD%92-1"><span class="nav-number">2.2.</span> <span class="nav-text">Logistic回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="nav-number">2.3.</span> <span class="nav-text">决策边界</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cost-function"><span class="nav-number">2.4.</span> <span class="nav-text">cost function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9B%B4%E4%BC%98%E7%9A%84%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">2.5.</span> <span class="nav-text">更优的优化算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="nav-number">2.6.</span> <span class="nav-text">多分类</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95Perceptron-Learning-Algorithm"><span class="nav-number">3.</span> <span class="nav-text">感知器学习算法Perceptron Learning Algorithm</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88-overfit"><span class="nav-number">4.</span> <span class="nav-text">过拟合(overfit)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">4.1.</span> <span class="nav-text">防止过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">4.2.</span> <span class="nav-text">正则化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">4.2.1.</span> <span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal-Equation"><span class="nav-number">4.2.2.</span> <span class="nav-text">Normal Equation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic%E5%9B%9E%E5%BD%92-2"><span class="nav-number">4.2.3.</span> <span class="nav-text">Logistic回归</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">5.1.</span> <span class="nav-text">逻辑运算的例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-number">5.2.</span> <span class="nav-text">多分类的例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cost-function-1"><span class="nav-number">5.3.</span> <span class="nav-text">cost function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">5.4.</span> <span class="nav-text">误差反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8CGradient-Checking"><span class="nav-number">5.5.</span> <span class="nav-text">梯度检验Gradient Checking</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96Random-Initialization"><span class="nav-number">5.6.</span> <span class="nav-text">随机初始化Random Initialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%AD%A5%E9%AA%A4"><span class="nav-number">5.7.</span> <span class="nav-text">训练步骤</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BB%BA%E8%AE%AE"><span class="nav-number">6.</span> <span class="nav-text">建议</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="nav-number">7.</span> <span class="nav-text">朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%87%8F"><span class="nav-number">7.1.</span> <span class="nav-text">特征量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B"><span class="nav-number">7.2.</span> <span class="nav-text">朴素贝叶斯常见模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#k-means%E8%81%9A%E7%B1%BB"><span class="nav-number">8.</span> <span class="nav-text">k-means聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4"><span class="nav-number">8.1.</span> <span class="nav-text">步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k%E7%9A%84%E5%8F%96%E5%80%BC"><span class="nav-number">8.2.</span> <span class="nav-text">k的取值</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PCA%E9%99%8D%E7%BB%B4"><span class="nav-number">9.</span> <span class="nav-text">PCA降维</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4-1"><span class="nav-number">9.1.</span> <span class="nav-text">步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K%E7%9A%84%E5%8F%96%E5%80%BC"><span class="nav-number">9.2.</span> <span class="nav-text">K的取值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%9C%E7%94%A8"><span class="nav-number">9.3.</span> <span class="nav-text">作用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B"><span class="nav-number">10.</span> <span class="nav-text">异常检测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F"><span class="nav-number">10.1.</span> <span class="nav-text">高斯分布检测系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-vs-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">10.2.</span> <span class="nav-text">异常检测 vs 监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B-1"><span class="nav-number">10.2.1.</span> <span class="nav-text">异常检测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">10.2.2.</span> <span class="nav-text">监督学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-number">10.3.</span> <span class="nav-text">特征选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%A3%80%E6%B5%8B%E5%88%86%E5%B8%83"><span class="nav-number">10.4.</span> <span class="nav-text">多元高斯分布检测分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5-Sigma"><span class="nav-number">10.4.1.</span> <span class="nav-text">协方差矩阵$\Sigma$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4-2"><span class="nav-number">10.4.2.</span> <span class="nav-text">步骤</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">11.</span> <span class="nav-text">推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">11.1.</span> <span class="nav-text">基于内容的推荐系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="nav-number">11.2.</span> <span class="nav-text">基于协同过滤的推荐系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%A5%E9%AA%A4-3"><span class="nav-number">11.3.</span> <span class="nav-text">步骤</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9D%87%E5%80%BC%E5%8C%96"><span class="nav-number">11.4.</span> <span class="nav-text">均值化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Map-reduce"><span class="nav-number">12.</span> <span class="nav-text">Map-reduce</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Run-Qing Chen"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Run-Qing Chen</p>
  <div class="site-description" itemprop="description">覆苍天以为衾，卧大地以为庐。</div>
</div>


   <div class="feed-link motion-element">
     <a href="/atom.xml" rel="alternate">
       <i class="fa fa-rss"></i>
       RSS
     </a>
   </div>
 
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">227</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">81</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">78</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/RexKing6" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;RexKing6" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1010026261@qq.com" title="E-Mail → mailto:1010026261@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://zhimi.vercel.app/index_zh-cn.html" title="https:&#x2F;&#x2F;zhimi.vercel.app&#x2F;index_zh-cn.html" rel="noopener" target="_blank">執迷</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://whitepuffer.github.io/" title="https:&#x2F;&#x2F;whitepuffer.github.io&#x2F;" rel="noopener" target="_blank">江斓</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://kexue.fm/" title="https:&#x2F;&#x2F;kexue.fm&#x2F;" rel="noopener" target="_blank">科学空间</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yongyuan.name/" title="https:&#x2F;&#x2F;yongyuan.name&#x2F;" rel="noopener" target="_blank">袁勇</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/abcjennifer" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;abcjennifer" rel="noopener" target="_blank">Rachel Zhang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dmkf.xyz/" title="http:&#x2F;&#x2F;dmkf.xyz&#x2F;" rel="noopener" target="_blank">代码咖啡</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://wuxiaolong.me/" title="http:&#x2F;&#x2F;wuxiaolong.me&#x2F;" rel="noopener" target="_blank">吴小龙同学</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.tennfy.com/" title="http:&#x2F;&#x2F;www.tennfy.com&#x2F;" rel="noopener" target="_blank">TENNFY WU</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fab fa-accessible-icon"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Run-Qing Chen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">4m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">59:56</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"6XDsO3aHIjDk3nV6eLJCufbl-MdYXbMMI","app_key":"YK4qOc0TpkazN6exhuqsnwmB","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.rexking6.top","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#37c6c0","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"appID":"YS7HT61SEB","apiKey":"0fd1eba022e7883c76ff4a71aee2acdc","indexName":"blog_NAME","hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"找不到关于 ${query} 的文章","hits_stats":"共找到 ${hits} 篇文章，花了 ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="...">
<meta property="og:type" content="article">
<meta property="og:title" content="莫烦TensorFlow笔记">
<meta property="og:url" content="https://blog.rexking6.top/2018/03/22/%E8%8E%AB%E7%83%A6TensorFlow%E7%AC%94%E8%AE%B0/">
<meta property="og:site_name" content="RexKing6&#39;s Note">
<meta property="og:description" content="...">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521700297.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521700309.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521700906.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521700920.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521700946.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521701537.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521701598.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521701646.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521719105.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521719153.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521719607.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521720587.png">
<meta property="og:image" content="http://image.rexking6.top/img/clip1521720851.png">
<meta property="article:published_time" content="2018-03-22T00:21:15.000Z">
<meta property="article:modified_time" content="2021-07-10T11:40:09.111Z">
<meta property="article:author" content="Run-Qing Chen">
<meta property="article:tag" content="TensorFlow">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://image.rexking6.top/img/clip1521700297.png">

<link rel="canonical" href="https://blog.rexking6.top/2018/03/22/%E8%8E%AB%E7%83%A6TensorFlow%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>莫烦TensorFlow笔记 | RexKing6's Note</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="RexKing6's Note" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">RexKing6's Note</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/rexking6" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.rexking6.top/2018/03/22/%E8%8E%AB%E7%83%A6TensorFlow%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Run-Qing Chen">
      <meta itemprop="description" content="覆苍天以为衾，卧大地以为庐。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="RexKing6's Note">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          莫烦TensorFlow笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-03-22 08:21:15" itemprop="dateCreated datePublished" datetime="2018-03-22T08:21:15+08:00">2018-03-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-10 19:40:09" itemprop="dateModified" datetime="2021-07-10T19:40:09+08:00">2021-07-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index"><span itemprop="name">TensorFlow</span></a>
                </span>
            </span>

          
            <span id="/2018/03/22/%E8%8E%AB%E7%83%A6TensorFlow%E7%AC%94%E8%AE%B0/" class="post-meta-item leancloud_visitors" data-flag-title="莫烦TensorFlow笔记" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h1><h2 id="张量（Tensor"><a href="#张量（Tensor" class="headerlink" title="张量（Tensor):"></a>张量（Tensor):</h2><p>张量有多种：</p>
<ul>
<li>零阶张量为 纯量或标量 (scalar) 也就是一个数值. 比如 [1]</li>
<li>一阶张量为 向量 (vector), 比如 一维的 [1, 2, 3]</li>
<li>二阶张量为 矩阵 (matrix), 比如 二维的 [[1, 2, 3],[4, 5, 6],[7, 8, 9]]</li>
<li>以此类推, 还有 三阶 三维的 …</li>
</ul>
<h2 id="基本步骤"><a href="#基本步骤" class="headerlink" title="基本步骤"></a>基本步骤</h2><h3 id="创捷数据"><a href="#创捷数据" class="headerlink" title="创捷数据"></a>创捷数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># create data</span><br><span class="line">x_data = np.random.rand(100).astype(np.float32)</span><br><span class="line">y_data = x_data*0.1 + 0.3</span><br></pre></td></tr></table></figure>
<h3 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))</span><br><span class="line">biases = tf.Variable(tf.zeros([1]))</span><br><span class="line"></span><br><span class="line">y = Weights*x_data + biases</span><br></pre></td></tr></table></figure>
<h3 id="计算误差"><a href="#计算误差" class="headerlink" title="计算误差"></a>计算误差</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.square(y-y_data))</span><br></pre></td></tr></table></figure>
<h3 id="传播误差"><a href="#传播误差" class="headerlink" title="传播误差"></a>传播误差</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(0.5)</span><br><span class="line">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ol>
<li><p>初始化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
</li>
<li><p>激活Session</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)          # Very important</span><br><span class="line"></span><br><span class="line">for step in range(201):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    if step % 20 == 0:</span><br><span class="line">        print(step, sess.run(Weights), sess.run(biases))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Session用法"><a href="#Session用法" class="headerlink" title="Session用法"></a>Session用法</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># method 1</span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br><span class="line"># [[12]]</span><br><span class="line"></span><br><span class="line"># method 2</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    result2 = sess.run(product)</span><br><span class="line">    print(result2)</span><br><span class="line"># [[12]]</span><br></pre></td></tr></table></figure>
<h2 id="Variable-变量"><a href="#Variable-变量" class="headerlink" title="Variable 变量"></a>Variable 变量</h2><ol>
<li><p>定义变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(0, name=&#x27;counter&#x27;)</span><br><span class="line"></span><br><span class="line"># 定义常量 one</span><br><span class="line">one = tf.constant(1)</span><br><span class="line"></span><br><span class="line"># 定义加法步骤 (注: 此步并没有直接计算)</span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line"></span><br><span class="line"># 将 State 更新成 new_value</span><br><span class="line">update = tf.assign(state, new_value)</span><br></pre></td></tr></table></figure>
</li>
<li><p>定义<code>init</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
</li>
<li><p>激活<code>init</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    for _ in range(3):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>一定要把 <code>sess</code> 的指针指向 <code>state</code> 再进行 <code>print</code></p>
<h2 id="Placeholder-传入值"><a href="#Placeholder-传入值" class="headerlink" title="Placeholder 传入值"></a>Placeholder 传入值</h2><p><code>placeholder</code> 是 Tensorflow 中的占位符，暂时储存变量.</p>
<p>Tensorflow 如果想要从外部传入data, 那就需要用到 <code>tf.placeholder()</code>, 然后以这种形式传输数据 <code>sess.run(***, feed_dict=&#123;input: **&#125;)</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式</span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"># mul = multiply 是将input1和input2 做乘法运算，并输出为 output </span><br><span class="line">ouput = tf.multiply(input1, input2)</span><br></pre></td></tr></table></figure>
<p>接下来, 传值的工作交给了 <code>sess.run()</code> , 需要传入的值放在了<code>feed_dict=&#123;&#125;</code> 并一一对应每一个 <code>input. placeholder</code> 与 <code>feed_dict=&#123;&#125;</code> 是绑定在一起出现的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(ouput, feed_dict=&#123;input1: [7.], input2: [2.]&#125;))</span><br><span class="line"># [ 14.]</span><br></pre></td></tr></table></figure>
<h2 id="优化器-optimizer"><a href="#优化器-optimizer" class="headerlink" title="优化器 optimizer"></a>优化器 optimizer</h2><ul>
<li><code>tf.train.Optimizer</code></li>
<li><code>tf.train.GradientDescentOptimizer</code></li>
<li><code>tf.train.AdadeltaOptimizer</code></li>
<li><code>tf.train.AdagradOptimizer</code></li>
<li><code>tf.train.AdagradDAOptimizer</code></li>
<li><code>tf.train.MomentumOptimizer</code></li>
<li><code>tf.train.AdamOptimizer</code></li>
<li><code>tf.train.FtrlOptimizer</code></li>
<li><code>tf.train.ProximalGradientDescentOptimizer</code></li>
<li><code>tf.train.ProximalAdagradOptimizer</code></li>
<li><code>tf.train.RMSPropOptimizer</code></li>
</ul>
<h2 id="可视化工具-Tensorboard"><a href="#可视化工具-Tensorboard" class="headerlink" title="可视化工具 Tensorboard"></a>可视化工具 Tensorboard</h2><h3 id="可视化神经网络结构"><a href="#可视化神经网络结构" class="headerlink" title="可视化神经网络结构"></a>可视化神经网络结构</h3><p><img src="http://image.rexking6.top/img/clip1521700297.png" alt=""></p>
<p>同时我们也可以展开看每个layer中的一些具体的结构：</p>
<p><img src="http://image.rexking6.top/img/clip1521700309.png" alt=""></p>
<p>为<code>xs</code>指定名称为<code>x_in</code>:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xs= tf.placeholder(tf.float32, [None, 1],name=&#x27;x_in&#x27;)</span><br></pre></td></tr></table></figure></p>
<p>使用<code>with tf.name_scope(&#39;inputs&#39;)</code>可以将<code>xs</code>和<code>ys</code>包含进来，形成一个大的图层，图层的名字就是<code>with tf.name_scope()</code>方法里的参数。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&#x27;inputs&#x27;):</span><br><span class="line">    # define placeholder for inputs to network</span><br><span class="line">    xs = tf.placeholder(tf.float32, [None, 1])</span><br><span class="line">    ys = tf.placeholder(tf.float32, [None, 1])</span><br></pre></td></tr></table></figure></p>
<p>之后需要使用 <code>tf.summary.FileWriter()</code> (<code>tf.train.SummaryWriter()</code>这种方式已经在 tf &gt;= 0.12 版本中摒弃) 将上面‘绘画’出的图保存到一个目录中，以方便后期在浏览器中可以浏览。 这个方法中的第二个参数需要使用<code>sess.graph</code> ， 因此我们需要把这句话放在获取<code>session</code>的后面。 这里的<code>graph</code>是将前面定义的框架信息收集起来，然后放在<code>logs/</code>目录下面。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session() # get session</span><br><span class="line"># tf.train.SummaryWriter soon be deprecated, use following</span><br><span class="line">writer = tf.summary.FileWriter(&quot;logs/&quot;, sess.graph)</span><br></pre></td></tr></table></figure></p>
<p>最后在你的terminal（终端）中 ，使用以下命令<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=logs</span><br></pre></td></tr></table></figure></p>
<p>输出的<code>logs</code>目录不要放在桌面上，最好直接放在磁盘的下级目录。</p>
<h3 id="可视化训练过程"><a href="#可视化训练过程" class="headerlink" title="可视化训练过程"></a>可视化训练过程</h3><p><img src="http://image.rexking6.top/img/clip1521700906.png" alt=""></p>
<p>在histograms里面我们还可以看到更多的layers的变化:</p>
<p><img src="http://image.rexking6.top/img/clip1521700920.png" alt=""></p>
<p>这里还有一个events , 在这次练习中我们会把 整个训练过程中的误差值（loss）在event里面显示出来, 甚至你可以显示更多你想要显示的东西.</p>
<p><img src="http://image.rexking6.top/img/clip1521700946.png" alt=""></p>
<h4 id="在-layer-中为-Weights-biases-设置变化图表"><a href="#在-layer-中为-Weights-biases-设置变化图表" class="headerlink" title="在 layer 中为 Weights, biases 设置变化图表"></a>在 layer 中为 Weights, biases 设置变化图表</h4><p>在 <code>add_layer()</code>方法中添加一个参数 <code>n_layer</code>,用来标识层数, 并且用变量 <code>layer_name</code> 代表其每层的名名称, 代码如下:</p>
<p><img src="http://image.rexking6.top/img/clip1521701537.png" alt=""></p>
<p>层中的<code>Weights</code>设置变化图, tensorflow中提供了<code>tf.histogram_summary()</code>方法,用来绘制图片, 第一个参数是图表的名称, 第二个参数是图表要记录的变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def add_layer(inputs, in_size, out_size, n_layer, activation_function=None):</span><br><span class="line">    # add one more layer and return the output of this layer</span><br><span class="line">    layer_name = &#x27;layer%s&#x27; % n_layer</span><br><span class="line">    with tf.name_scope(layer_name):</span><br><span class="line">        with tf.name_scope(&#x27;weights&#x27;):</span><br><span class="line">            Weights = tf.Variable(tf.random_normal([in_size, out_size]), name=&#x27;W&#x27;)</span><br><span class="line">            tf.summary.histogram(layer_name + &#x27;/weights&#x27;, Weights)</span><br><span class="line">        with tf.name_scope(&#x27;biases&#x27;):</span><br><span class="line">            biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name=&#x27;b&#x27;)</span><br><span class="line">            tf.summary.histogram(layer_name + &#x27;/biases&#x27;, biases)</span><br><span class="line">        with tf.name_scope(&#x27;Wx_plus_b&#x27;):</span><br><span class="line">            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)</span><br><span class="line">        if activation_function is None:</span><br><span class="line">            outputs = Wx_plus_b</span><br><span class="line">        else:</span><br><span class="line">            outputs = activation_function(Wx_plus_b, )</span><br><span class="line">        tf.summary.histogram(layer_name + &#x27;/outputs&#x27;, outputs)</span><br><span class="line">    return outputs</span><br></pre></td></tr></table></figure>
<p>修改之后的名称会显示在每个tensorboard中每个图表的上方显示, 如下图所示:</p>
<p><img src="http://image.rexking6.top/img/clip1521701598.png" alt=""></p>
<h4 id="设置loss的变化图"><a href="#设置loss的变化图" class="headerlink" title="设置loss的变化图"></a>设置loss的变化图</h4><p><code>Loss</code> 的变化图和之前设置的方法略有不同. loss是在tesnorBorad 的event下面的, 这是由于我们使用的是<code>tf.scalar_summary()</code> 方法.</p>
<p><img src="http://image.rexking6.top/img/clip1521701646.png" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&#x27;loss&#x27;):</span><br><span class="line">    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1]))</span><br><span class="line">    tf.summary.scalar(&#x27;loss&#x27;, loss)</span><br></pre></td></tr></table></figure>
<h4 id="给所有训练图合并"><a href="#给所有训练图合并" class="headerlink" title="给所有训练图合并"></a>给所有训练图合并</h4><p>接下来， 开始合并打包。 <code>tf.merge_all_summaries()</code> 方法会对我们所有的 <code>summaries</code> 合并到一起. 因此在原有代码片段中添加：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">merged = tf.summary.merge_all()</span><br></pre></td></tr></table></figure>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>为了较为直观显示训练过程中每个参数的变化，我们每隔上50次就记录一次结果 , 同时我们也应注意, merged 也是需要run 才能发挥作用的,所以在for循环中写下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if i%50 == 0:</span><br><span class="line">    rs = sess.run(merged,feed_dict=&#123;xs:x_data,ys:y_data&#125;)</span><br><span class="line">    writer.add_summary(rs, i)</span><br></pre></td></tr></table></figure>
<h1 id="卷积神经网络CNN"><a href="#卷积神经网络CNN" class="headerlink" title="卷积神经网络CNN"></a>卷积神经网络CNN</h1><h2 id="层"><a href="#层" class="headerlink" title="层"></a>层</h2><p><code>tf.truncated_normal(shape, mean, stddev)</code> :<code>shape</code>表示生成张量的维度，<code>mean</code>是均值，<code>stddev</code>是标准差。这个函数产生正态分布，均值和标准差自己设定。</p>
<p><code>tf.nn.conv2d(x,W,strides=[1,1,1,1]，padding=&#39;SAME&#39;)</code>函数是tensoflow里面的二维的卷积函数，<code>x</code>是图片的所有参数，<code>W</code>是此卷积层的权重，然后定义步长<code>strides=[1,1,1,1]</code>值，<code>strides[0]</code>和<code>strides[3]</code>的两个<code>1</code>是默认值，中间两个<code>1</code>代表<code>padding</code>时在<code>x</code>方向运动一步，<code>y</code>方向运动一步，<code>padding</code>采用的方式是<code>SAME</code>。</p>
<p><code>tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1])</code>:池化的核函数大小为2x2，因此<code>ksize=[1,2,2,1]</code>，步长为2，因此<code>strides=[1,2,2,1]</code>。</p>
<h2 id="Saver-保存读取"><a href="#Saver-保存读取" class="headerlink" title="Saver 保存读取"></a>Saver 保存读取</h2><h3 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h3><p>保存时, 首先要建立一个 <code>tf.train.Saver()</code> 用来保存, 提取变量. 再创建一个名为<code>my_net</code>的文件夹, 用这个 saver 来保存变量到这个目录 <code>&quot;my_net/save_net.ckpt&quot;</code>。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    save_path = saver.save(sess, &quot;my_net/save_net.ckpt&quot;)</span><br><span class="line">    print(&quot;Save to path: &quot;, save_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h3 id="提取"><a href="#提取" class="headerlink" title="提取"></a>提取</h3><p>提取时, 先建立零时的<code>W</code> 和 <code>b</code>容器. 找到文件目录, 并用<code>saver.restore()</code>我们放在这个目录的变量。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    # 提取变量</span><br><span class="line">    saver.restore(sess, &quot;my_net/save_net.ckpt&quot;)</span><br><span class="line">    print(&quot;weights:&quot;, sess.run(W))</span><br><span class="line">    print(&quot;biases:&quot;, sess.run(b))</span><br></pre></td></tr></table></figure></p>
<h1 id="循环神经网络RNN"><a href="#循环神经网络RNN" class="headerlink" title="循环神经网络RNN"></a>循环神经网络RNN</h1><p>莫烦老师讲得感觉他自己有些不理解了，略。</p>
<h1 id="自编码-Autoencoder"><a href="#自编码-Autoencoder" class="headerlink" title="自编码 (Autoencoder)"></a>自编码 (Autoencoder)</h1><h2 id="压缩与解压"><a href="#压缩与解压" class="headerlink" title="压缩与解压"></a>压缩与解压</h2><p><img src="http://image.rexking6.top/img/clip1521719105.png" alt=""></p>
<p>可以看出图片其实是经过了压缩,再解压的这一道工序. 当压缩的时候, 原有的图片质量被缩减, 解压时用信息量小却包含了所有关键信息的文件恢复出原本的图片。</p>
<p><img src="http://image.rexking6.top/img/clip1521719153.png" alt=""></p>
<p>原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作. 所以, 何不压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习. 这样学习起来就简单轻松了. 可以看出, 从头到尾, 我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习.</p>
<h2 id="编码器-Encoder"><a href="#编码器-Encoder" class="headerlink" title="编码器 Encoder"></a>编码器 Encoder</h2><p>这部分也叫作 encoder 编码器. 编码器能得到原数据的精髓, 然后我们只需要再创建一个小的神经网络学习这个精髓的数据,不仅减少了神经网络的负担, 而且同样能达到很好的效果.</p>
<p>这是一个通过自编码整理出来的数据, 他能从原数据中总结出每种类型数据的特征, 如果把这些特征类型都放在一张二维的图片上, 每种类型都已经被很好的用原数据的精髓区分开来. 如果你了解 PCA 主成分分析, 再提取主要特征时, 自编码和它一样,甚至超越了 PCA. 换句话说, 自编码可以像 PCA 一样 给特征属性降维.</p>
<h2 id="解码器-Decoder"><a href="#解码器-Decoder" class="headerlink" title="解码器 Decoder"></a>解码器 Decoder</h2><p> 解码器在训练的时候是要将精髓信息解压成原始信息, 那么这就提供了一个解压器的作用, 甚至我们可以认为是一个生成器 (类似于GAN). 那做这件事的一种特殊自编码叫做 variational autoencoders.</p>
<h2 id="编程实践"><a href="#编程实践" class="headerlink" title="编程实践"></a>编程实践</h2><p>今天的代码，我们会运用两个类型：</p>
<p>第一，是通过Feature的压缩并解压，并将结果与原始数据进行对比，观察处理过后的数据是不是如预期跟原始数据很相像。（这里会用到MNIST数据）<br>第二，我们只看<code>encoder</code>压缩的过程，使用它将一个数据集压缩到只有两个Feature时，将数据放入一个二维坐标系内，特征压缩的效果如下：</p>
<p><img src="http://image.rexking6.top/img/clip1521719607.png" alt=""></p>
<p>同样颜色的点，代表分到同一类的数据。</p>
<h3 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Parameter</span><br><span class="line">learning_rate = 0.01</span><br><span class="line">training_epochs = 5 # 五组训练</span><br><span class="line">batch_size = 256</span><br><span class="line">display_step = 1</span><br><span class="line">examples_to_show = 10</span><br></pre></td></tr></table></figure>
<p>MNIST数据，每张图片大小是 28x28 pix，即 784 Features：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Network Parameters</span><br><span class="line">n_input = 784  # MNIST data input (img shape: 28*28)</span><br></pre></td></tr></table></figure></p>
<ul>
<li>在压缩环节：我们要把这个Features不断压缩，经过第一个隐藏层压缩至256个 Features，再经过第二个隐藏层压缩至128个。</li>
<li>在解压环节：我们将128个Features还原至256个，再经过一步还原至784个。</li>
<li>在对比环节：比较原始数据与还原后的拥有 784 Features 的数据进行 cost 的对比，根据 cost 来提升我的 Autoencoder 的准确率，下图是两个隐藏层的 weights 和 biases 的定义： </li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># hidden layer settings</span><br><span class="line">n_hidden_1 = 256 # 1st layer num features</span><br><span class="line">n_hidden_2 = 128 # 2nd layer num features</span><br><span class="line">weights = &#123;</span><br><span class="line">	&#x27;encoder_h1&#x27;:tf.Variable(tf.random_normal([n_input,n_hidden_1])),</span><br><span class="line">	&#x27;encoder_h2&#x27;: tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),</span><br><span class="line">	&#x27;decoder_h1&#x27;: tf.Variable(tf.random_normal([n_hidden_2,n_hidden_1])),</span><br><span class="line">	&#x27;decoder_h2&#x27;: tf.Variable(tf.random_normal([n_hidden_1, n_input])),</span><br><span class="line">	&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">	&#x27;encoder_b1&#x27;: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">	&#x27;encoder_b2&#x27;: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">	&#x27;decoder_b1&#x27;: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">	&#x27;decoder_b2&#x27;: tf.Variable(tf.random_normal([n_input])),</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>
<p>下面来定义 Encoder 和 Decoder ，使用的 Activation function 是 <code>sigmoid</code>， 压缩之后的值应该在 [0,1] 这个范围内。在 <code>decoder</code> 过程中，通常使用对应于 <code>encoder</code> 的 Activation function：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Building the encoder</span><br><span class="line">def encoder(x):</span><br><span class="line">    # Encoder Hidden layer with sigmoid activation #1</span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&#x27;encoder_h1&#x27;]),</span><br><span class="line">                                   biases[&#x27;encoder_b1&#x27;]))</span><br><span class="line">    # Decoder Hidden layer with sigmoid activation #2</span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[&#x27;encoder_h2&#x27;]),</span><br><span class="line">                                   biases[&#x27;encoder_b2&#x27;]))</span><br><span class="line">    return layer_2</span><br><span class="line">    </span><br><span class="line"># Building the decoder</span><br><span class="line">def decoder(x):</span><br><span class="line">    # Encoder Hidden layer with sigmoid activation #1</span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&#x27;decoder_h1&#x27;]),</span><br><span class="line">                                   biases[&#x27;decoder_b1&#x27;]))</span><br><span class="line">    # Decoder Hidden layer with sigmoid activation #2</span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[&#x27;decoder_h2&#x27;]),</span><br><span class="line">                                   biases[&#x27;decoder_b2&#x27;]))</span><br><span class="line">    return layer_2</span><br></pre></td></tr></table></figure><br>来实现 Encoder 和 Decoder 输出的结果：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Construct model</span><br><span class="line">encoder_op = encoder(X) 			# 128 Features</span><br><span class="line">decoder_op = decoder(encoder_op)	# 784 Features</span><br><span class="line"></span><br><span class="line"># Prediction</span><br><span class="line">y_pred = decoder_op	# After </span><br><span class="line"># Targets (Labels) are the input data.</span><br><span class="line">y_true = X			# Before</span><br></pre></td></tr></table></figure><br>再通过我们非监督学习进行对照，即对 “原始的有 784 Features 的数据集” 和 “通过 ‘Prediction’ 得出的有 784 Features 的数据集” 进行最小二乘法的计算，并且使 cost 最小化:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Define loss and optimizer, minimize the squared error</span><br><span class="line">cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure><br>最后，通过 <code>Matplotlib</code> 的 <code>pyplot</code> 模块将结果显示出来， 注意在输出时MNIST数据集经过压缩之后 x 的最大值是1，而非255：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># Launch the graph</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    total_batch = int(mnist.train.num_examples/batch_size)</span><br><span class="line">    # Training cycle</span><br><span class="line">    for epoch in range(training_epochs):</span><br><span class="line">        # Loop over all batches</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)  # max(x) = 1, min(x) = 0</span><br><span class="line">            # Run optimization op (backprop) and cost op (to get loss value)</span><br><span class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;X: batch_xs&#125;)</span><br><span class="line">        # Display logs per epoch step</span><br><span class="line">        if epoch % display_step == 0:</span><br><span class="line">            print(&quot;Epoch:&quot;, &#x27;%04d&#x27; % (epoch+1),</span><br><span class="line">                  &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(c))</span><br><span class="line"></span><br><span class="line">    print(&quot;Optimization Finished!&quot;)</span><br><span class="line"></span><br><span class="line">    # # Applying encode and decode over test set</span><br><span class="line">    encode_decode = sess.run(</span><br><span class="line">        y_pred, feed_dict=&#123;X: mnist.test.images[:examples_to_show]&#125;)</span><br><span class="line">    # Compare original images with their reconstructions</span><br><span class="line">    f, a = plt.subplots(2, 10, figsize=(10, 2))</span><br><span class="line">    for i in range(examples_to_show):</span><br><span class="line">        a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))</span><br><span class="line">        a[1][i].imshow(np.reshape(encode_decode[i], (28, 28)))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>通过5个 Epoch 的训练，（通常情况下，想要得到好的的效果，我们应进行10 ~ 20个 Epoch 的训练）我们的结果如下：</p>
<p><img src="http://image.rexking6.top/img/clip1521720587.png" alt=""></p>
<p>上面一行是真实数据，下面一行是经过 <code>encoder</code> 和 <code>decoder</code> 之后的数据，如果继续进行训练，效果会更好。</p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>在类型二中，我们只显示 <code>encoder</code> 之后的数据， 并画在一个二维直角坐标系内。做法很简单，我们将原有 784 Features 的数据压缩成仅剩 2 Features 的数据：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Parameters</span><br><span class="line">learning_rate = 0.01    # 0.01 this learning rate will be better! Tested</span><br><span class="line">training_epochs = 10	# 10 Epoch 训练</span><br><span class="line">batch_size = 256</span><br><span class="line">display_step = 1</span><br></pre></td></tr></table></figure><br>通过四层 Hidden Layers 实现将 784 Features 压缩至 2 Features：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># hidden layer settings</span><br><span class="line">n_hidden_1 = 128</span><br><span class="line">n_hidden_2 = 64</span><br><span class="line">n_hidden_3 = 10</span><br><span class="line">n_hidden_4 = 2</span><br></pre></td></tr></table></figure><br>Weights 和 biases 也要做相应的变化：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">weights = &#123;</span><br><span class="line">    &#x27;encoder_h1&#x27;: tf.Variable(tf.truncated_normal([n_input, n_hidden_1],)),</span><br><span class="line">    &#x27;encoder_h2&#x27;: tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],)),</span><br><span class="line">    &#x27;encoder_h3&#x27;: tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3],)),</span><br><span class="line">    &#x27;encoder_h4&#x27;: tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4],)),</span><br><span class="line"></span><br><span class="line">    &#x27;decoder_h1&#x27;: tf.Variable(tf.truncated_normal([n_hidden_4, n_hidden_3],)),</span><br><span class="line">    &#x27;decoder_h2&#x27;: tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_2],)),</span><br><span class="line">    &#x27;decoder_h3&#x27;: tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_1],)),</span><br><span class="line">    &#x27;decoder_h4&#x27;: tf.Variable(tf.truncated_normal([n_hidden_1, n_input],)),</span><br><span class="line">	&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    &#x27;encoder_b1&#x27;: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    &#x27;encoder_b2&#x27;: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    &#x27;encoder_b3&#x27;: tf.Variable(tf.random_normal([n_hidden_3])),</span><br><span class="line">    &#x27;encoder_b4&#x27;: tf.Variable(tf.random_normal([n_hidden_4])),</span><br><span class="line"></span><br><span class="line">    &#x27;decoder_b1&#x27;: tf.Variable(tf.random_normal([n_hidden_3])),</span><br><span class="line">    &#x27;decoder_b2&#x27;: tf.Variable(tf.random_normal([n_hidden_2])),</span><br><span class="line">    &#x27;decoder_b3&#x27;: tf.Variable(tf.random_normal([n_hidden_1])),</span><br><span class="line">    &#x27;decoder_b4&#x27;: tf.Variable(tf.random_normal([n_input])),</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><br>与类型一类似，创建四层神经网络。（注意：在第四层时，输出量不再是 [0,1] 范围内的数，而是将数据通过默认的 Linear activation function 调整为 (-∞,∞) ：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def encoder(x):</span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&#x27;encoder_h1&#x27;]),</span><br><span class="line">                                   biases[&#x27;encoder_b1&#x27;]))</span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[&#x27;encoder_h2&#x27;]),</span><br><span class="line">                                   biases[&#x27;encoder_b2&#x27;]))</span><br><span class="line">    layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights[&#x27;encoder_h3&#x27;]),</span><br><span class="line">                                   biases[&#x27;encoder_b3&#x27;]))</span><br><span class="line">    layer_4 = tf.add(tf.matmul(layer_3, weights[&#x27;encoder_h4&#x27;]),</span><br><span class="line">                                    biases[&#x27;encoder_b4&#x27;])</span><br><span class="line">    return layer_4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def decoder(x):</span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights[&#x27;decoder_h1&#x27;]),</span><br><span class="line">                                   biases[&#x27;decoder_b1&#x27;]))</span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights[&#x27;decoder_h2&#x27;]),</span><br><span class="line">                                   biases[&#x27;decoder_b2&#x27;]))</span><br><span class="line">    layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights[&#x27;decoder_h3&#x27;]),</span><br><span class="line">                                biases[&#x27;decoder_b3&#x27;]))</span><br><span class="line">    layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights[&#x27;decoder_h4&#x27;]),</span><br><span class="line">                                biases[&#x27;decoder_b4&#x27;]))</span><br><span class="line">    return layer_4</span><br></pre></td></tr></table></figure></p>
<p>在输出图像时，我们只关心 <code>encoder</code> 压缩之后，即 <code>decoder</code> 解压之前的结果：</p>
<p><img src="http://image.rexking6.top/img/clip1521720851.png" alt=""></p>
<h1 id="scope-命名方法"><a href="#scope-命名方法" class="headerlink" title="scope 命名方法"></a>scope 命名方法</h1><p>scope 能让你命名变量的时候轻松很多. 同时也会在 reusing variable 代码中常常见到. 讨论下 tensorflow 当中的两种定义 scope 的方式.</p>
<h2 id="tf-name-scope"><a href="#tf-name-scope" class="headerlink" title="tf.name_scope()"></a><code>tf.name_scope()</code></h2><p>在 Tensorflow 当中有两种途径生成变量 variable, 一种是 <code>tf.get_variable()</code>, 另一种是 <code>tf.Variable()</code>. 如果在 <code>tf.name_scope()</code> 的框架下使用这两种方式, 结果会如下.<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">with tf.name_scope(&quot;a_name_scope&quot;):</span><br><span class="line">    initializer = tf.constant_initializer(value=1)</span><br><span class="line">    var1 = tf.get_variable(name=&#x27;var1&#x27;, shape=[1], dtype=tf.float32, initializer=initializer)</span><br><span class="line">    var2 = tf.Variable(name=&#x27;var2&#x27;, initial_value=[2], dtype=tf.float32)</span><br><span class="line">    var21 = tf.Variable(name=&#x27;var2&#x27;, initial_value=[2.1], dtype=tf.float32)</span><br><span class="line">    var22 = tf.Variable(name=&#x27;var2&#x27;, initial_value=[2.2], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.initialize_all_variables())</span><br><span class="line">    print(var1.name)        # var1:0</span><br><span class="line">    print(sess.run(var1))   # [ 1.]</span><br><span class="line">    print(var2.name)        # a_name_scope/var2:0</span><br><span class="line">    print(sess.run(var2))   # [ 2.]</span><br><span class="line">    print(var21.name)       # a_name_scope/var2_1:0</span><br><span class="line">    print(sess.run(var21))  # [ 2.0999999]</span><br><span class="line">    print(var22.name)       # a_name_scope/var2_2:0</span><br><span class="line">    print(sess.run(var22))  # [ 2.20000005]</span><br></pre></td></tr></table></figure></p>
<p>可以看出使用 <code>tf.Variable()</code> 定义的时候, 虽然 <code>name</code> 都一样, 但是为了不重复变量名, Tensorflow 输出的变量名并不是一样的. 所以, 本质上 <code>var2</code>, <code>var21</code>, <code>var22</code> 并不是一样的变量. 而另一方面, 使用<code>tf.get_variable()</code>定义的变量不会被<code>tf.name_scope()</code>当中的名字所影响.</p>
<h2 id="tf-variable-scope"><a href="#tf-variable-scope" class="headerlink" title="tf.variable_scope()"></a><code>tf.variable_scope()</code></h2><p>如果想要达到重复利用变量的效果, 我们就要使用 <code>tf.variable_scope()</code>, 并搭配 <code>tf.get_variable()</code> 这种方式产生和提取变量. 不像 <code>tf.Variable()</code> 每次都会产生新的变量, <code>tf.get_variable()</code> 如果遇到了同样名字的变量时, 它会单纯的提取这个同样名字的变量(避免产生新变量). 而在重复使用的时候, 一定要在代码中强调 <code>scope.reuse_variables()</code>, 否则系统将会报错, 以为你只是单纯的不小心重复使用到了一个变量.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">with tf.variable_scope(&quot;a_variable_scope&quot;) as scope:</span><br><span class="line">    initializer = tf.constant_initializer(value=3)</span><br><span class="line">    var3 = tf.get_variable(name=&#x27;var3&#x27;, shape=[1], dtype=tf.float32, initializer=initializer)</span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">    var3_reuse = tf.get_variable(name=&#x27;var3&#x27;,)</span><br><span class="line">    var4 = tf.Variable(name=&#x27;var4&#x27;, initial_value=[4], dtype=tf.float32)</span><br><span class="line">    var4_reuse = tf.Variable(name=&#x27;var4&#x27;, initial_value=[4], dtype=tf.float32)</span><br><span class="line">    </span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(var3.name)            # a_variable_scope/var3:0</span><br><span class="line">    print(sess.run(var3))       # [ 3.]</span><br><span class="line">    print(var3_reuse.name)      # a_variable_scope/var3:0</span><br><span class="line">    print(sess.run(var3_reuse)) # [ 3.]</span><br><span class="line">    print(var4.name)            # a_variable_scope/var4:0</span><br><span class="line">    print(sess.run(var4))       # [ 4.]</span><br><span class="line">    print(var4_reuse.name)      # a_variable_scope/var4_1:0</span><br><span class="line">    print(sess.run(var4_reuse)) # [ 4.]</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2018\05\17\三元组损失与TensorFlow在线三元组挖掘\" rel="bookmark">三元组损失与TensorFlow在线三元组挖掘</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2018\04\15\将Keras作为tensorflow的精简接口\" rel="bookmark">将Keras作为TensorFlow的精简接口</a></div>
    </li>
  </ul>

        <div class="reward-container">
  <div>一分一毛，也是心意。</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="Run-Qing Chen 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="Run-Qing Chen 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Run-Qing Chen
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://blog.rexking6.top/2018/03/22/%E8%8E%AB%E7%83%A6TensorFlow%E7%AC%94%E8%AE%B0/" title="莫烦TensorFlow笔记">https://blog.rexking6.top/2018/03/22/莫烦TensorFlow笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/03/12/coursera%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="prev" title="coursera深度学习笔记">
      <i class="fa fa-chevron-left"></i> coursera深度学习笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/03/24/%E4%BD%BF%E7%94%A8Google-Colaboratory%E5%85%8D%E8%B4%B9GPU/" rel="next" title="使用Google Colaboratory免费GPU">
      使用Google Colaboratory免费GPU <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

    <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">基础架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%EF%BC%88Tensor"><span class="nav-number">1.1.</span> <span class="nav-text">张量（Tensor):</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%AD%A5%E9%AA%A4"><span class="nav-number">1.2.</span> <span class="nav-text">基本步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E6%8D%B7%E6%95%B0%E6%8D%AE"><span class="nav-number">1.2.1.</span> <span class="nav-text">创捷数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.2.</span> <span class="nav-text">搭建模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E8%AF%AF%E5%B7%AE"><span class="nav-number">1.2.3.</span> <span class="nav-text">计算误差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E6%92%AD%E8%AF%AF%E5%B7%AE"><span class="nav-number">1.2.4.</span> <span class="nav-text">传播误差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">1.2.5.</span> <span class="nav-text">训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Session%E7%94%A8%E6%B3%95"><span class="nav-number">1.3.</span> <span class="nav-text">Session用法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Variable-%E5%8F%98%E9%87%8F"><span class="nav-number">1.4.</span> <span class="nav-text">Variable 变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Placeholder-%E4%BC%A0%E5%85%A5%E5%80%BC"><span class="nav-number">1.5.</span> <span class="nav-text">Placeholder 传入值</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8-optimizer"><span class="nav-number">1.6.</span> <span class="nav-text">优化器 optimizer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7-Tensorboard"><span class="nav-number">1.7.</span> <span class="nav-text">可视化工具 Tensorboard</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">1.7.1.</span> <span class="nav-text">可视化神经网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-number">1.7.2.</span> <span class="nav-text">可视化训练过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9C%A8-layer-%E4%B8%AD%E4%B8%BA-Weights-biases-%E8%AE%BE%E7%BD%AE%E5%8F%98%E5%8C%96%E5%9B%BE%E8%A1%A8"><span class="nav-number">1.7.2.1.</span> <span class="nav-text">在 layer 中为 Weights, biases 设置变化图表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AEloss%E7%9A%84%E5%8F%98%E5%8C%96%E5%9B%BE"><span class="nav-number">1.7.2.2.</span> <span class="nav-text">设置loss的变化图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%99%E6%89%80%E6%9C%89%E8%AE%AD%E7%BB%83%E5%9B%BE%E5%90%88%E5%B9%B6"><span class="nav-number">1.7.2.3.</span> <span class="nav-text">给所有训练图合并</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">1.7.2.4.</span> <span class="nav-text">训练数据</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN"><span class="nav-number">2.</span> <span class="nav-text">卷积神经网络CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%82"><span class="nav-number">2.1.</span> <span class="nav-text">层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Saver-%E4%BF%9D%E5%AD%98%E8%AF%BB%E5%8F%96"><span class="nav-number">2.2.</span> <span class="nav-text">Saver 保存读取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98"><span class="nav-number">2.2.1.</span> <span class="nav-text">保存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%8F%96"><span class="nav-number">2.2.2.</span> <span class="nav-text">提取</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN"><span class="nav-number">3.</span> <span class="nav-text">循环神经网络RNN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%87%AA%E7%BC%96%E7%A0%81-Autoencoder"><span class="nav-number">4.</span> <span class="nav-text">自编码 (Autoencoder)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%8B%E7%BC%A9%E4%B8%8E%E8%A7%A3%E5%8E%8B"><span class="nav-number">4.1.</span> <span class="nav-text">压缩与解压</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-Encoder"><span class="nav-number">4.2.</span> <span class="nav-text">编码器 Encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder"><span class="nav-number">4.3.</span> <span class="nav-text">解码器 Decoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5"><span class="nav-number">4.4.</span> <span class="nav-text">编程实践</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Autoencoder"><span class="nav-number">4.4.1.</span> <span class="nav-text">Autoencoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encoder"><span class="nav-number">4.4.2.</span> <span class="nav-text">Encoder</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#scope-%E5%91%BD%E5%90%8D%E6%96%B9%E6%B3%95"><span class="nav-number">5.</span> <span class="nav-text">scope 命名方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-name-scope"><span class="nav-number">5.1.</span> <span class="nav-text">tf.name_scope()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-variable-scope"><span class="nav-number">5.2.</span> <span class="nav-text">tf.variable_scope()</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Run-Qing Chen"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Run-Qing Chen</p>
  <div class="site-description" itemprop="description">覆苍天以为衾，卧大地以为庐。</div>
</div>


   <div class="feed-link motion-element">
     <a href="/atom.xml" rel="alternate">
       <i class="fa fa-rss"></i>
       RSS
     </a>
   </div>
 
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">189</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/RexKing6" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;RexKing6" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1010026261@qq.com" title="E-Mail → mailto:1010026261@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zxpblog.cn/" title="https:&#x2F;&#x2F;www.zxpblog.cn&#x2F;" rel="noopener" target="_blank">赵小平</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://whitepuffer.github.io/" title="https:&#x2F;&#x2F;whitepuffer.github.io&#x2F;" rel="noopener" target="_blank">江斓</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://kexue.fm/" title="https:&#x2F;&#x2F;kexue.fm&#x2F;" rel="noopener" target="_blank">科学空间</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://yongyuan.name/" title="https:&#x2F;&#x2F;yongyuan.name&#x2F;" rel="noopener" target="_blank">袁勇</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://blog.csdn.net/abcjennifer" title="https:&#x2F;&#x2F;blog.csdn.net&#x2F;abcjennifer" rel="noopener" target="_blank">Rachel Zhang</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://dmkf.xyz/" title="http:&#x2F;&#x2F;dmkf.xyz&#x2F;" rel="noopener" target="_blank">代码咖啡</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://wuxiaolong.me/" title="http:&#x2F;&#x2F;wuxiaolong.me&#x2F;" rel="noopener" target="_blank">吴小龙同学</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.tennfy.com/" title="http:&#x2F;&#x2F;www.tennfy.com&#x2F;" rel="noopener" target="_blank">TENNFY WU</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fab fa-accessible-icon"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Run-Qing Chen</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">3.3m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">49:46</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"6XDsO3aHIjDk3nV6eLJCufbl-MdYXbMMI","app_key":"YK4qOc0TpkazN6exhuqsnwmB","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
